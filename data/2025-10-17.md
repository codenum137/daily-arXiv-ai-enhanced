<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 8]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Structure-Preserving Error-Correcting Codes for Polynomial Frames](https://arxiv.org/abs/2510.13882)
*Baigang Chen,Dongfang Zhao*

Main category: cs.IT

TL;DR: 提出了一种结构保持的可靠性层，在多项式环中直接进行错误校正，避免格式转换和往返延迟，适用于FFT/NTT分析、编码计算和隐私保护机器学习等场景。


<details>
  <summary>Details</summary>
Motivation: 现代计算框架中多项式数据在NIC、存储和加速器之间传输时，即使罕见的静默数据损坏也可能通过下游算术级联放大。传统防御方法不匹配低延迟流水线：检测重传增加RTT，字节流ECC忽略代数结构并强制格式转换。

Method: 设计了两种互补方案：针对奇数长度使用Hensel提升的BCH理想和幂等编码器；针对2的幂次长度使用重根负循环码和导数式解码。通过环自同构提供原地交织以分散突发错误。

Result: 在四个帧大小(N=1024,2048,4096,8192)上，在符号错误率10^-6到10^-5时达到每帧失败目标10^-9，t=8-9，仅产生0.20%-1.56%开销，交织后容忍约32-72字节未知错误突发（作为擦除标记时约翻倍）。

Conclusion: 通过将错误校正与环语义对齐，从代数编码角度为多项式帧计算提供了可部署的鲁棒性实用方案。

Abstract: Modern FFT/NTT analytics, coded computation, and privacy-preserving ML
interface routinely move polynomial frames across NICs, storage, and
accelerators. However, even rare silent data corruption (SDC) can flip a few
ring coefficients and cascade through downstream arithmetic. Conventional
defenses are ill-matched to current low-latency pipelines:
detect-and-retransmit adds RTTs, while byte-stream ECC ignores the algebraic
structure and forces format conversions. To that end, we propose a
structure-preserving reliability layer that operates in the encoded data's
original polynomial ring, adds a small amount of systematic redundancy, and
corrects symbol errors/flagged erasures without round-trip or format changes.
We construct two complementary schemes: one for odd length $N_{odd}$ via a
Hensel-lifted BCH ideal with an idempotent encoder, and one for power-of-two
length $N_{2^m}$ via a repeated-root negacyclic code with derivative-style
decoding. In particular, to stay robust against clustered errors, a ring
automorphism provides in-place interleaving to disperse bursts. Implementation
wise, on four frame sizes $N\!=\!1024, 2048, 4096, 8192$, we meet a per-frame
failure target of $10^{-9}$ at symbol error rates $10^{-6}\text{--}10^{-5}$
with $t\!=\!8\text{--}9$, incurring only $0.20\%\text{--}1.56\%$ overhead and
tolerating $\sim\!32\text{--}72$\,B unknown-error bursts (roughly doubled when
flagged as erasures) after interleaving. By aligning error correction with ring
semantics, we take a practical step toward deployable robustness for
polynomial-frame computations from an algebraic coding perspective.

</details>


### [2] [Location-Aided Distributed Beamforming for Near-Field Communications with Element-Wise RIS](https://arxiv.org/abs/2510.14226)
*Xiao Zheng,Wenchi Cheng,Jingqing Wang,Zhuohui Yao,Jiangzhou Wang*

Main category: cs.IT

TL;DR: 本文提出了一种新的元素级RIS架构和分布式位置辅助传输方案，用于解决主动可重构智能表面在近场通信中的信道估计困难和离散相位约束问题。


<details>
  <summary>Details</summary>
Motivation: 主动RIS能够抵抗被动RIS的双重衰落衰减，但现有工作忽视了RIS辅助系统中信道估计的固有困难以及实际部署中的离散相位约束问题。

Method: 设计了新的元素级RIS架构，提供动态元素选择能力；基于菲涅尔衍射理论构建空间域位置到相位域波相位分布的映射；提出分布式波束成形设计，采用确定后对齐的相位策略。

Result: 渐近分析表明，当RIS较大时，所提方案能够以固定比例的反射元素实现最优增益，仿真验证了其相对于其他协议的优越性。

Conclusion: 所提出的方案能够有效增强CSI受限的RIS辅助近场通信的反射增益，同时降低估计开销。

Abstract: Active reconfigurable intelligent surface (RIS) emerges as an effective
technique to resist the double-fading attenuation of passive RIS. By embedding
with power harvesting function, it further evolves to zero-power active RIS,
which can effectively enhance the flexibility of RIS deployment without
external power demand. Nevertheless, existing works neglected the inherent
difficulty of channel estimation (CE) for RIS-assisted systems, and the
discrete phase shift constraint in practical deployment. In this paper we
design a new element-wise RIS architecture and propose a distributed
location-aided transmission scheme with low complexity to enhance the reflected
gain for channel state information (CSI)-limited RIS-assisted near-field
communications. Specifically, the new element-wise RIS provides dynamic element
selection capability with low hardware resources. Based on Fresnel diffraction
theory, we construct the mapping from locations in space-domain to phase
distributions of waves in phase-domain and reveal the priority of elements for
harvesting and reflecting. {Then, the distributed beamforming design with the
phase of determine-then-align is proposed, where the estimation overhead
reduction stems from exempted requirements of RIS-associated CE at base station
(BS).} The asymptotic analysis indicates that the proposed scheme can achieve
the optimal gain with a fixed proportion of reflective elements when RIS is
large, followed by simulations to verify its superiority to other protocols.

</details>


### [3] [The asymptotic number of equivalence classes of linear codes with given dimension](https://arxiv.org/abs/2510.14424)
*Andrea Di Giusto,Alberto Ravagnani*

Main category: cs.IT

TL;DR: 本文研究了具有指定长度和维度的线性码的等价类数量的渐近行为，推导了在不同等价关系下的显式渐近公式，并建立了与离散高斯分布的连接。


<details>
  <summary>Details</summary>
Motivation: 虽然之前已经研究了给定长度下不等价码的总数，但维度随长度变化的情况尚未被考虑。本文旨在填补这一空白，研究线性码等价类数量的渐近行为。

Method: 采用渐近分析方法，推导了在三种标准等价关系下的显式渐近公式，同时得到了q-二项式系数和的精确渐近表达式。

Result: 获得了固定字母大小和增加长度情况下线性码等价类数量的渐近公式，并建立了这些渐近量与布朗运动产生的离散高斯分布之间的自然连接。

Conclusion: 本文不仅解决了该领域的一个开放性问题，还为线性码等价类数量的渐近行为提供了概率解释，揭示了与离散高斯分布的深刻联系。

Abstract: We investigate the asymptotic number of equivalence classes of linear codes
with prescribed length and dimension. While the total number of inequivalent
codes of a given length has been studied previously, the case where the
dimension varies as a function of the length has not yet been considered. We
derive explicit asymptotic formulas for the number of equivalence classes under
three standard notions of equivalence, for a fixed alphabet size and increasing
length. Our approach also yields an exact asymptotic expression for the sum of
all q-binomial coefficients, which is of independent interest and answers an
open question in this context. Finally, we establish a natural connection
between these asymptotic quantities and certain discrete Gaussian distributions
arising from Brownian motion, providing a probabilistic interpretation of our
results.

</details>


### [4] [Rotatable Antenna-Enhanced Beamforming: Signal Enhancement and Interference Suppression](https://arxiv.org/abs/2510.14574)
*Jie Feng,Zhenbing Liu,Junjie Dai,Hongbin Chen,Fangjiong Chen*

Main category: cs.IT

TL;DR: 本文研究了可旋转天线增强的单/多波束形成技术，通过优化天线旋转来利用新的空间自由度，显著提升了阵列增益性能。


<details>
  <summary>Details</summary>
Motivation: 传统的固定方向天线阵列由于天线定向增益在不同转向角度上的显著变化，可能难以有效增强信号和/或抑制干扰。

Method: 联合优化天线旋转矢量和天线权重向量，对于无干扰的单波束形成情况推导出闭式最优解，对于多波束形成情况提出高效的交替优化算法。

Result: 仿真结果表明，所提出的基于可旋转天线的方案在阵列增益方面显著优于传统的基于固定方向天线和基于各向同性天线的方案。

Conclusion: 通过利用天线旋转带来的新空间自由度，可旋转天线技术能够有效突破传统波束形成的性能限制。

Abstract: Conventional beamforming with fixed-orientation antenna (FOA) arrays may
struggle to effectively enhance signal and/or suppress interference due to
significant variations in antenna directive gains over different steering
angles. To break this limitation, we investigate in this paper the rotatable
antenna (RA)-enhanced single/multi-beam forming by exploiting the new spatial
degrees of freedom (DoFs) via antennas' rotation optimization. Specifically,
the antenna rotation vector (ARV) and antenna weight vector (AWV) are jointly
optimized to maximize the minimum array gain over signal directions, subject to
a given constraint on the maximum array gain over interference directions. For
the special case of single-beam forming without interference, the optimal ARV
is derived in closed-form with the maximum ratio combining (MRC) beamformer
applied to the AWV. For the general case of multi-beam forming, we propose an
efficient alternating optimization (AO) algorithm to find a high-quality
suboptimal solution by iteratively optimizing one of the ARV and AWV with the
other being fixed. Simulation results demonstrate that the proposed RA-based
scheme can significantly outperform the traditional FOA-based and isotropic
antenna (IA)-based schemes in terms of array gain.

</details>


### [5] [Task-Based Quantization for Channel Estimation in RIS Empowered MmWave Systems](https://arxiv.org/abs/2510.14649)
*Gyoseung Lee,In-soo Kim,Yonina C. Eldar,A. Lee Swindlehurst,Hyeongtaek Lee,Minje Kim,Junil Choi*

Main category: cs.IT

TL;DR: 本文研究了采用低分辨率量化的可重构智能表面(RIS)毫米波多用户单输入多输出通信系统的信道估计问题，提出了基于任务的量化信道估计设计，在有限比特分辨率约束下提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 由于大规模天线阵列和宽信号带宽中模数转换器(ADC)的高成本和功耗，设计具有低分辨率ADC的毫米波系统是有益的。需要解决在有限比特分辨率约束下的信道估计问题。

Method: 提出基于任务的量化信道估计设计，考虑混合模拟和数字架构。开发了两种信道估计器：针对纯无源RIS的级联信道估计器，以及利用RIS中少量半无源元件额外信息的分离信道估计器。

Result: 数值结果表明，所提出的基于任务量化的信道估计设计优于纯数字方法，并能有效接近无限分辨率ADC系统的性能。所提出的信道估计器在训练开销较小的情况下优于基线方法。

Conclusion: 基于任务的量化信道估计设计能够有效应对低分辨率ADC的挑战，在有限比特分辨率约束下实现接近理想性能的信道估计，为毫米波RIS系统提供了实用的解决方案。

Abstract: In this paper, we investigate channel estimation for reconfigurable
intelligent surface (RIS) empowered millimeter-wave (mmWave) multi-user
single-input multiple-output communication systems using low-resolution
quantization. Due to the high cost and power consumption of analog-to-digital
converters (ADCs) in large antenna arrays and for wide signal bandwidths,
designing mmWave systems with low-resolution ADCs is beneficial. To tackle this
issue, we propose a channel estimation design using task-based quantization
that considers the underlying hybrid analog and digital architecture in order
to improve the system performance under finite bit-resolution constraints. Our
goal is to accomplish a channel estimation task that minimizes the mean squared
error distortion between the true and estimated channel. We develop two types
of channel estimators: a cascaded channel estimator for an RIS with purely
passive elements, and an estimator for the separate RIS-related channels that
leverages additional information from a few semi-passive elements at the RIS
capable of processing the received signals with radio frequency chains.
Numerical results demonstrate that the proposed channel estimation designs
exploiting task-based quantization outperform purely digital methods and can
effectively approach the performance of a system with unlimited resolution
ADCs. Furthermore, the proposed channel estimators are shown to be superior to
baselines with small training overhead.

</details>


### [6] [Rate-Adaptive Spatially Coupled MacKay-Neal Codes with Thresholds Close to Capacity](https://arxiv.org/abs/2510.14843)
*Ayman Zahr,Gianluigi Liva*

Main category: cs.IT

TL;DR: 本文分析了速率自适应MacKay-Neal码集合的渐近性能，其中内码是原图空间耦合LDPC码。通过定义并行信道模型，计算了BP解码阈值，显示SC MN码集合在[0,1]速率范围内距离二进制输入加性高斯白噪声信道容量仅0.15 dB。


<details>
  <summary>Details</summary>
Motivation: 研究速率自适应码集合的渐近性能，特别是结合原图空间耦合LDPC码的MacKay-Neal码结构，以接近信道容量。

Method: 使用密度进化分析，通过定义并行信道模型来计算BP解码阈值。

Result: SC MN码集合在[0,1]速率范围内距离二进制输入加性高斯白噪声信道容量仅0.15 dB。

Conclusion: SC MN码集合在速率自适应编码中表现出优异的性能，接近信道容量极限。

Abstract: We analyze by density evolution the asymptotic performance of rate-adaptive
MacKay-Neal (MN) code ensembles, where the inner code is a protograph spatially
coupled (SC) low-density parity-check code. By resorting to a suitably-defined
parallel channel model, we compute belief propagation decoding thresholds,
showing that SC MN code ensembles can perform within 0.15 dB from the
binary-input additive white Gaussian noise capacity over the full [0,1] rate
range.

</details>


### [7] [Rate-Adaptive Protograph-Based MacKay-Neal Codes](https://arxiv.org/abs/2510.14856)
*Ayman Zahr,Emna Ben Yacoub,Balázs Matuz,Gianluigi Liva*

Main category: cs.IT

TL;DR: 本文分析了基于原图的速率自适应MacKay-Neal(MN)码，通过外部分布匹配器(DM)调整速率，结合内层原图LDPC码，在固定块长度下实现宽范围速率自适应，性能接近香农极限1dB以内。


<details>
  <summary>Details</summary>
Motivation: 为高速无线/光通信链路提供固定块长度下的速率自适应编码方案，避免为不同速率设计不同LDPC码的复杂性。

Method: 采用外部分布匹配器与内层原图LDPC码相结合的非线性编码结构，通过密度演进和错误平层分析评估性能，推导内层LDPC码的平均输入输出权重分布。

Result: 使用单一LDPC码集合可在宽速率范围内实现接近香农极限1dB的性能，通过调整DM参数选择码率，保持固定块长度。

Conclusion: 该构造为采用二进制输入调制的高速无线/光通信链路提供了具有恒定块长度和固定LDPC码的速率自适应解决方案。

Abstract: Rate-adaptive MacKay-Neal (MN) codes based on protographs are analyzed. The
code construction employs an outer distribution matcher (DM) to adapt the rate
of the scheme. The DM is coupled with an inner protograph-based low-density
parity-check (LDPC) code. The performance achievable by the resulting code
structure, that is nonlinear, is studied by means of an equivalent
communication model that reduces the problem to the analysis of the inner
(linear) LDPC code with transmission that takes place in parallel over the
communication channel, and over a suitably defined binary symmetric channel. A
density evolution analysis of protograph MN code ensembles is outlined, and it
is complemented by an error floor analysis that relies on the derivation of the
average input-output weight distribution of the inner LDPC code ensemble.
Conditions on the shape of the normalized logarithmic asymptotic input-output
weight distribution are defined, which allow discarding code ensembles with bad
error floor properties during the code design phase. Examples of code designs
are provided, showing how the use of a single LDPC code ensemble allows
operating within 1 dB from the Shannon limit over a wide range of code rates,
where the code rate is selected by tuning the DM parameters. By enabling rate
flexibility with a constant blocklength, and with a fixed LDPC code as inner
code, the construction provides an appealing solution for very high-throughput
wireless (optical) links that employ binary-input modulations.

</details>


### [8] [The Whole Is Less than the Sum of Parts: Subsystem Inconsistency in Partial Information Decomposition](https://arxiv.org/abs/2510.14864)
*Aobo Lyu,Andrew Clark,Netanel Raviv*

Main category: cs.IT

TL;DR: 本文识别了部分信息分解（PID）框架中违反整体等于部分之和原则的问题，提出了针对三变量系统的系统信息分解（SID）框架来解决该问题，但证明在四变量及以上系统中，基于格的结构无法完全消除这种不一致性。


<details>
  <summary>Details</summary>
Motivation: 部分信息分解（PID）自2010年提出以来在多个领域得到应用，但缺乏统一的理论框架，存在关键的概念和技术挑战。本文旨在解决PID违反整体等于部分之和原则这一根本问题。

Method: 通过三变量系统的反例展示PID如何违反整体等于部分之和原则，然后引入新的公理化框架——系统信息分解（SID），专门针对三变量系统重新定义基于协同关系的信息原子求和规则。

Result: SID框架成功解决了三变量系统中的整体等于部分之和原则违反问题，但进一步证明在四变量及以上系统中，现有基于格的结构无法通过任何部分求和方法完全消除这种不一致性。

Conclusion: 基于（反链）格的信息分解方法对于一般的多变量系统存在固有的不足，需要新的理论框架来克服这些根本限制。

Abstract: Partial Information Decomposition (PID) was proposed by Williams and Beer in
2010 as a tool for analyzing fine-grained interactions between multiple random
variables, and has since found numerous applications ranging from neuroscience
to privacy. However, a unified theoretical framework remains elusive due to key
conceptual and technical challenges. We identify and illustrate a crucial
problem: PID violates the set-theoretic principle that the whole equals the sum
of its parts (WESP). Through a counterexample in a three-variable system, we
demonstrate how such violations naturally arise, revealing a fundamental
limitation of current lattice-based PID frameworks. To address this issue, we
introduce a new axiomatic framework, termed System Information Decomposition
(SID), specifically tailored for three-variable systems. SID resolves the WESP
violation by redefining the summation rules of decomposed information atoms
based on synergistic relationships. However, we further show that for systems
with four or more variables, no partial summation approach within the existing
lattice-based structures can fully eliminate WESP inconsistencies. Our results
thus highlight the inherent inadequacy of (antichain) lattice-based
decompositions for general multivariate systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [9] [Benefits and Limitations of Communication in Multi-Agent Reasoning](https://arxiv.org/abs/2510.13903)
*Michael Rizvi-Martel,Satwik Bhattamishra,Neil Rathi,Guillaume Rabusseau,Michael Hahn*

Main category: cs.MA

TL;DR: 本文提出了一个理论框架来分析多智能体系统的表达能力，研究了状态跟踪、记忆和k跳推理三种算法家族，推导了所需智能体数量、通信结构和可实现的加速效果，并通过实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链提示在大型语言模型中推广了逐步推理，但随着问题复杂性和上下文长度的增加，模型性能仍然下降。多智能体范式通过将复杂任务分解为更短、可管理的任务提供了有前景的解决方案，但这类系统的基本能力尚未得到充分理解。

Method: 提出了一个理论框架来分析多智能体系统的表达能力，应用于三种算法家族：状态跟踪、回忆和k跳推理。推导了所需智能体数量、智能体间通信的数量和结构以及可实现的加速效果的理论界限。

Result: 确定了通信确实有益的机制，描绘了智能体数量和带宽之间的权衡，并揭示了当任一资源受限时的内在限制。实验结果表明关键量之间的权衡与理论预测一致。

Conclusion: 该分析为设计可扩展的多智能体推理系统提供了原则性指导，揭示了多智能体系统在复杂推理任务中的基本能力和限制。

Abstract: Chain-of-thought prompting has popularized step-by-step reasoning in large
language models, yet model performance still degrades as problem complexity and
context length grow. By decomposing difficult tasks with long contexts into
shorter, manageable ones, recent multi-agent paradigms offer a promising
near-term solution to this problem. However, the fundamental capacities of such
systems are poorly understood. In this work, we propose a theoretical framework
to analyze the expressivity of multi-agent systems. We apply our framework to
three algorithmic families: state tracking, recall, and $k$-hop reasoning. We
derive bounds on (i) the number of agents required to solve the task exactly,
(ii) the quantity and structure of inter-agent communication, and (iii) the
achievable speedups as problem size and context scale. Our results identify
regimes where communication is provably beneficial, delineate tradeoffs between
agent count and bandwidth, and expose intrinsic limitations when either
resource is constrained. We complement our theoretical analysis with a set of
experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical
outcomes confirm the tradeoffs between key quantities predicted by our theory.
Collectively, our analysis offers principled guidance for designing scalable
multi-agent reasoning systems.

</details>


### [10] [Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations](https://arxiv.org/abs/2510.13982)
*Jinkun Chen,Sher Badshah,Xuemin Yu,Sijia Han,Jiechao Gao*

Main category: cs.MA

TL;DR: 本文批评当前基于LLM的多智能体系统局限于静态沙盒环境，提出需要超越静态范式，发展开放、持续协同进化的自适应多智能体模拟。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的多智能体系统和社会模拟大多受限于预定义任务、有限动态和刚性评估标准，无法捕捉真实世界社会的复杂性。作者认为静态、任务特定的基准从根本上不足，需要重新思考。

Method: 批判性回顾结合LLM与多智能体动态的新兴架构，强调平衡稳定性与多样性、评估意外行为、扩展至更复杂系统等关键挑战，并为这一快速发展的领域引入新的分类法。

Result: 提出了以开放性、持续协同进化和开发有弹性、社会对齐的AI生态系统为中心的研究路线图。

Conclusion: 呼吁研究社区超越静态范式，帮助塑造下一代自适应、社会感知的多智能体模拟。

Abstract: What if artificial agents could not just communicate, but also evolve, adapt,
and reshape their worlds in ways we cannot fully predict? With llm now powering
multi-agent systems and social simulations, we are witnessing new possibilities
for modeling open-ended, ever-changing environments. Yet, most current
simulations remain constrained within static sandboxes, characterized by
predefined tasks, limited dynamics, and rigid evaluation criteria. These
limitations prevent them from capturing the complexity of real-world societies.
In this paper, we argue that static, task-specific benchmarks are fundamentally
inadequate and must be rethought. We critically review emerging architectures
that blend llm with multi-agent dynamics, highlight key hurdles such as
balancing stability and diversity, evaluating unexpected behaviors, and scaling
to greater complexity, and introduce a fresh taxonomy for this rapidly evolving
field. Finally, we present a research roadmap centered on open-endedness,
continuous co-evolution, and the development of resilient, socially aligned AI
ecosystems. \textbf{We call on the community to move beyond static paradigms
and help shape the next generation of adaptive, socially-aware multi-agent
simulations.}

</details>


### [11] [The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems](https://arxiv.org/abs/2510.14401)
*Prateek Gupta,Qiankun Zhong,Hiromu Yakura,Thomas Eisenmann,Iyad Rahwan*

Main category: cs.MA

TL;DR: 本文提出了一个去除显式奖励信号的公共池资源模拟框架，结合社会学习和基于规范的惩罚机制，研究LLM在多智能体社会中合作与规范的内生演化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统在公共池资源游戏中通常为智能体提供明确的奖励函数，而人类合作往往在没有完全了解收益和群体信息的情况下，通过启发式、沟通和惩罚机制产生。

Method: 引入一个公共池资源模拟框架，移除显式奖励信号，嵌入文化演化机制：社会学习（从成功同伴处学习策略和信念）和基于规范的惩罚（基于Ostrom资源治理原则）。智能体通过环境反馈从收获、监控和惩罚的后果中学习。

Result: 通过重现现有人类行为研究的关键发现验证了模拟的有效性，并在不同环境和社会初始化条件下（资源丰富vs稀缺；利他vs自私）检验规范演化，发现不同LLM在维持合作和规范形成方面存在系统性差异。

Conclusion: 该框架为研究混合动机LLM社会中涌现规范提供了严格测试平台，可为AI系统在社会和组织环境中的设计提供参考，其中与协作规范的对接对AI中介环境的稳定性、公平性和有效治理至关重要。

Abstract: A growing body of multi-agent studies with Large Language Models (LLMs)
explores how norms and cooperation emerge in mixed-motive scenarios, where
pursuing individual gain can undermine the collective good. While prior work
has explored these dynamics in both richly contextualized simulations and
simplified game-theoretic environments, most LLM systems featuring common-pool
resource (CPR) games provide agents with explicit reward functions directly
tied to their actions. In contrast, human cooperation often emerges without
full visibility into payoffs and population, relying instead on heuristics,
communication, and punishment. We introduce a CPR simulation framework that
removes explicit reward signals and embeds cultural-evolutionary mechanisms:
social learning (adopting strategies and beliefs from successful peers) and
norm-based punishment, grounded in Ostrom's principles of resource governance.
Agents also individually learn from the consequences of harvesting, monitoring,
and punishing via environmental feedback, enabling norms to emerge
endogenously. We establish the validity of our simulation by reproducing key
findings from existing studies on human behavior. Building on this, we examine
norm evolution across a $2\times2$ grid of environmental and social
initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and
benchmark how agentic societies comprised of different LLMs perform under these
conditions. Our results reveal systematic model differences in sustaining
cooperation and norm formation, positioning the framework as a rigorous testbed
for studying emergent norms in mixed-motive LLM societies. Such analysis can
inform the design of AI systems deployed in social and organizational contexts,
where alignment with cooperative norms is critical for stability, fairness, and
effective governance of AI-mediated environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: 提出了一种名为STDW的自训练动态加权方法，通过动态平衡源域和目标域损失贡献来增强渐进域适应的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统渐进域适应方法存在知识迁移效率低和中间数据不完整的问题，需要解决从源域到目标域的平滑知识迁移挑战。

Method: 引入动态加权机制，通过时间变化超参数ϱ控制域特定学习强度，结合自训练生成伪标签并优化加权目标函数进行迭代模型更新。

Result: 在旋转MNIST、颜色偏移MNIST、肖像数据集和Cover Type数据集上的实验表明，STDW优于现有基线方法。

Conclusion: 该方法为鲁棒渐进域适应提供了理论见解和实用框架，在动态现实场景中具有应用潜力。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [13] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出一种结合对抗训练的统一框架，同时实现鲁棒分类和高保真生成建模，解决了传统联合能量模型训练不稳定和样本质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有混合方法如联合能量模型在单一框架中同时实现鲁棒分类和高质量生成存在挑战，主要受限于SGLD训练的不稳定性和样本质量差的问题。

Method: 采用对抗训练原则，包含三个创新：1) 用基于对抗训练的稳定方法替代SGLD；2) 协同对抗训练增强分类鲁棒性；3) 两阶段训练解决批归一化与EBM训练的不兼容性。

Result: 在CIFAR-10、CIFAR-100和ImageNet上显著提升对抗鲁棒性，生成保真度超越BigGAN并接近扩散模型，首次实现MCMC-based EBM在复杂高分辨率数据集上的高质量生成。

Conclusion: 该方法解决了JEM扩展的关键稳定性问题，证明对抗训练可作为统一框架的有效基础，能够同时生成和鲁棒分类视觉数据。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [14] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames提出了一种场景驱动的关键帧选择新范式，通过预测语义连贯的查询相关片段来保留时间连续性，支持任意数量关键帧选择以适应不同用户预算。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长视频理解中面临的计算成本高、上下文窗口限制问题，以及现有关键帧选择方法导致信息丢失、时间不连续和缺乏多尺度灵活性的缺陷。

Method: 首先构建PeakClips数据集，然后采用三阶段渐进式课程学习：两个监督微调阶段用于时间定位和关键片段感知，以及一个强化学习阶段直接优化场景驱动预测策略。

Result: 在主要长视频理解基准测试上的广泛实验表明，K-frames提供了有效、可解释且即插即用的多尺度关键帧选择解决方案。

Conclusion: K-frames通过场景驱动的关键帧选择方法，有效解决了长视频理解中的关键帧选择问题，提供了灵活的多尺度解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [15] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了多视图半监督标签分布学习方法MVSS-LDL，通过利用各视图的局部最近邻结构并强调多视图间局部结构的互补性，解决了多视图半监督标签分布学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有的标签分布学习方法主要针对单视图的监督学习问题，而多视图标签分布学习问题以及包含未标记数据的半监督场景尚未被考虑。本文旨在填补这一空白。

Method: 首先计算每个视图的k-最近邻集合，然后通过整合其他视图中的最近邻信息来补充当前视图的最近邻集合，最后基于补充后的最近邻集合构建基于图学习的多视图半监督标签分布学习模型。

Result: 数值研究表明，MVSS-LDL方法在分类性能上明显优于现有的单视图标签分布学习方法。

Conclusion: MVSS-LDL是首个针对多视图标签分布学习的尝试，通过考虑局部最近邻结构的互补性，不同视图可以相互提供局部结构信息来互补，取得了显著的性能提升。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [16] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: 提出Weight Weaving方法，通过用户定义的池化函数在λ值搜索空间中汇集模型权重，无需评估数据即可改进模型合并性能


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法严重依赖缩放超参数λ，但缺乏无需数据的原则性设置方法，研究者通常使用评估集数据调优λ，这在实践中不可行

Method: Weight Weaving技术使用用户定义的池化函数（如平均、随机选择或现有模型合并方法）在λ值搜索空间中汇集模型权重，具有高模块性和最小约束

Result: 在三个ViT变体和三个实验设置（视觉多任务学习、持续学习和领域泛化）中验证，该方法持续改进多个模型合并方法，在无数据设置下平均准确率提升高达15.9个百分点

Conclusion: Weight Weaving是一种即插即用的正交技术，消除了评估数据需求，显著提高了模型合并方法的性能

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [17] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 本文提出了一种从检索系统角度处理ICD编码分配问题的新方法，将任务重新定义为分类和排序任务，以考虑编码顺序的重要性，在识别高优先级代码方面表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床笔记通常伴随ICD诊断代码序列，正确分配和排序这些代码对医疗诊断和报销至关重要。现有方法将其视为分类任务，忽略了代码顺序的重要性，而代码顺序对不同目的具有关键意义。

Method: 从检索系统角度处理该任务，将其重新定义为分类和排序任务，以考虑代码的顺序信息。

Result: 所提框架在识别高优先级代码方面表现优异，正确排序主要诊断代码的准确率达到47%，而最先进分类器仅为20%。在分类指标上，微平均和宏平均F1分数分别达到0.6065和0.2904，超过了之前最佳模型的0.597和0.2660。

Conclusion: 该研究证明了从检索系统角度处理ICD编码分配问题的有效性，能够更好地考虑代码顺序，在识别高优先级代码和整体分类性能方面均优于传统分类方法。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [18] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 本文提出了一种新的数据保真度损失函数——分布一致性（DC）损失，用于逆问题中的信号恢复。DC损失通过测试观测测量值是否与当前估计所隐含的噪声分布在统计上一致，来替代传统的逐点匹配方法，从而避免对测量噪声的过拟合。


<details>
  <summary>Details</summary>
Motivation: 传统的逆问题解决方案使用均方误差（MSE）或负对数似然等逐点匹配的数据保真度损失函数，这往往导致对噪声的过拟合。作者希望开发一种能够从分布层面评估数据一致性的方法，避免这种过拟合问题。

Method: 引入分布一致性（DC）损失函数，该方法使用基于模型的概率分数对每个测量值进行分布级校准，替代传统的逐点匹配。DC损失与现代正则化器兼容，优化方式与传统损失相同，且无需先验知识即可避免对测量噪声的过拟合。

Result: 在图像去噪应用中，使用DC损失替代MSE损失消除了提前停止的需求，并获得了更高的PSNR；在医学图像重建中，DC损失减少了高度迭代重建中的伪影，并增强了手工正则化的效果。

Conclusion: DC损失为逆问题提供了一个统计基础扎实、性能增强的替代方案，可替代传统的保真度损失函数，特别适用于测量噪声分布已知且测量数据集包含许多独立噪声值的实际逆问题。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [19] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: FedHFT是一个高效的个性化联邦微调框架，通过混合掩码适配器处理资源异构性，使用双层优化方法处理非独立同分布数据，在数据隐私保护下实现多客户端协同微调。


<details>
  <summary>Details</summary>
Motivation: 解决个性化自然语言理解应用中的两个主要挑战：(i) 由于数据机密性或隐私要求导致的有限和/或异构数据；(ii) 边缘设备等参与客户端计算资源的差异性。

Method: 1. 引入混合掩码适配器处理客户端资源异构性；2. 采用基于掩码个性化和客户端聚类的双层优化方法处理非独立同分布数据分布。

Result: 在各种自然语言理解任务中，相比代表性异构联邦学习方法，在数据和资源异构条件下实现了显著的性能和效率提升。

Conclusion: FedHFT框架能够有效解决联邦学习中的数据异构性和资源异构性问题，在保护数据隐私的同时实现高性能的协同微调。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [20] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitNet Distillation (BitDistill) 是一种轻量级方法，可将全精度大语言模型微调为1.58位精度（三元权重{-1, 0, 1}），在特定下游任务上实现强性能，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决全精度大语言模型在特定任务上的计算成本高和内存占用大的问题，同时保持任务特定性能。

Method: 结合三种关键技术：BitNet中的SubLN模块、基于MiniLM的多头注意力蒸馏，以及作为关键预热步骤的持续预训练，以缓解全精度和1.58位模型在特定任务上的性能差距。

Result: 实验结果显示，BitDistill在不同模型规模下实现与全精度模型相当的性能，同时实现高达10倍的内存节省和2.65倍的CPU推理加速。

Conclusion: BitDistill提供了一种高效的方法，能够在保持性能的同时显著降低大语言模型的计算和内存需求，适用于资源受限的环境。

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [21] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: 本文提出了条件Clifford-可操纵核，通过用输入特征场计算的等变表示增强核，解决了CSCNNs核基不完备的问题，在多个PDE预测任务中表现出更好的表达能力。


<details>
  <summary>Details</summary>
Motivation: Clifford-可操纵CNNs的核基不完备，限制了模型的表达能力，需要解决这个问题以提升模型性能。

Method: 提出条件Clifford-可操纵核，用输入特征场计算的等变表示增强核，并通过隐式参数化高效求解等变约束。

Result: 在流体动力学和相对论电动力学等多个PDE预测任务中，该方法持续优于基线方法。

Conclusion: 条件Clifford-可操纵核通过增强核的完备性，显著提升了模型在PDE预测任务中的表达能力。

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [22] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一个新的因果表示学习(CRL)基准，使用高保真模拟视觉数据，包含约20万张图像和300万视频帧，涵盖4个领域的24个子场景，旨在解决现有评估方法在真实性和评估精度之间的困境。


<details>
  <summary>Details</summary>
Motivation: 现有CRL评估方法面临两难：要么使用过于简化的合成数据集，要么依赖真实世界任务的下游性能，缺乏既具有真实视觉复杂性又能访问真实因果生成过程的基准。

Method: 创建包含静态图像生成、动态物理模拟、机器人操作和交通情况分析四个领域的高保真模拟数据集，提供对底层因果结构的灵活访问，允许用户修改配置以符合CRL的不同假设。

Result: 构建了包含20万张图像和300万视频帧的综合测试平台，涵盖从静态到动态、简单到复杂结构、单到多智能体交互的多样化场景，并评估了代表性CRL方法。

Conclusion: 该基准填补了严格评估和现实应用之间的空白，为从业者提供了选择或扩展适当CRL框架的实证见解，以解决能从CRL视角受益的实际问题。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [23] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出了一种名为Neural Effect Search的新方法，直接从数据中发现未知的因果效应，无需预先假设，通过预训练基础模型和稀疏自编码器将非结构化数据转化为有意义表示，并在生态学实验中成功实现了首个无监督因果效应识别。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验依赖手工假设和昂贵分析，限制了因果效应估计的规模，可能导致固守流行但不完整的假设。

Method: 使用预训练基础模型将试验中的非结构化数据转化为有意义表示，通过稀疏自编码器进行解释，并引入Neural Effect Search递归程序通过渐进分层解决多重检验和效应纠缠问题。

Result: 在半合成实验中验证了算法的鲁棒性，在实验生态学背景下首次成功实现了真实世界科学试验中的无监督因果效应识别。

Conclusion: 该方法能够直接从数据中发现未知的因果效应，突破了传统随机对照试验的限制，为大规模因果发现提供了新途径。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [24] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR: 提出TENDE方法，利用基于分数的扩散模型通过条件互信息估计传递熵，解决了现有方法维度灾难、分布假设限制和数据量需求大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有传递熵估计方法存在维度灾难、需要限制性分布假设或需要指数级大数据集才能可靠收敛的问题，限制了其在实际应用中的有效性。

Method: TENDE方法利用基于分数的扩散模型学习相关条件分布的概率密度函数，通过条件互信息来估计传递熵，对基础数据生成过程做出最小假设。

Result: 在合成基准测试和真实数据上，TENDE相比现有神经估计器和其他最先进方法表现出更高的准确性和鲁棒性。

Conclusion: TENDE提供了一种灵活、可扩展的传递熵估计方法，突破了传统方法的局限性，在神经科学、金融和复杂系统分析等领域具有广泛应用前景。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [25] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 本文研究了一个双边市场平台，通过在线学习定价策略来最大化利润，同时控制队列长度。提出了一种新颖的定价策略，在未知供需曲线的情况下实现了近乎最优的性能权衡。


<details>
  <summary>Details</summary>
Motivation: 在双边市场中，平台需要设计定价和匹配算法来最大化利润，同时保持合理的队列长度。由于实际中供需曲线可能未知，需要开发能够在未知环境下有效运作的在线学习策略。

Method: 提出了一种基于在线学习的定价策略，包含动态组件和概率组件。动态组件优化低遗憾和小队列长度之间的权衡，概率组件解决获取有用样本进行快速学习与保持小队列长度之间的张力。

Result: 证明了在γ∈(0,1/6]范围内，实现了三个性能指标的权衡：Õ(T^(1-γ))遗憾、Õ(T^(γ/2))平均队列长度和Õ(T^γ)最大队列长度。在允许的γ范围内，这种遗憾与平均队列长度的权衡是最优的。

Conclusion: 所提出的策略在未知供需曲线的情况下，能够实现近乎最优的性能，显著改进了现有结果，为双边市场平台的动态定价和队列管理提供了有效的解决方案。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [26] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 本文综述了利用预训练扩散模型结合蒙特卡洛方法解决贝叶斯逆问题的方法，无需额外训练，主要通过扭曲机制引导模拟后验分布。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出色，最近显示出作为先验解决贝叶斯逆问题的潜力，本文旨在系统梳理相关方法。

Method: 使用预训练扩散模型作为先验，结合蒙特卡洛方法，通过扭曲扩散过程中的中间分布来引导模拟后验分布。

Result: 展示了多种蒙特卡洛方法如何辅助从扭曲分布中采样，形成了一套完整的贝叶斯逆问题求解框架。

Conclusion: 预训练扩散模型与蒙特卡洛方法的结合为贝叶斯逆问题提供了有效的无训练解决方案，扭曲机制是实现这一目标的关键技术。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [27] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/abs/2505.24842)
*Harsh Chaudhari,Jamie Hayes,Matthew Jagielski,Ilia Shumailov,Milad Nasr,Alina Oprea*

Main category: cs.LG

TL;DR: 模型蒸馏存在安全漏洞，攻击者可通过少量数据投毒在教师模型中注入偏见，这些偏见会在学生模型中传播并显著放大。仅需25个中毒样本（0.25%投毒率），学生模型在目标场景中76.9%时间会产生偏见响应，比教师模型的69.4%更高。现有防御措施对此类攻击效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着模型蒸馏技术的广泛应用，部署小型语言模型的安全性问题日益突出。本研究旨在揭示蒸馏模型对训练过程中偏见内容注入攻击的脆弱性，以及这些偏见如何在师生模型间传播放大。

Method: 提出两种传播模式：无目标传播（偏见影响多个任务）和目标传播（针对特定任务同时保持其他任务正常行为）。通过仅25个中毒样本（0.25%投毒率）进行数据投毒，在六种偏见类型（定向广告、钓鱼链接、叙事操纵、不安全编码实践等）上验证攻击效果，涵盖多种蒸馏方法和文本代码生成模态。

Result: 在目标传播场景中，学生模型产生偏见响应的比例达76.9%，高于教师模型的69.4%。在无目标传播中，学生模型在未见任务上出现对抗性偏见的频率是教师模型的6-29倍。现有防御措施（困惑度过滤、偏见检测系统、基于LLM的自动评分框架）对这些攻击效果有限。

Conclusion: 蒸馏模型存在严重安全漏洞，偏见可在师生模型间传播放大。需要专门的安全防护措施，研究提出了构建有效对抗性偏见缓解策略的实用设计原则。

Abstract: Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [28] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: 本文首次系统分析了基于大语言模型的提示优化中的投毒风险，发现基于反馈的攻击比注入查询更危险，提出了无需访问奖励模型的假奖励攻击，并设计了一种轻量级高亮防御方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型系统现在支撑着日常AI应用，其性能通常依赖于精心设计的提示。基于LLM的提示优化器通过迭代优化来减少这种努力，但这一优化阶段的安全性尚未得到充分研究。

Method: 使用HarmBench进行系统性分析，发现系统对操纵反馈的脆弱性远高于注入查询。提出了简单的假奖励攻击方法，无需访问奖励模型，并设计了一种轻量级高亮防御机制。

Result: 基于反馈的攻击将攻击成功率提高了ΔASR=0.48。假奖励攻击显著增加了脆弱性，而高亮防御将假奖励的ΔASR从0.23降低到0.07，且不降低实用性。

Conclusion: 提示优化管道应被视为一级攻击面，需要为反馈渠道和优化框架建立更强的安全保障措施。

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [29] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文系统评估了多种先进的损失函数（点对点、成对、列表式）在S&P 500数据上对Transformer模型股票排名能力的影响，为基于排名的投资组合选择提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 量化交易策略依赖准确的股票排名来识别盈利投资。虽然Transformer模型在理解金融时间序列方面很有前景，但不同训练损失函数如何影响其股票排名能力尚未完全了解。标准损失函数追求简单预测准确性，往往不足够，因为它们不能直接教导模型学习股票收益的正确顺序。

Method: 在S&P 500数据上，系统评估了包括点对点、成对、列表式在内的多种先进损失函数，用于每日股票收益预测，以促进基于排名的投资组合选择。重点关注每种损失函数如何影响模型识别资产间盈利相对排序的能力。

Result: 研究提供了一个全面的基准，揭示了不同损失函数如何影响模型学习对投资组合选择至关重要的横截面和时间模式的能力。

Conclusion: 这项工作为优化基于排名的交易策略提供了实用指导，填补了先进排名损失函数在金融收益排名应用中的系统性比较空白。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [30] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 本文综述了传统数据集表征方法的局限性，并探讨了基于张量的方法作为更强大的替代方案，能够提供更好的可解释性和洞察力。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习数据集表征方法（统计、结构和基于模型的分析）往往无法提供深度理解和创新所需的洞察力与可解释性。

Method: 调查当前最先进的传统数据分析技术，分析其局限性，并讨论各种基于张量的方法如何提供更强大的数据集表征替代方案。

Result: 通过示例说明张量方法如何揭示细微的数据特征，提供增强的可解释性和可操作智能。

Conclusion: 提倡采用基于张量的表征方法，有望在理解复杂数据集方面实现飞跃，为智能、可解释的数据驱动发现铺平道路。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [31] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [32] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个生产部署的多智能体标注框架，通过可配置的多智能体协作解决金融服务中的大规模标注积压问题，在摩根大通成功消除100万条语音积压，平均与人工标注者达成86%一致，每年节省5000+小时人工标注工作。


<details>
  <summary>Details</summary>
Motivation: 解决金融服务中数百万客户语音需要准确分类的标注积压挑战，传统方法效率低下且成本高昂。

Method: 结合专用智能体与结构化推理和基于法官的共识机制，支持通过配置而非代码更改定义自定义标注类型，实现动态任务适应。

Result: 消除100万条语音积压，平均86%与人工标注一致，85%高置信度标注，13.8% Top-1准确率提升，15.1% Top-5准确率提升，16.9% F1分数提升。

Conclusion: 弥合理论多智能体系统与实际企业部署之间的差距，为面临类似标注挑战的组织提供蓝图。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [33] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA框架通过对比学习在扩散嵌入中组织潜在空间，使潜在几何与系统动态对齐，支持非线性轨迹遍历以实现可控生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成方面表现出色，但其潜在空间没有为可解释控制进行显式组织，需要改进潜在空间结构以实现更好的可控性。

Method: 应用对比学习在扩散嵌入中，通过对比目标恢复更解缠和结构化的表示，组织扩散潜在空间使遍历方向反映底层动态因素。

Result: 在流体动力学、神经钙成像、治疗性神经刺激和面部表情等基准测试中，ConDA相比线性遍历和基于条件的基线方法，产生具有改进可控性的可解释潜在表示。

Conclusion: 扩散潜在编码包含动态相关结构，但利用这种结构需要潜在组织并沿着潜在流形进行遍历。

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [34] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 本文对分子性质预测中的核方法进行了首次全面的谱分析，发现更丰富的谱特征并不总是提高预测精度，甚至在某些情况下与性能呈负相关。


<details>
  <summary>Details</summary>
Motivation: 虽然深度模型在分子性质预测中达到最先进精度，但核方法因其在低数据环境下的鲁棒性和透明理论基础仍被广泛使用。然而，分子核的系统谱分析研究稀缺。

Method: 在QM9数据集上对分子指纹、预训练transformer、全局和局部3D表示等七种分子性质进行核岭回归的谱分析，使用四种谱度量指标，并实现截断核来研究谱与预测性能的关系。

Result: 令人惊讶的是，更丰富的谱特征并不一致提高精度。对于transformer和局部3D表示，谱丰富度甚至与性能呈负相关。在许多核中，仅保留前2%的特征值就能恢复几乎所有性能。

Conclusion: 研究结果挑战了"更丰富的谱带来更好泛化"的常见启发式，揭示了表示、核特征和预测性能之间的微妙关系，对数据有限的科学和现实任务中的核与自监督学习方法评估具有指导意义。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [35] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: GenCluster是一个可扩展的测试时计算框架，使用开源模型实现了IOI金牌级别的性能，通过大规模生成、行为聚类、排名和轮询提交策略来高效探索解决方案空间。


<details>
  <summary>Details</summary>
Motivation: 竞争性编程已成为评估大型语言模型推理和问题解决能力的重要基准，IOI作为最负盛名的年度竞赛，是衡量人类和AI编程能力的关键基准。虽然一些专有模型声称达到IOI金牌水平，但使用开源模型实现类似性能仍面临重大挑战。

Method: GenCluster结合大规模生成、行为聚类、排名和轮询提交策略，在有限验证预算下高效探索多样化的解决方案空间。该方法性能随可用计算资源一致扩展。

Result: 实验表明GenCluster能够使用开源模型gpt-oss-120b首次在IOI 2025中获得金牌，为LLM推理的透明和可复现评估设立了新基准。

Conclusion: GenCluster框架成功缩小了开源和闭源系统之间的差距，证明了使用开源模型在竞争性编程中达到顶尖水平的可行性，推动了透明和可复现的AI评估标准。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [36] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为DR-RPO的在线策略优化算法，用于解决强化学习中的分布偏移问题。该算法结合了参考策略正则化和线性函数逼近，在鲁棒马尔可夫决策过程中实现了次线性遗憾和多项式次优性边界。


<details>
  <summary>Details</summary>
Motivation: 强化学习在分布偏移下的决策是一个核心挑战，传统策略优化方法在鲁棒强化学习中缺乏理论和实证探索。本文旨在填补这一空白，研究在线设置下的鲁棒策略优化问题。

Method: 提出了DR-RPO算法，这是一种无模型的在线策略优化方法。算法结合了参考策略正则化，将RMDP转化为在转移和策略上双重约束的变体。采用d-矩形线性MDP公式，结合线性函数逼近和上置信界奖励进行乐观探索。

Result: 理论分析表明策略优化可以在鲁棒强化学习中实现多项式次优性边界和样本效率，与基于价值的方法性能相当。实证结果在多个领域验证了理论的正确性和DR-RPO的鲁棒性。

Conclusion: DR-RPO算法成功地将策略优化方法扩展到鲁棒强化学习领域，证明了策略优化方法在分布偏移环境下同样可以实现高效的样本利用和鲁棒性能。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [37] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 该论文对通用型和专用型时间序列基础模型在PPG信号分析中的性能进行了全面基准测试，通过51个任务评估了7个维度，发现在全调优场景下专用模型胜率得分高出27%。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在时间序列分析中的应用日益增多，特别是生理信号处理领域，目前大多数时间序列基础模型都是专用模型，而近期出现了通用型模型。本文旨在系统比较这两种模型在PPG信号分析中的性能差异。

Method: 通过包含51个任务的综合测试套件，覆盖心脏状态评估、实验室值估计和跨模态推理，从胜率得分、平均性能、特征质量、调优增益、性能方差、可迁移性和可扩展性等7个维度评估模型性能。

Result: 在全调优场景下，专用模型实现了27%更高的胜率得分。研究还提供了关于泛化性、公平性、注意力可视化和训练数据选择重要性的进一步分析。

Conclusion: 研究为不同下游场景中通用型和专用型时间序列基础模型的优势和局限性提供了全面理解，表明专用模型在特定任务上具有明显优势，同时强调了训练数据选择的重要性。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [38] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: CAST是一个无需探针的框架，通过直接估计变换矩阵和光谱分析来理解Transformer层的功能，为现有可解释性方法提供补充视角。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然取得了显著成功，但其内部机制仍然不透明。为了弥补这一局限性，需要开发新的可解释性方法来理解Transformer层的功能。

Method: 使用Moore-Penrose伪逆估计每层的实际变换矩阵，并应用光谱分析，包含六个可解释的指标来表征层行为。

Result: 分析揭示了编码器模型和解码器模型之间的不同行为：解码器模型表现出压缩-扩展循环，而编码器模型保持一致的高秩处理。核分析进一步显示了层间的功能关系模式。

Conclusion: CAST框架为理解Transformer模型内部机制提供了有价值的补充视角，揭示了不同模型架构的独特行为特征和功能分区模式。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [39] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出了一种用于扩散模型的非参数化数据归因方法，通过图像块级相似性来衡量训练样本对生成输出的影响，无需模型梯度或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型归因方法需要访问模型梯度或重新训练，限制了在专有或大规模设置中的适用性，因此需要开发不依赖模型内部信息的归因方法。

Method: 基于最优得分函数的解析形式，通过计算生成图像与训练图像之间的块级相似性来测量影响，支持多尺度表示，并通过基于卷积的加速保持计算效率。

Result: 实验表明该方法实现了强大的归因性能，与基于梯度的方法接近，并显著优于现有的非参数化基线方法。

Conclusion: 该方法提供了一种高效、可解释且不依赖模型内部信息的归因框架，能够揭示训练数据与输出之间的内在关系。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [40] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAREBench是一个早期事件预测基准测试，评估多模态输入下的预测准确性和时间稳定性，发现现有方法特别是LLM在准确性和稳定性联合优化方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前早期事件预测系统基准测试忽略风险评分的稳定性，且主要评估表格输入，缺乏对风险轨迹行为的测试。

Method: 引入CAREBench基准，使用多模态输入（表格EHR、ECG波形和临床文本），提出基于局部Lipschitz常数的稳定性指标量化短期风险变异性。

Result: 在六个预测任务中，现有方法特别是LLM难以同时优化准确性和稳定性，在高精度操作点召回率表现较差。

Conclusion: 需要开发能够产生证据对齐、稳定轨迹的模型，以在连续监测环境中赢得临床医生的信任。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [41] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 提出了一种结合谱残差方法和储层计算的SR-RC模型，用于时间序列异常检测，在不牺牲学习效率的情况下提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 储层计算在边缘AI应用中具有优势，但单独使用可能需要过大的储层规模。注意力机制能提高精度但计算量大。需要一种既能提高异常检测性能又不牺牲学习效率的方法。

Method: 将谱残差方法（一种无学习的自底向上注意力机制）与储层计算集成，构建SR-RC模型。谱残差方法适合硬件实现，与储层计算兼容。

Result: 在基准任务和真实世界时间序列数据集上，SR-RC优于传统储层计算和基于谱残差方法提取值的逻辑回归模型。

Conclusion: SR-RC为在边缘AI中部署储层计算进行时间序列异常检测提供了实用方向，两种方法都适合硬件实现。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [42] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 本文提出了主动可观测马尔可夫决策过程（AOMDP），其中智能体不仅选择控制动作，还决定是否测量潜在状态。测量动作可以揭示真实状态但可能对环境产生负面延迟影响。研究表明这种减少不确定性的方法可以提高样本效率并增加最优策略的价值。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的强化学习应用中，测量状态可能成本高昂且对后续结果产生负面影响。需要一种方法让智能体能够主动决定何时进行状态测量，以平衡测量成本和信息收益。

Method: 将AOMDP建模为周期性部分可观测MDP，提出基于信念状态的在线RL算法。为了近似信念状态，进一步提出顺序蒙特卡洛方法来联合近似未知静态环境参数和未观测潜在状态的后验分布。

Result: 在数字健康应用中评估了所提出的算法，智能体决定何时提供数字干预以及何时通过调查评估用户健康状态。研究表明减少不确定性可以证明提高样本效率并增加最优策略价值。

Conclusion: AOMDP框架为处理状态测量成本问题提供了有效解决方案，通过主动测量决策平衡了信息获取和测量成本之间的关系，在数字健康等实际应用中具有重要价值。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [43] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: LLM-ERM是一个提出-验证框架，使用LLM引导的程序搜索替代穷举枚举，在少量样本下有效学习短程序，而基于梯度的训练方法需要指数级样本。


<details>
  <summary>Details</summary>
Motivation: 解决程序学习中样本效率和计算可行性之间的差距，传统方法要么计算成本高（穷举枚举），要么样本效率低（梯度训练）。

Method: 使用预训练推理增强的LLM生成k个候选程序，在保留数据上编译验证并选择最佳假设，无需反馈、自适应或梯度。

Result: LLM-ERM仅需200个样本就能解决奇偶变体、模式匹配和素数测试等任务，而SGD训练的transformer即使有100,000个样本也会过拟合。

Conclusion: 语言引导的程序合成恢复了有限类ERM的统计效率，同时保持计算可行性，为学习梯度训练无法处理的简洁假设提供了实用途径。

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [44] [DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis](https://arxiv.org/abs/2510.14336)
*Shruti Sarika Chakraborty,Peter Minary*

Main category: cs.LG

TL;DR: 本文提出DARTS-GT，通过非对称注意力机制和可微分架构搜索来改进图Transformer，实现了深度异质性架构，并开发了首个图Transformer的定量可解释性框架。


<details>
  <summary>Details</summary>
Motivation: 当前图Transformer存在设计僵化、缺乏量化可解释性的问题，固定GNN类型限制了深度特定组件选择的潜力，复杂架构难以区分性能提升是来自有意义的模式还是虚假相关性。

Method: 重新设计图Transformer注意力机制，将结构编码与特征表示解耦：查询来自节点特征，键和值来自GNN变换。使用可微分架构搜索（DARTS）为每层选择最优GNN算子，实现Transformer注意力内部的深度异质性。开发基于因果消融的定量可解释性框架。

Result: 在八个基准测试中，DARTS-GT在四个数据集上达到最先进水平，在其他数据集上保持竞争力。发现的架构揭示了数据集特定模式。可解释性分析显示视觉注意力显著性与因果重要性并不总是相关。

Conclusion: DARTS-GT发现的异质性架构始终产生比基线更可解释的模型，证明图Transformer无需在性能和可解释性之间做出选择。

Abstract: Graph Transformers (GTs) have emerged as powerful architectures for
graph-structured data, yet remain constrained by rigid designs and lack
quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN
types across all layers, missing potential benefits of depth-specific component
selection, while their complex architectures become opaque where performance
gains cannot be distinguished between meaningful patterns and spurious
correlations. We redesign GT attention through asymmetry, decoupling structural
encoding from feature representation: queries derive from node features while
keys and values come from GNN transformations. Within this framework, we use
Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at
each layer, enabling depth-wise heterogeneity inside transformer attention
itself (DARTS-GT). To understand discovered architectures, we develop the first
quantitative interpretability framework for GTs through causal ablation. Our
metrics (Head-deviation, Specialization, and Focus), identify which heads and
nodes drive predictions while enabling model comparison. Experiments across
eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while
remaining competitive on others, with discovered architectures revealing
dataset-specific patterns. Our interpretability analysis reveals that visual
attention salience and causal importance do not always correlate, indicating
widely used visualization approaches may miss components that actually matter.
Crucially, heterogeneous architectures found by DARTS-GT consistently produced
more interpretable models than baselines, establishing that Graph Transformers
need not choose between performance and interpretability.

</details>


### [45] [Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis](https://arxiv.org/abs/2510.14342)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 该论文提出了一种基于jet bundles和Weil代数的自动微分几何框架，将反向模式AD解释为余切拉回，泰勒模式对应Weil代数中的求值，并推导了正确性、稳定性和复杂性的简洁陈述。


<details>
  <summary>Details</summary>
Motivation: 从微分几何的角度重新解释自动微分理论，为深度学习和科学计算中开发保持结构的微分方法提供理论基础。

Method: 使用jet bundles和Weil代数的几何框架，将反向模式AD形式化为余切拉回操作，泰勒模式对应Weil代数中的求值。

Result: 推导了反向模式的函子恒等式、高阶导数的代数精确性以及截断误差的显式界，并证明张量化Weil代数可以一次性计算所有混合导数，成本与代数维度线性相关。

Conclusion: 该框架通过微分几何的视角解释AD理论，为开发结构保持的微分方法奠定了基础，避免了嵌套JVP/VJP调度的组合爆炸问题。

Abstract: We present a geometric formulation of automatic differentiation (AD) using
jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,
while Taylor-mode corresponds to evaluation in a Weil algebra. From these
principles, we derive concise statements on correctness, stability, and
complexity: a functorial identity for reverse-mode, algebraic exactness of
higher-order derivatives, and explicit bounds on truncation error. We further
show that tensorized Weil algebras permit one-pass computation of all mixed
derivatives with cost linear in the algebra dimension, avoiding the
combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD
theory through the lens of differential geometry and offers a foundation for
developing structure-preserving differentiation methods in deep learning and
scientific computing. Code and examples are available at
https://git.nilu.no/geometric-ad/jet-weil-ad.

</details>


### [46] [Revisit Modality Imbalance at the Decision Layer](https://arxiv.org/abs/2510.14411)
*Xiaoyu Ma,Hao Chen*

Main category: cs.LG

TL;DR: 本文揭示了多模态学习中的模态不平衡问题不仅发生在表示学习阶段，还显著体现在决策层。研究发现即使经过充分预训练和平衡优化，模型仍会系统性地偏向某些模态（如音频）。这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态引起。作者建议未来多模态系统应在决策层引入自适应权重分配机制。


<details>
  <summary>Details</summary>
Motivation: 多模态学习虽然能整合不同模态信息提升性能，但常面临模态不平衡问题，即主导模态在联合优化过程中会压制较弱模态。本文旨在揭示这种不平衡不仅存在于表示学习阶段，更显著地体现在决策层。

Method: 在音频-视觉数据集（CREMAD和Kinetic-Sounds）上进行实验，分析模型在充分预训练和平衡优化后的表现。通过深入分析特征空间和决策权重分布的内在差异来探究模态偏见的根源。

Result: 实验表明，即使经过广泛预训练和平衡优化，模型仍然表现出对某些模态（如音频）的系统性偏见。这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态引起。在融合阶段聚合未校准的模态输出会导致决策层权重偏差，阻碍较弱模态的有效贡献。

Conclusion: 未来多模态系统应更加关注在决策层引入自适应权重分配机制，根据每个模态的能力实现相对平衡的权重分配，以解决模态不平衡问题。

Abstract: Multimodal learning integrates information from different modalities to
enhance model performance, yet it often suffers from modality imbalance, where
dominant modalities overshadow weaker ones during joint optimization. This
paper reveals that such an imbalance not only occurs during representation
learning but also manifests significantly at the decision layer. Experiments on
audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after
extensive pretraining and balanced optimization, models still exhibit
systematic bias toward certain modalities, such as audio. Further analysis
demonstrates that this bias originates from intrinsic disparities in
feature-space and decision-weight distributions rather than from optimization
dynamics alone. We argue that aggregating uncalibrated modality outputs at the
fusion stage leads to biased decision-layer weighting, hindering weaker
modalities from contributing effectively. To address this, we propose that
future multimodal systems should focus more on incorporate adaptive weight
allocation mechanisms at the decision layer, enabling relative balanced
according to the capabilities of each modality.

</details>


### [47] [Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods](https://arxiv.org/abs/2510.14419)
*Tapio Pahikkala,Riikka Numminen,Parisa Movahedi,Napsu Karmitsa,Antti Airola*

Main category: cs.LG

TL;DR: 本文提出了交互一致性指数（IC-index）来评估药物-靶点亲和力预测中交互方向预测的性能，补充现有预测性能评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有药物-靶点亲和力预测方法主要关注预测准确度，但缺乏对交互效应的评估。正确捕捉交互效应对于药物分配决策至关重要。

Method: 引入IC-index评估交互方向预测性能，分析预测器对交互的捕捉能力，并研究机器学习算法的置换等变性对交互预测的影响。

Result: IC-index对无法捕捉交互的预测器具有不变性，置换等变算法在遇到未见药物或靶点时无法捕捉交互，需通过侧信息来改善。

Conclusion: IC-index作为补充指标能有效评估交互预测性能，在多个生物医学数据集上的实验验证了其有效性。

Abstract: Consider two sets of entities and their members' mutual affinity values, say
drug-target affinities (DTA). Drugs and targets are said to interact in their
effects on DTAs if drug's effect on it depends on the target. Presence of
interaction implies that assigning a drug to a target and another drug to
another target does not provide the same aggregate DTA as the reversed
assignment would provide. Accordingly, correctly capturing interactions enables
better decision-making, for example, in allocation of limited numbers of drug
doses to their best matching targets. Learning to predict DTAs is popularly
done from either solely from known DTAs or together with side information on
the entities, such as chemical structures of drugs and targets. In this paper,
we introduce interaction directions' prediction performance estimator we call
interaction concordance index (IC-index), for both fixed predictors and machine
learning algorithms aimed for inferring them. IC-index complements the
popularly used DTA prediction performance estimators by evaluating the ratio of
correctly predicted directions of interaction effects in data. First, we show
the invariance of IC-index on predictors unable to capture interactions.
Secondly, we show that learning algorithm's permutation equivariance regarding
drug and target identities implies its inability to capture interactions when
either drug, target or both are unseen during training. In practical
applications, this equivariance is remedied via incorporation of appropriate
side information on drugs and targets. We make a comprehensive empirical
evaluation over several biomedical interaction data sets with various
state-of-the-art machine learning algorithms. The experiments demonstrate how
different types of affinity strength prediction methods perform in terms of
IC-index complementing existing prediction performance estimators.

</details>


### [48] [MergeMoE: Efficient Compression of MoE Models via Expert Output Merging](https://arxiv.org/abs/2510.14436)
*Ruijie Miao,Yilun Yao,Zihan Wang,Zhiming Wang,Bairen Yi,LingJun Liu,Yikai Zhao,Tong Yang*

Main category: cs.LG

TL;DR: 本文提出MergeMoE方法，通过数学优化构建压缩矩阵来压缩MoE模型，在相同压缩比下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: MoE技术在扩展模型规模方面很有前景，但其巨大的内存开销使得压缩成为重要研究方向。专家合并是一种新提出的MoE模型压缩技术。

Method: 从专家输出合并的角度重新解释专家合并过程，将其视为在前向计算中插入额外矩阵，从而建立优化公式。基于此分析开发MergeMoE方法，利用数学优化构建压缩矩阵。

Result: 在多个MoE模型上评估MergeMoE，结果显示该方法在相同压缩比下始终优于基线方法。

Conclusion: 从专家输出合并角度分析专家合并过程，并提出基于数学优化的MergeMoE方法，有效压缩MoE模型并优于现有方法。

Abstract: The Mixture-of-Experts (MoE) technique has proven to be a promising solution
to efficiently scale the model size, which has been widely applied in recent
LLM advancements. However, the substantial memory overhead of MoE models has
made their compression an important research direction. In this work, we
provide a theoretical analysis of expert merging, a recently proposed technique
for compressing MoE models. Rather than interpreting expert merging from the
conventional perspective of parameter aggregation, we approach it from the
perspective of merging experts' outputs. Our key insight is that the merging
process can be interpreted as inserting additional matrices into the forward
computation, which naturally leads to an optimization formulation. Building on
this analysis, we introduce MergeMoE, a method that leverages mathematical
optimization to construct the compression matrices. We evaluate MergeMoE on
multiple MoE models and show that our algorithm consistently outperforms the
baselines with the same compression ratios.

</details>


### [49] [Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints](https://arxiv.org/abs/2510.14449)
*Jahidul Arafat,Fariha Tasmin,Md Kaosar Uddin,Sanjaya Poudel,Eftakhar Ahmed Arnob*

Main category: cs.LG

TL;DR: 本文对UCI葡萄酒数据集进行多分类研究，比较手动梯度下降与scikit-learn优化器的性能，分析L1正则化对特征稀疏性的影响，提出最优5特征子集实现62%复杂度降低和92-94%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决葡萄酒多分类中模型准确性、特征维度和可解释性之间的权衡问题，为分析化学中的生产部署提供实用指导。

Method: 使用UCI葡萄酒数据集（178个样本，3个品种，13个化学特征），比较手动梯度下降实现与scikit-learn优化求解器，量化L1正则化对特征稀疏性的影响。

Result: 手动梯度下降达到92.59%平均测试准确率，scikit-learn提供24倍训练加速和98.15%准确率。L1正则化实现54-69%特征减少，仅损失4.63%准确率。最优5特征子集实现62%复杂度降低，估计92-94%准确率，每样本节省80美元成本，时间减少56%。

Conclusion: 研究为资源受限环境中的化学分析提供了实用指南，在全面化学分析与针对性特征测量之间找到平衡，实现实时质量控制。

Abstract: Multi-class wine classification presents fundamental trade-offs between model
accuracy, feature dimensionality, and interpretability - critical factors for
production deployment in analytical chemistry. This paper presents a
comprehensive empirical study of One-vs-Rest logistic regression on the UCI
Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing
from-scratch gradient descent implementation against scikit-learn's optimized
solvers and quantifying L1 regularization effects on feature sparsity. Manual
gradient descent achieves 92.59 percent mean test accuracy with smooth
convergence, validating theoretical foundations, though scikit-learn provides
24x training speedup and 98.15 percent accuracy. Class-specific analysis
reveals distinct chemical signatures with heterogeneous patterns where color
intensity varies dramatically (0.31 to 16.50) across cultivars. L1
regularization produces 54-69 percent feature reduction with only 4.63 percent
accuracy decrease, demonstrating favorable interpretability-performance
trade-offs. We propose an optimal 5-feature subset achieving 62 percent
complexity reduction with estimated 92-94 percent accuracy, enabling
cost-effective deployment with 80 dollars savings per sample and 56 percent
time reduction. Statistical validation confirms robust generalization with
sub-2ms prediction latency suitable for real-time quality control. Our findings
provide actionable guidelines for practitioners balancing comprehensive
chemical analysis against targeted feature measurement in resource-constrained
environments.

</details>


### [50] [From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?](https://arxiv.org/abs/2510.14488)
*Sujai Hiremath,Dominik Janzing,Philipp Faller,Patrick Blöbaum,Elke Kirschbaum,Shiva Prasad Kasiviswanathan,Kyra Gan*

Main category: cs.LG

TL;DR: 提出了Guess2Graph框架，使用专家猜测来指导统计测试序列而非替代测试，保持统计一致性的同时提升性能。开发了PC-Guess和gPC-Guess两个变体，后者在专家质量较高时能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 因果发现算法在样本有限时表现不佳，现有方法需要完美预测或不确定性估计，在实际应用中不可靠。

Method: 提出Guess2Graph框架，将专家猜测作为统计测试的指导而非约束。开发PC-Guess和gPC-Guess两个具体实现，后者是学习增强变体。

Result: 理论上两种方法在专家错误时仍保持正确性，gPC-Guess在专家优于随机时有限样本性能更优。实证显示两者随专家准确性单调改进，gPC-Guess获得显著更强增益。

Conclusion: Guess2Graph框架通过智能整合专家知识有效提升因果发现性能，同时保持统计鲁棒性。

Abstract: Causal discovery algorithms often perform poorly with limited samples. While
integrating expert knowledge (including from LLMs) as constraints promises to
improve performance, guarantees for existing methods require perfect
predictions or uncertainty estimates, making them unreliable for practical use.
We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide
the sequence of statistical tests rather than replacing them. This maintains
statistical consistency while enabling performance improvements. We develop two
instantiations of G2G: PC-Guess, which augments the PC algorithm, and
gPC-Guess, a learning-augmented variant designed to better leverage
high-quality expert input. Theoretically, both preserve correctness regardless
of expert error, with gPC-Guess provably outperforming its non-augmented
counterpart in finite samples when experts are "better than random."
Empirically, both show monotonic improvement with expert accuracy, with
gPC-Guess achieving significantly stronger gains.

</details>


### [51] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 提出可逆学习框架，通过可逆性度量Phi和选择性状态回滚操作，提高基于价值的强化学习在部分不可逆环境中的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于价值的强化学习智能体容易产生价值高估和在部分不可逆环境中不稳定的问题。

Method: 引入在线状态-动作可逆性度量Phi，量化在固定时间窗口内返回先前状态的可能性；结合选择性回滚操作，当动作预期回报显著低于瞬时估计值时，智能体被惩罚并返回前一个状态。

Result: 在CliffWalking v0环境中，灾难性跌落减少超过99.8%，平均回合回报提高55%；在Taxi v3环境中，非法动作抑制率≥99.9%，累积奖励提升65.7%，两个环境中的奖励方差均显著降低。

Conclusion: 回滚机制是实现安全和性能提升的关键组件，为安全和可靠的顺序决策迈出了稳健的一步。

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [52] [Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective](https://arxiv.org/abs/2510.14510)
*Xingjian Wu,Xiangfei Qiu,Hanyin Cheng,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种选择性表示空间（SRS）模块，通过可学习的选择性分块和动态重组技术，自适应地选择和重排时间序列中的信息块，以增强基于分块的时间序列预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统分块技术将时间序列划分为相邻块，导致固定的表示空间，限制了表示的丰富性。作者旨在构建一个选择性表示空间，灵活包含最具信息量的块以提升预测性能。

Method: 提出SRS模块，包含可学习的选择性分块和动态重组技术，能够自适应地从上下文时间序列中选择和重排块。基于SRS构建了SRSNet模型，仅包含SRS模块和MLP头部。

Result: SRSNet在多个领域的真实世界数据集上实现了最先进的性能。SRS模块作为即插即用组件，也能提升现有基于分块模型的性能。

Conclusion: 选择性表示空间方法能够有效增强时间序列预测中基于分块模型的表示能力，提升预测性能，且具有良好的通用性。

Abstract: Time Series Forecasting has made significant progress with the help of
Patching technique, which partitions time series into multiple patches to
effectively retain contextual semantic information into a representation space
beneficial for modeling long-term dependencies. However, conventional patching
partitions a time series into adjacent patches, which causes a fixed
representation space, thus resulting in insufficiently expressful
representations. In this paper, we pioneer the exploration of constructing a
selective representation space to flexibly include the most informative patches
for forecasting. Specifically, we propose the Selective Representation Space
(SRS) module, which utilizes the learnable Selective Patching and Dynamic
Reassembly techniques to adaptively select and shuffle the patches from the
contextual time series, aiming at fully exploiting the information of
contextual time series to enhance the forecasting performance of patch-based
models. To demonstrate the effectiveness of SRS module, we propose a simple yet
effective SRSNet consisting of SRS and an MLP head, which achieves
state-of-the-art performance on real-world datasets from multiple domains.
Furthermore, as a novel plugin-and-play module, SRS can also enhance the
performance of existing patch-based models. The resources are available at
https://github.com/decisionintelligence/SRSNet.

</details>


### [53] [On the Identifiability of Tensor Ranks via Prior Predictive Matching](https://arxiv.org/abs/2510.14523)
*Eliezer da Silva,Arto Klami,Diego Mesquita,Iñigo Urteaga*

Main category: cs.LG

TL;DR: 本文提出了一种基于先验预测矩匹配的严格方法来确定概率张量模型中的秩可识别性，将矩匹配条件转化为对数线性方程组，并建立了秩可识别性与该系统可解性之间的等价关系。


<details>
  <summary>Details</summary>
Motivation: 张量分解中潜在维度（秩）的选择是一个核心挑战，通常依赖于启发式方法，需要一种更严格的方法来确定秩的可识别性。

Method: 将矩匹配条件转化为关于边际矩、先验超参数和秩的对数线性方程组，通过系统可解性判断秩可识别性，并针对四种基础张量模型进行分析。

Result: 证明PARAFAC/CP模型、张量链模型和张量环模型的秩是可识别的，而Tucker模型的秩由于对称拓扑导致系统欠定而不可识别；对于可识别模型，推导了仅基于观测数据矩的显式闭式秩估计器。

Conclusion: 该方法为张量分解中的秩选择提供了理论基础，并针对不同张量模型结构给出了秩可识别性的明确结论，同时提供了实用的秩估计方法。

Abstract: Selecting the latent dimensions (ranks) in tensor factorization is a central
challenge that often relies on heuristic methods. This paper introduces a
rigorous approach to determine rank identifiability in probabilistic tensor
models, based on prior predictive moment matching. We transform a set of moment
matching conditions into a log-linear system of equations in terms of marginal
moments, prior hyperparameters, and ranks; establishing an equivalence between
rank identifiability and the solvability of such system. We apply this
framework to four foundational tensor-models, demonstrating that the linear
structure of the PARAFAC/CP model, the chain structure of the Tensor Train
model, and the closed-loop structure of the Tensor Ring model yield solvable
systems, making their ranks identifiable. In contrast, we prove that the
symmetric topology of the Tucker model leads to an underdetermined system,
rendering the ranks unidentifiable by this method. For the identifiable models,
we derive explicit closed-form rank estimators based on the moments of observed
data only. We empirically validate these estimators and evaluate the robustness
of the proposal.

</details>


### [54] [MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving](https://arxiv.org/abs/2510.14557)
*Jungi Lee,Junyong Park,Soohyun Cha,Jaehoon Cho,Jaewoong Sim*

Main category: cs.LG

TL;DR: 本文提出MX+格式，一种针对块浮点数(BFP)格式的扩展，通过重新利用异常值的指数字段作为扩展尾数来提高精度，从而在4位精度下实现更好的语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的超低位宽BFP变体由于块中的异常值问题，难以提供合理的语言模型性能，需要一种成本效益高且非侵入式的解决方案来改进BFP格式。

Method: 提出MX+格式，基于关键洞察：异常值不需要使用其元素数据类型中的指数字段，因此可以将指数字段重新用作扩展尾数来提高异常值元素的精度。

Result: 评估显示MX+相比4位MX格式(MXFP4)实现了显著更高的模型性能，存储开销和速度损失可忽略不计。

Conclusion: MX+为高效LLM推理提供了一个有吸引力的替代方案，可以替代MXFP4或MXFP6格式。

Abstract: Reduced-precision data formats are crucial for cost-effective serving of
large language models (LLMs). While numerous reduced-precision formats have
been introduced thus far, they often require intrusive modifications to the
software frameworks or are rather unconventional for widespread adoption across
hardware vendors. In this paper, we instead focus on recent industry-driven
variants of block floating-point (BFP) formats and conduct a comprehensive
analysis to push their limits for efficient LLM serving. Our analysis shows
that existing ultra low-bit BFP variants struggle to provide reasonable
language model performance due to outlier values in blocks. To address the
outliers with BFPs, we propose MX+, a cost-effective and non-intrusive
extension designed for seamless integration into the microscaling (MX) formats.
MX+ builds on the key insight that the outlier does not need to use its
exponent field in the element data type, which allows us to repurpose the
exponent field as an extended mantissa to increase the precision of the outlier
element. Our evaluation shows that MX+ achieves significantly higher model
performance compared to the 4-bit MX format (MXFP4) with negligible storage
overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6
for efficient LLM inference.

</details>


### [55] [Redundancy-Aware Test-Time Graph Out-of-Distribution Detection](https://arxiv.org/abs/2510.14562)
*Yue Hou,He Zhu,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: RedOUT是一个无监督框架，通过将结构熵集成到图分类的测试时OOD检测中，解决训练和测试数据分布差异导致的预测不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，训练和测试数据之间的分布差异导致模型在面对分布外样本时做出不准确预测。现有图OOD检测方法虽然利用数据驱动技术提取有效表示，但其性能仍受到导致语义偏移的结构冗余的影响。

Method: 提出RedOUT框架，引入冗余感知图信息瓶颈（ReGIB），将目标分解为基本信息和无关冗余。通过最小化结构熵来减少解耦的冗余，并提出理论上有界的上下界进行优化。

Result: 在真实世界数据集上的广泛实验表明，RedOUT在OOD检测方面表现出卓越性能。具体而言，该方法平均提升6.7%，在ClinTox/LIPO数据集对上显著超越最佳竞争对手17.3%。

Conclusion: RedOUT通过整合结构熵和冗余感知信息瓶颈，有效解决了图OOD检测中的结构冗余问题，显著提升了检测性能。

Abstract: Distributional discrepancy between training and test data can lead models to
make inaccurate predictions when encountering out-of-distribution (OOD) samples
in real-world applications. Although existing graph OOD detection methods
leverage data-centric techniques to extract effective representations, their
performance remains compromised by structural redundancy that induces semantic
shifts. To address this dilemma, we propose RedOUT, an unsupervised framework
that integrates structural entropy into test-time OOD detection for graph
classification. Concretely, we introduce the Redundancy-aware Graph Information
Bottleneck (ReGIB) and decompose the objective into essential information and
irrelevant redundancy. By minimizing structural entropy, the decoupled
redundancy is reduced, and theoretically grounded upper and lower bounds are
proposed for optimization. Extensive experiments on real-world datasets
demonstrate the superior performance of RedOUT on OOD detection. Specifically,
our method achieves an average improvement of 6.7%, significantly surpassing
the best competitor by 17.3% on the ClinTox/LIPO dataset pair.

</details>


### [56] [Selective Labeling with False Discovery Rate Control](https://arxiv.org/abs/2510.14581)
*Huipeng Huang,Wenbo Liao,Huajun Xi,Hao Zeng,Mengchen Zhao,Hongxin Wei*

Main category: cs.LG

TL;DR: 本文提出了一种名为Conformal Labeling的新方法，通过控制AI预测标签的假发现率(FDR)，识别可被证明可信的AI预测实例。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集的高质量标注成本高昂，而AI模型预测标签虽然成本低但存在不可避免的标注错误。现有方法缺乏对AI标注质量的理论保证，导致AI标注子集中的错误率过高。

Method: 通过构建每个测试实例的conformal p值，比较AI模型的预测置信度与校准实例中被AI错误标注的置信度，然后选择p值低于数据依赖阈值的测试实例，证明AI预测的可信性。

Result: 实验表明该方法在图像和文本标注、LLM问答等多种任务中实现了严格的FDR控制和高功率。

Conclusion: Conformal Labeling方法提供了理论保证，能够将AI标注的假发现率控制在名义水平以下，确保预定义比例的AI分配标签平均上是正确的。

Abstract: Obtaining high-quality labels for large datasets is expensive, requiring
massive annotations from human experts. While AI models offer a cost-effective
alternative by predicting labels, their label quality is compromised by the
unavoidable labeling errors. Existing methods mitigate this issue through
selective labeling, where AI labels a subset and human labels the remainder.
However, these methods lack theoretical guarantees on the quality of
AI-assigned labels, often resulting in unacceptably high labeling error within
the AI-labeled subset. To address this, we introduce \textbf{Conformal
Labeling}, a novel method to identify instances where AI predictions can be
provably trusted. This is achieved by controlling the false discovery rate
(FDR), the proportion of incorrect labels within the selected subset. In
particular, we construct a conformal $p$-value for each test instance by
comparing AI models' predicted confidence to those of calibration instances
mislabeled by AI models. Then, we select test instances whose $p$-values are
below a data-dependent threshold, certifying AI models' predictions as
trustworthy. We provide theoretical guarantees that Conformal Labeling controls
the FDR below the nominal level, ensuring that a predefined fraction of
AI-assigned labels is correct on average. Extensive experiments demonstrate
that our method achieves tight FDR control with high power across various
tasks, including image and text labeling, and LLM QA.

</details>


### [57] [Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking](https://arxiv.org/abs/2510.14586)
*Daria Frolova,Talgat Daulbaev,Egor Sevryugov,Sergei A. Nikolenko,Dmitry N. Ivankov,Ivan Oseledets,Marina A. Pak*

Main category: cs.LG

TL;DR: Matcha是一种新颖的分子对接流程，结合多阶段流匹配、学习评分和物理有效性过滤，在蛋白质-配体结合姿态预测方面实现了速度、准确性和物理合理性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在蛋白质-配体结合姿态预测中难以平衡速度、准确性和物理合理性，需要开发更有效的解决方案。

Method: 采用三阶段流匹配模型（在R³、SO(3)和SO(2)几何空间上运行），结合专用评分模型和无监督物理有效性过滤器来精炼对接预测。

Result: 在Astex和PDBbind测试集上表现出优异的对接成功率和物理合理性，比现代大规模共折叠模型快约25倍。

Conclusion: Matcha提供了一种高效准确的蛋白质-配体对接方法，在速度和准确性方面优于现有方法。

Abstract: Accurate prediction of protein-ligand binding poses is crucial for
structure-based drug design, yet existing methods struggle to balance speed,
accuracy, and physical plausibility. We introduce Matcha, a novel molecular
docking pipeline that combines multi-stage flow matching with learned scoring
and physical validity filtering. Our approach consists of three sequential
stages applied consecutively to refine docking predictions, each implemented as
a flow matching model operating on appropriate geometric spaces
($\mathbb{R}^3$, $\mathrm{SO}(3)$, and $\mathrm{SO}(2)$). We enhance the
prediction quality through a dedicated scoring model and apply unsupervised
physical validity filters to eliminate unrealistic poses. Compared to various
approaches, Matcha demonstrates superior performance on Astex and PDBbind test
sets in terms of docking success rate and physical plausibility. Moreover, our
method works approximately 25 times faster than modern large-scale co-folding
models. The model weights and inference code to reproduce our results are
available at https://github.com/LigandPro/Matcha.

</details>


### [58] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: MAHA是一种针对多模态问答的检索增强生成系统，通过模态感知知识图谱结合密集向量检索和图遍历，有效处理包含文本、图像、表格、方程和图形的非结构化多模态文档。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成系统主要处理单模态文本数据，在处理非结构化多模态文档时效果有限，这些文档包含文本、图像、表格、方程和图形等多种模态，每种模态都提供独特信息。

Method: 提出模态感知混合检索架构(MAHA)，通过模态感知知识图谱集成密集向量检索和结构化图遍历，知识图谱编码跨模态语义和关系。

Result: 在多个基准数据集上的评估显示，MAHA显著优于基线方法，ROUGE-L得分达到0.486，提供完整的模态覆盖。

Conclusion: MAHA能够将嵌入与显式文档结构相结合，实现有效的多模态检索，建立了一个可扩展且可解释的检索框架，通过模态感知推理推进了RAG系统的发展。

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [59] [First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training](https://arxiv.org/abs/2510.14614)
*Gyudong Kim,Hyukju Na,Jin Hyeon Kim,Hyunsung Jang,Jaemin Park,Jaegi Hwang,Namkoo Ha,Seungryong Kim,Young Geun Kim*

Main category: cs.LG

TL;DR: FAL是一种高效的Transformer架构，通过将第一层的注意力输出重定向到后续层的MLP输入，消除了每块的MHA-MLP连接，从而减少通信开销并实现MHA和MLP在单GPU上的并行执行。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer设计在张量并行训练中存在显著的通信开销，特别是在每个块的MHA-MLP连接处需要进行all-reduce通信。研究发现MHA-MLP连接可以绕过以提高效率。

Method: 提出FAL架构，将第一层MHA输出重定向到后续层的MLP输入，消除每块的MHA-MLP连接；FAL+进一步将归一化的第一注意力输出添加到后续层的MHA输出中，以增强MLP输入质量。

Result: FAL将多GPU训练时间减少高达44%，单GPU吞吐量提高1.18倍，困惑度优于基线GPT；FAL+在不增加训练时间的情况下实现更低的困惑度。

Conclusion: FAL和FAL+通过重新设计Transformer架构的连接方式，有效减少了通信开销，提高了训练效率和模型质量。

Abstract: As training billion-scale transformers becomes increasingly common, employing
multiple distributed GPUs along with parallel training methods has become a
standard practice. However, existing transformer designs suffer from
significant communication overhead, especially in Tensor Parallelism (TP),
where each block's MHA-MLP connection requires an all-reduce communication.
Through our investigation, we show that the MHA-MLP connections can be bypassed
for efficiency, while the attention output of the first layer can serve as an
alternative signal for the bypassed connection. Motivated by the observations,
we propose FAL (First Attentions Last), an efficient transformer architecture
that redirects the first MHA output to the MLP inputs of the following layers,
eliminating the per-block MHA-MLP connections. This removes the all-reduce
communication and enables parallel execution of MHA and MLP on a single GPU. We
also introduce FAL+, which adds the normalized first attention output to the
MHA outputs of the following layers to augment the MLP input for the model
quality. Our evaluation shows that FAL reduces multi-GPU training time by up to
44%, improves single-GPU throughput by up to 1.18x, and achieves better
perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity
without increasing the training time than the baseline.

</details>


### [60] [LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching](https://arxiv.org/abs/2510.14623)
*Zhuo Cao,Xuan Zhao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: LeapFactual是一种基于条件流匹配的新型反事实解释算法，能够生成可靠且信息丰富的反事实解释，即使真实和学习到的决策边界存在差异。该方法模型无关，可处理人类参与的系统，并在基准和真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和人工智能模型在高风险领域（如医疗和科学研究）的广泛应用，需要不仅准确而且可解释的模型。现有反事实解释方法存在梯度消失、潜在空间不连续以及对学习与真实决策边界对齐过度依赖等关键限制。

Method: 提出LeapFactual算法，基于条件流匹配生成反事实解释。该方法模型无关，不限于具有可微分损失函数的模型，能够处理人类参与的系统。

Result: 在基准和真实数据集上的广泛实验表明，LeapFactual能够生成准确且符合分布的反事实解释，提供可操作的见解。可靠的反事实样本可以作为新的训练数据来增强模型性能。

Conclusion: LeapFactual方法具有广泛适用性，能够增强科学知识发现和非专家可解释性，为高风险领域提供更可靠的模型解释能力。

Abstract: The growing integration of machine learning (ML) and artificial intelligence
(AI) models into high-stakes domains such as healthcare and scientific research
calls for models that are not only accurate but also interpretable. Among the
existing explainable methods, counterfactual explanations offer
interpretability by identifying minimal changes to inputs that would alter a
model's prediction, thus providing deeper insights. However, current
counterfactual generation methods suffer from critical limitations, including
gradient vanishing, discontinuous latent spaces, and an overreliance on the
alignment between learned and true decision boundaries. To overcome these
limitations, we propose LeapFactual, a novel counterfactual explanation
algorithm based on conditional flow matching. LeapFactual generates reliable
and informative counterfactuals, even when true and learned decision boundaries
diverge. Following a model-agnostic approach, LeapFactual is not limited to
models with differentiable loss functions. It can even handle human-in-the-loop
systems, expanding the scope of counterfactual explanations to domains that
require the participation of human annotators, such as citizen science. We
provide extensive experiments on benchmark and real-world datasets showing that
LeapFactual generates accurate and in-distribution counterfactual explanations
that offer actionable insights. We observe, for instance, that our reliable
counterfactual samples with labels aligning to ground truth can be beneficially
used as new training data to enhance the model. The proposed method is broadly
applicable and enhances both scientific knowledge discovery and non-expert
interpretability.

</details>


### [61] [Galaxy Morphology Classification with Counterfactual Explanation](https://arxiv.org/abs/2510.14655)
*Zhuo Cao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 提出一种结合可逆流的编码器-解码器架构，用于星系形态分类，在保持良好预测性能的同时提供反事实解释以增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 星系形态研究对理解星系演化至关重要，但大规模数据的形态分类工作繁重。现有机器学习方法缺乏可解释性，难以理解模型决策过程。

Method: 扩展经典编码器-解码器架构，集成可逆流技术，能够生成反事实解释来揭示模型的决策依据。

Result: 该方法不仅获得良好的预测性能，还能提供关于决策过程的额外信息。

Conclusion: 提出的可逆流增强架构在星系形态分类中实现了预测性能与模型可解释性的平衡，为天文学研究提供了更透明的机器学习工具。

Abstract: Galaxy morphologies play an essential role in the study of the evolution of
galaxies. The determination of morphologies is laborious for a large amount of
data giving rise to machine learning-based approaches. Unfortunately, most of
these approaches offer no insight into how the model works and make the results
difficult to understand and explain. We here propose to extend a classical
encoder-decoder architecture with invertible flow, allowing us to not only
obtain a good predictive performance but also provide additional information
about the decision process with counterfactual explanations.

</details>


### [62] [Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings](https://arxiv.org/abs/2510.14666)
*Shayan Gharib,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 本文提出了一种基于黎曼几何的无监督域自适应方法，通过将一阶和二阶矩表示为对称正定矩阵，使用Siegel嵌入在SPD矩阵流形上进行对齐，以解决分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用临时相似性度量在嵌入空间中对齐源域和目标域的低阶统计矩，缺乏几何原理支撑。本文旨在利用分布的固有几何特性，提供更原则性的对齐方法。

Method: 通过Siegel嵌入将一阶和二阶矩表示为单个对称正定矩阵，在SPD矩阵的共享流形上使用自然几何距离进行同时对齐，保持源域和目标域的均值和协方差结构。

Result: 方法在图像去噪和图像分类基准测试中得到验证，代码已公开。

Conclusion: 提出的黎曼流形距离方法为跨域比较提供了更忠实的度量，并与目标域误差界限建立了理论联系。

Abstract: We address the problem of distribution shift in unsupervised domain
adaptation with a moment-matching approach. Existing methods typically align
low-order statistical moments of the source and target distributions in an
embedding space using ad-hoc similarity measures. We propose a principled
alternative that instead leverages the intrinsic geometry of these
distributions by adopting a Riemannian distance for this alignment. Our key
novelty lies in expressing the first- and second-order moments as a single
symmetric positive definite (SPD) matrix through Siegel embeddings. This
enables simultaneous adaptation of both moments using the natural geometric
distance on the shared manifold of SPD matrices, preserving the mean and
covariance structure of the source and target distributions and yielding a more
faithful metric for cross-domain comparison. We connect the Riemannian manifold
distance to the target-domain error bound, and validate the method on image
denoising and image classification benchmarks. Our code is publicly available
at https://github.com/shayangharib/GeoAdapt.

</details>


### [63] [FedPPA: Progressive Parameter Alignment for Personalized Federated Learning](https://arxiv.org/abs/2510.14698)
*Maulidi Adi Prasetia,Muhamad Risqi U. Saputra,Guntur Dharma Putra*

Main category: cs.LG

TL;DR: 提出FedPPA方法解决联邦学习中模型和数据异构性问题，通过渐进参数对齐和熵加权平均提升个性化性能


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习场景中客户端存在计算资源异构和数据非独立同分布问题，现有个性化联邦学习方法未充分考虑模型和数据异构的共存问题

Method: 提出渐进参数对齐方法，逐步对齐客户端与全局模型的公共层权重；集成基于熵的加权平均机制

Result: 在MNIST、FMNIST和CIFAR-10数据集上的实验表明，FedPPA优于现有联邦学习算法，在个性化适应方面表现优异

Conclusion: FedPPA能有效缓解全局与局部模型不一致性，同时保留客户端本地知识，在非IID设置下增强个性化鲁棒性

Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving
machine learning paradigm that enables multiple clients to collaboratively
train a model without sharing their data. In real-world scenarios, however,
clients often have heterogeneous computational resources and hold
non-independent and identically distributed data (non-IID), which poses
significant challenges during training. Personalized Federated Learning (PFL)
has emerged to address these issues by customizing models for each client based
on their unique data distribution. Despite its potential, existing PFL
approaches typically overlook the coexistence of model and data heterogeneity
arising from clients with diverse computational capabilities. To overcome this
limitation, we propose a novel method, called Progressive Parameter Alignment
(FedPPA), which progressively aligns the weights of common layers across
clients with the global model's weights. Our approach not only mitigates
inconsistencies between global and local models during client updates, but also
preserves client's local knowledge, thereby enhancing personalization
robustness in non-IID settings. To further enhance the global model performance
while retaining strong personalization, we also integrate entropy-based
weighted averaging into the FedPPA framework. Experiments on three image
classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate
that FedPPA consistently outperforms existing FL algorithms, achieving superior
performance in personalized adaptation.

</details>


### [64] [Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling](https://arxiv.org/abs/2510.14717)
*Alexandru Meterez,Depen Morwani,Jingfeng Wu,Costin-Andrei Oncescu,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: 本文提出了Seesaw框架，通过将学习率乘以1/√2并同时加倍批量大小来替代传统学习率衰减，在保持损失动态的同时减少串行步骤，加速大语言模型预训练。


<details>
  <summary>Details</summary>
Motivation: 批量大小递增训练是加速大语言模型预训练的有效策略，但对于Adam等自适应优化器的最优策略尚不明确，通常需要启发式调参。

Method: 开发了批量大小调度的原则性框架Seesaw：当标准调度器将学习率减半时，Seesaw将学习率乘以1/√2并加倍批量大小。理论上证明了SGD和归一化SGD中学习率衰减与批量大小递增的等价性。

Result: 在150M/300M/600M参数模型上，Seesaw在相同FLOPs下与余弦衰减性能相当，同时将实际训练时间减少约36%，接近理论极限。

Conclusion: Seesaw提供了一种原则性的批量大小调度方法，能够显著加速大语言模型预训练，同时保持模型性能。

Abstract: Increasing the batch size during training -- a ''batch ramp'' -- is a
promising strategy to accelerate large language model pretraining. While for
SGD, doubling the batch size can be equivalent to halving the learning rate,
the optimal strategy for adaptive optimizers like Adam is less clear. As a
result, any batch-ramp scheduling, if used at all, is typically tuned
heuristically. This work develops a principled framework for batch-size
scheduling and introduces Seesaw: whenever a standard scheduler would halve the
learning rate, Seesaw instead multiplies it by $1/\sqrt{2}$ and doubles the
batch size, preserving loss dynamics while reducing serial steps.
Theoretically, we provide, to our knowledge, the first finite-sample proof of
equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy
linear regression, and we extend this equivalence to normalized SGD, a
tractable proxy for Adam, under a variance-dominated regime observed in
practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla
scale using a constant (critical) batch size, Seesaw matches cosine decay at
equal FLOPs while reducing wall-clock time by $\approx 36\%$, approaching the
theoretical limit implied by our analysis.

</details>


### [65] [Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References](https://arxiv.org/abs/2510.14719)
*Hongzheng Chen,Bin Fan,Alexander Collins,Bastian Hagedorn,Evghenii Gaburov,Masahiro Masuda,Matthew Brookhart,Chris Sullivan,Jason Knight,Zhiru Zhang,Vinod Grover*

Main category: cs.LG

TL;DR: Tawa是一个自动化编译器，能够从高级瓦片化程序生成高性能的warp专用代码，通过异步引用(aref)抽象简化GPU数据流编程，在H100 GPU上实现优于cuBLAS和Triton的性能。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的专用硬件单元支持高性能异步数据流执行，但传统的SIMT编程模型与这种任务并行硬件不匹配，导致编程困难。硬件级的warp专业化虽然能实现峰值性能，但需要开发者手动管理复杂的低级通信和软件流水线，过程繁琐且易出错。

Method: 提出Tawa编译器，采用新颖的异步引用(aref)IR抽象来表达warp级通信，无需暴露低级硬件细节。Tawa自动将程序划分为生产者-消费者角色，并管理复杂的数据流流水线。

Result: 在NVIDIA H100 GPU上评估代表性LLM内核，Tawa实现了高硬件利用率，相比高度优化的cuBLAS GEMM内核达到1.1倍加速。对于注意力工作负载，相比Triton达到1.2倍加速，并与手工优化的CUTLASS C++ FlashAttention-3内核性能相当，但编程工作量大幅减少。

Conclusion: Tawa通过自动化warp专用代码生成，成功弥合了GPU数据流编程的可用性差距，在保持高性能的同时显著降低了编程复杂性，为GPU数据流编程提供了可持续的解决方案。

Abstract: Modern GPUs feature specialized hardware units that enable high-performance,
asynchronous dataflow execution. However, the conventional SIMT programming
model is fundamentally misaligned with this task-parallel hardware, creating a
significant programmability gap. While hardware-level warp specialization is
the key to unlocking peak performance, it forces developers to manually
orchestrate complex, low-level communication and software pipelines--a process
that is labor-intensive, error-prone, and unsustainable. To address this
challenge, we present Tawa, an automated compiler that systematically generates
high-performance, warp-specialized code from a high-level, tile-based program.
Central to our approach is a novel IR abstraction, asynchronous references
(aref), which expresses warp-level communication without exposing low-level
hardware details. Using this abstraction, Tawa automatically partitions
programs into producer-consumer roles and manages the intricate dataflow
pipeline, relieving developers of invasive kernel rewriting. Evaluation on
NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers
high hardware utilization, achieving up to 1.1$\times$ speedup over highly
optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains
1.2$\times$ speedup over Triton and matches the performance of the
hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming
effort.

</details>


### [66] [Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries](https://arxiv.org/abs/2510.14751)
*Divyat Mahajan,Sachin Goyal,Badr Youbi Idrissi,Mohammad Pezeshki,Ioannis Mitliagkas,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: 本文提出未来摘要预测（FSP）方法，通过训练辅助头来预测长期未来的紧凑表示，相比传统下一词预测（NTP）和多词预测（MTP）在数学、推理和编程任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统下一词预测（NTP）在长程推理、规划和创意写作方面存在局限，多词预测（MTP）只能捕捉短程依赖关系，改进有限。需要一种能够处理长期依赖关系的方法。

Method: 提出未来摘要预测（FSP），训练辅助头预测长期未来的紧凑表示。探索两种变体：手工制作的摘要（如词袋摘要）和学习的摘要（使用从右到左训练的反向语言模型生成的嵌入）。

Result: 在大规模预训练实验（30亿和80亿参数模型）中，FSP在数学、推理和编程基准测试中相比NTP和MTP都取得了改进。

Conclusion: FSP方法能够有效提升语言模型在长程任务上的表现，为长文本生成提供了更好的解决方案。

Abstract: Next-token prediction (NTP) has driven the success of large language models
(LLMs), but it struggles with long-horizon reasoning, planning, and creative
writing, with these limitations largely attributed to teacher-forced training.
Multi-token prediction (MTP) partially mitigates these issues by predicting
several future tokens at once, but it mostly captures short-range dependencies
and offers limited improvement. We propose future summary prediction (FSP),
which trains an auxiliary head to predict a compact representation of the
long-term future, preserving information relevant for long-form generations. We
explore two variants of FSP: handcrafted summaries, for example, a bag of words
summary of the future of the sequence, and learned summaries, which use
embeddings produced by a reverse language model trained from right to left.
Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate
that FSP provides improvements over both NTP and MTP across math, reasoning,
and coding benchmarks.

</details>


### [67] [Causal Discovery for Linear DAGs with Dependent Latent Variables via Higher-order Cumulants](https://arxiv.org/abs/2510.14780)
*Ming Cai,Penggang Gao,Hisayuki Hara*

Main category: cs.LG

TL;DR: 提出了一种新算法，用于在线性非高斯无环模型（LvLiNGAM）中估计因果有向无环图，允许潜在变量之间、观测变量之间以及两者之间存在因果结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设潜在混杂因子相互独立，或者无法正确处理观测变量间存在因果关系的模型，需要开发能够处理更复杂因果结构的算法。

Method: 利用观测数据的高阶累积量来识别因果结构，允许潜在变量间、观测变量间以及两者之间的因果关系。

Result: 广泛的模拟实验和真实世界数据实验验证了所提算法的有效性和实用性。

Conclusion: 提出的新算法能够有效识别LvLiNGAM中的因果有向无环图，解决了现有方法的局限性，具有实际应用价值。

Abstract: This paper addresses the problem of estimating causal directed acyclic graphs
in linear non-Gaussian acyclic models with latent confounders (LvLiNGAM).
Existing methods assume mutually independent latent confounders or cannot
properly handle models with causal relationships among observed variables.
  We propose a novel algorithm that identifies causal DAGs in LvLiNGAM,
allowing causal structures among latent variables, among observed variables,
and between the two. The proposed method leverages higher-order cumulants of
observed data to identify the causal structure. Extensive simulations and
experiments with real-world data demonstrate the validity and practical utility
of the proposed algorithm.

</details>


### [68] [Efficient Dynamic Structured Sparse Training with Learned Shuffles](https://arxiv.org/abs/2510.14812)
*Abhishek Tyagi,Arjun Iyer,Liam Young,William H Renninger,Christopher Kanan,Yuhao Zhu*

Main category: cs.LG

TL;DR: 提出了一种结合学习排列矩阵的结构化稀疏训练方法（PA-DST），在保持高精度的同时显著提升了训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏虽然能加速GPU上的训练和推理，但在准确性上仍落后于非结构化动态稀疏训练（DST），主要原因是表达能力受限。

Method: 为每个层学习一个排列矩阵，与结构化权重矩阵联合训练，应用于块状、N:M和对角线三种典型结构。

Result: 在ImageNet-1K（ViT-B/16）和WikiText-103（GPT-2）上，PA-DST在90-95%稀疏度下与非结构化基线（RigL、SET）精度相当，但训练速度提升1.21倍，推理速度提升2.9倍。

Conclusion: 结构+学习排列的组合在准确性和效率之间找到了最佳平衡点。

Abstract: Structured sparsity accelerates training and inference on modern GPUs, yet it
still trails unstructured dynamic sparse training (DST) in accuracy. The
shortfall stems from a loss of expressivity: whereas a dense layer can realize
every possible mask obtained by choosing any $w$ active weights out of $n$, a
fixed block or N:M layout explores only a subset of those possibilities. We
propose to close this gap by learning, for each layer, a single permutation
matrix jointly with the structured weight matrix. Applied to three canonical
structures -- block, N:M, and diagonals -- we show that permutation-augmented
DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\% sparsity on
ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\times$
and infers up to $2.9\times$ faster. The results position structure + learned
permutation as a sweet spot between accuracy and efficiency.

</details>


### [69] [Tackling Time-Series Forecasting Generalization via Mitigating Concept Drift](https://arxiv.org/abs/2510.14814)
*Zhiyuan Zhao,Haoxin Liu,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 本文提出ShifTS框架，首先处理时间序列中的时间偏移，然后处理概念漂移，通过软注意力机制从回望和预测时间序列中寻找不变模式，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据具有动态特性，存在分布偏移问题。现有研究主要关注时间偏移，但对时间序列预测中的概念漂移方法设计关注较少。需要解决潜在的概念漂移问题，同时传统基于不变学习的概念漂移方法在时间序列预测中面临挑战。

Method: 提出软注意力机制，从回望和预测时间序列中寻找不变模式；引入ShifTS框架，先处理时间偏移再处理概念漂移，采用统一方法；该框架与具体方法无关。

Result: 大量实验证明ShifTS在多个数据集上能持续提升无关模型的预测准确性，且优于现有的概念漂移、时间偏移及组合基线方法。

Conclusion: ShifTS框架能有效解决时间序列预测中的分布偏移问题，特别是通过先处理时间偏移再处理概念漂移的统一方法，显著提升了预测性能。

Abstract: Time-series forecasting finds broad applications in real-world scenarios. Due
to the dynamic nature of time series data, it is important for time-series
forecasting models to handle potential distribution shifts over time. In this
paper, we initially identify two types of distribution shifts in time series:
concept drift and temporal shift. We acknowledge that while existing studies
primarily focus on addressing temporal shift issues in time series forecasting,
designing proper concept drift methods for time series forecasting has received
comparatively less attention.
  Motivated by the need to address potential concept drift, while conventional
concept drift methods via invariant learning face certain challenges in
time-series forecasting, we propose a soft attention mechanism that finds
invariant patterns from both lookback and horizon time series. Additionally, we
emphasize the critical importance of mitigating temporal shifts as a
preliminary to addressing concept drift. In this context, we introduce ShifTS,
a method-agnostic framework designed to tackle temporal shift first and then
concept drift within a unified approach. Extensive experiments demonstrate the
efficacy of ShifTS in consistently enhancing the forecasting accuracy of
agnostic models across multiple datasets, and outperforming existing concept
drift, temporal shift, and combined baselines.

</details>


### [70] [Reinforcement Learning with Stochastic Reward Machines](https://arxiv.org/abs/2510.14837)
*Jan Corazza,Ivan Gavran,Daniel Neider*

Main category: cs.LG

TL;DR: 本文提出了一种新型的随机奖励机，用于解决强化学习中奖励函数存在噪声的问题，并基于约束求解开发了相应的学习算法。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励机学习算法假设奖励是无噪声的理想环境，这在实践中存在局限性。为了解决奖励函数中的噪声问题，需要开发能够处理随机性奖励的机制。

Method: 引入随机奖励机概念，基于约束求解方法从强化学习智能体的探索中学习最小随机奖励机，可与现有奖励机强化学习算法结合使用。

Result: 在两个案例研究中验证了算法的有效性，表明其性能优于现有方法和处理噪声奖励函数的朴素方法。

Conclusion: 该算法能够保证在极限情况下收敛到最优策略，为处理噪声奖励函数的强化学习问题提供了有效解决方案。

Abstract: Reward machines are an established tool for dealing with reinforcement
learning problems in which rewards are sparse and depend on complex sequences
of actions. However, existing algorithms for learning reward machines assume an
overly idealized setting where rewards have to be free of noise. To overcome
this practical limitation, we introduce a novel type of reward machines, called
stochastic reward machines, and an algorithm for learning them. Our algorithm,
based on constraint solving, learns minimal stochastic reward machines from the
explorations of a reinforcement learning agent. This algorithm can easily be
paired with existing reinforcement learning algorithms for reward machines and
guarantees to converge to an optimal policy in the limit. We demonstrate the
effectiveness of our algorithm in two case studies and show that it outperforms
both existing methods and a naive approach for handling noisy reward functions.

</details>


### [71] [Backdoor Unlearning by Linear Task Decomposition](https://arxiv.org/abs/2510.14845)
*Amel Abdelraheem,Alessandro Favero,Gerome Bovet,Pascal Frossard*

Main category: cs.LG

TL;DR: 该论文提出了一种无需重新训练即可从基础模型中移除后门攻击的方法，通过发现后门在权重空间中的解耦特性，实现了高效的后门遗忘，同时保持模型在正常任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽然具有广泛泛化能力，但容易受到对抗性扰动和后门攻击。现有防御方法需要昂贵的微调，且会降低模型在其他任务上的性能。本文旨在研究是否可以在不损害模型通用能力的情况下移除后门。

Method: 研究发现后门在模型权重空间中与其他良性任务是解耦的，基于这一发现提出了简单的遗忘方法，利用解耦特性隔离和擦除后门影响。

Result: 在CLIP模型和常见对抗触发器的实验中，该方法实现了近乎完美的后门遗忘，同时平均保持了96%的干净准确率。即使攻击未知，也能通过反向工程触发器成功移除后门。

Conclusion: 该方法相比现有最先进防御方法，在遗忘后门和保持干净准确率之间取得了更好的平衡，为安全基础模型提供了有效解决方案。

Abstract: Foundation models have revolutionized computer vision by enabling broad
generalization across diverse tasks. Yet, they remain highly susceptible to
adversarial perturbations and targeted backdoor attacks. Mitigating such
vulnerabilities remains an open challenge, especially given that the
large-scale nature of the models prohibits retraining to ensure safety.
Existing backdoor removal approaches rely on costly fine-tuning to override the
harmful behavior, and can often degrade performance on other unrelated tasks.
This raises the question of whether backdoors can be removed without
compromising the general capabilities of the models. In this work, we address
this question and study how backdoors are encoded in the model weight space,
finding that they are disentangled from other benign tasks. Specifically, this
separation enables the isolation and erasure of the backdoor's influence on the
model with minimal impact on clean performance. Building on this insight, we
introduce a simple unlearning method that leverages such disentanglement.
Through extensive experiments with CLIP-based models and common adversarial
triggers, we show that, given the knowledge of the attack, our method achieves
approximately perfect unlearning, while retaining, on average, 96% of clean
accuracy. Additionally, we demonstrate that even when the attack and its
presence are unknown, our method successfully unlearns backdoors by proper
estimation using reverse-engineered triggers. Overall, our method consistently
yields better unlearning and clean accuracy tradeoffs when compared to present
state-of-the-art defenses.

</details>


### [72] [Predicting kernel regression learning curves from only raw data statistics](https://arxiv.org/abs/2510.14878)
*Dhruva Karkada,Joseph Turnbull,Yuxi Liu,James B. Simon*

Main category: cs.LG

TL;DR: 该论文提出了Hermite特征结构假设（HEA），用于预测核回归在真实数据集上的学习曲线，仅需数据协方差矩阵和目标函数的经验多项式分解两个测量值。


<details>
  <summary>Details</summary>
Motivation: 研究真实数据集上核回归的学习曲线预测问题，旨在开发一个能够从数据集结构直接映射到模型性能的端到端理论框架。

Method: 提出Hermite特征结构假设（HEA），通过分析近似核特征值和特征函数，将特征函数建模为数据的Hermite多项式，并应用于预测学习曲线。

Result: HEA在真实图像数据上表现良好，能够准确预测学习曲线，并且发现MLP在特征学习阶段也按照HEA预测的顺序学习Hermite多项式。

Conclusion: HEA框架证明了在真实数据集上为非线性学习算法构建从数据集结构到模型性能的端到端理论是可行的。

Abstract: We study kernel regression with common rotation-invariant kernels on real
datasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical
framework that predicts learning curves (test risk vs. sample size) from only
two measurements: the empirical data covariance matrix and an empirical
polynomial decomposition of the target function $f_*$. The key new idea is an
analytical approximation of a kernel's eigenvalues and eigenfunctions with
respect to an anisotropic data distribution. The eigenfunctions resemble
Hermite polynomials of the data, so we call this approximation the Hermite
eigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find
that real image data is often "Gaussian enough" for the HEA to hold well in
practice, enabling us to predict learning curves by applying prior results
relating kernel eigenstructure to test risk. Extending beyond kernel
regression, we empirically find that MLPs in the feature-learning regime learn
Hermite polynomials in the order predicted by the HEA. Our HEA framework is a
proof of concept that an end-to-end theory of learning which maps dataset
structure all the way to model performance is possible for nontrivial learning
algorithms on real datasets.

</details>


### [73] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 研究表明，仅通过纯采样方法（无需额外训练）即可从基础模型中激发出与强化学习后训练相媲美的推理能力，提出的迭代采样算法在多个任务上表现优异且保持样本多样性。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以在不进行额外训练的情况下，仅通过推理时的纯采样方法从基础语言模型中激发出与强化学习后训练相当的推理能力，以解决RL训练后模型样本多样性下降的问题。

Method: 提出一种简单的迭代采样算法，受马尔可夫链蒙特卡洛技术启发，利用基础模型自身的似然性进行采样，无需训练、精选数据集或验证器。

Result: 在MATH500、HumanEval和GPQA等多个单次任务上，该算法显著提升了推理能力，几乎达到甚至超过RL后训练的效果，同时避免了RL训练后样本多样性的崩溃。

Conclusion: 该方法无需训练即可实现与RL后训练相媲美的推理性能，且能保持样本多样性，在难以验证的领域具有广泛适用性。

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [74] [Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models](https://arxiv.org/abs/2510.14961)
*Jonas Geiping,Xinyu Yang,Guinan Su*

Main category: cs.LG

TL;DR: 本文提出了一种新的扩散强制采样器，用于加速循环深度语言模型的生成，通过在每个前向传递中解码新token，同时并行细化其潜在状态，实现了比自回归基线更高效的生成。


<details>
  <summary>Details</summary>
Motivation: 探索循环深度模型与扩散语言模型之间的关系，利用它们的相似性来加速循环深度模型的推理生成过程。

Method: 基于扩散文献原理开发扩散强制采样器，在每个前向传递中解码新token，同时通过循环并行细化这些token的潜在状态。

Result: 该采样器可直接应用于现有的35亿参数循环深度transformer，无需调优即可实现高达5倍的加速，生成表达能力严格优于自回归基线。

Conclusion: 研究不仅为循环深度模型在推理时并行化额外计算提供了高效机制，还表明此类模型可自然地视为强大的连续（尽管因果）扩散语言模型。

Abstract: Language models with recurrent depth, also referred to as universal or looped
when considering transformers, are defined by the capacity to increase their
computation through the repetition of layers. Recent efforts in pretraining
have demonstrated that these architectures can scale to modern language
modeling tasks while exhibiting advantages in reasoning tasks. In this work, we
examine the relationship between recurrent-depth models and diffusion language
models. Building on their similarities, we develop a new diffusion forcing
sampler for these models to accelerate generation. The sampler advances by
decoding new tokens at every forward pass of the model, while the latent states
of these tokens can be further refined in parallel through recurrence.
Theoretically, generation with our sampler is strictly more expressive than the
baseline autoregressive generation using the same time budget on modern
hardware. Moreover, this sampler, based on principles from diffusion
literature, can be directly applied to existing 3.5B recurrent-depth
transformers without any tuning, leading to up to a 5x speedup. Consequently,
our findings not only provide an efficient mechanism for parallelizing the
extra computation in recurrent-depth models at inference, but also suggest that
such models can be naturally viewed as strong continuous, though causal,
diffusion language models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)
*Wei Zou,Yupei Liu,Yanting Wang,Ying Chen,Neil Gong,Jinyuan Jia*

Main category: cs.CR

TL;DR: PIShield是一种高效且有效的提示注入攻击检测方法，通过在LLM的注入关键层提取最终token的内部表示，并使用线性分类器区分干净和受污染的提示。


<details>
  <summary>Details</summary>
Motivation: 现有的提示注入攻击检测方法性能不佳且计算开销高，LLM集成应用容易受到提示注入攻击，攻击者通过污染输入注入恶意提示，使LLM遵循攻击者意图而非用户意图。

Method: 提出PIShield方法，关键观察是LLM特定层（注入关键层）中最终token的内部表示能够捕捉干净和受污染提示的区别特征，使用标记的干净和受污染提示集在这些内部表示上训练简单线性分类器。

Result: 在5个不同基准数据集和8种提示注入攻击上比较PIShield与11个基线方法，结果表明PIShield既高效又有效，显著优于现有方法，并能抵抗强自适应攻击。

Conclusion: PIShield提供了一种既高效又有效的提示注入攻击检测解决方案，通过利用LLM内部表示的特征区分能力，大幅提升了检测性能并降低了计算开销。

Abstract: LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker's intent instead of the original user's. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.

</details>


### [76] [Every Language Model Has a Forgery-Resistant Signature](https://arxiv.org/abs/2510.14086)
*Matthew Finlayson,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CR

TL;DR: 该论文提出了一种基于语言模型输出几何约束的椭圆签名方法，用于识别模型来源和验证输出真实性。该方法利用语言模型输出位于高维椭圆表面的几何特性作为模型签名，具有难以伪造、自然存在、自包含和紧凑冗余的特点。


<details>
  <summary>Details</summary>
Motivation: 随着闭源语言模型API的普及，需要开发能够提取隐藏模型细节和识别模型输出的取证方法。现有方法主要利用语言模型架构和参数施加的几何约束，但椭圆约束这一较少被关注的几何特性可能提供更好的模型识别能力。

Method: 提出椭圆签名方法，利用语言模型输出位于高维椭圆表面的几何约束作为模型签名。开发了从小模型中提取椭圆的新技术，并讨论了在生产规模模型中应用的实际障碍。最后提出了基于椭圆签名的语言模型输出验证协议。

Result: 椭圆签名具有四个独特属性：难以伪造（无模型参数访问权限下无法生成椭圆上的logprobs）、自然存在（所有语言模型都具有这种椭圆约束）、自包含（无需模型输入或完整权重即可检测）和紧凑冗余（每个logprob输出中都可独立检测）。

Conclusion: 椭圆签名可作为语言模型的可靠标识符，并可用于构建类似密码学对称密钥消息认证系统的输出验证协议。虽然在小模型中可行，但在生产规模模型中应用仍面临实际挑战。

Abstract: The ubiquity of closed-weight language models with public-facing APIs has
generated interest in forensic methods, both for extracting hidden model
details (e.g., parameters) and for identifying models by their outputs. One
successful approach to these goals has been to exploit the geometric
constraints imposed by the language model architecture and parameters. In this
work, we show that a lesser-known geometric constraint--namely, that language
model outputs lie on the surface of a high-dimensional ellipse--functions as a
signature for the model and can be used to identify the source model of a given
output. This ellipse signature has unique properties that distinguish it from
existing model-output association methods like language model fingerprints. In
particular, the signature is hard to forge: without direct access to model
parameters, it is practically infeasible to produce log-probabilities
(logprobs) on the ellipse. Secondly, the signature is naturally occurring,
since all language models have these elliptical constraints. Thirdly, the
signature is self-contained, in that it is detectable without access to the
model inputs or the full weights. Finally, the signature is compact and
redundant, as it is independently detectable in each logprob output from the
model. We evaluate a novel technique for extracting the ellipse from small
models and discuss the practical hurdles that make it infeasible for
production-scale models. Finally, we use ellipse signatures to propose a
protocol for language model output verification, analogous to cryptographic
symmetric-key message authentication systems.

</details>


### [77] [Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies](https://arxiv.org/abs/2510.14198)
*Morium Akter Munny,Mahbub Alam,Sonjoy Kumar Paul,Daniel Timko,Muhammad Lutfor Rahman,Nitesh Saxena*

Main category: cs.CR

TL;DR: 首次对收费诈骗域名进行大规模分析，发现攻击者利用宽松注册商和非主流顶级域名，注册模式显示高度同步的自动化攻击活动，建立了基于注册数据的预测模型，准确率达80.4%。


<details>
  <summary>Details</summary>
Motivation: 收费诈骗通过注册伪装成合法交通机构的虚假域名欺骗用户进行欺诈支付，这类诈骗快速增长且危害严重，但尚未得到充分研究。

Method: 使用包含67,907个确认诈骗域名的数据集进行分析，识别注册模式和攻击策略，并构建基于注册数据的预测模型来预测哪些诈骗域名可能被暂停。

Result: 86.9%的域名集中在5个非主流TLD，72.9%通过单一提供商注册；发现高度时间聚集的注册模式，超过一半域名在2025年第一季度注册；预测模型达到80.4%准确率和92.3%灵敏度。

Conclusion: 攻击者利用冷门TLD、宽松注册商和协调注册爆发来规避检测，注册数据单独可能不足，结合域名URL和网页内容特征可进一步提高检测效果。

Abstract: Toll scams involve criminals registering fake domains that pretend to be
legitimate transportation agencies to trick users into making fraudulent
payments. Although these scams are rapidly increasing and causing significant
harm, they have not been extensively studied. We present the first large-scale
analysis of toll scam domains, using a newly created dataset of 67,907
confirmed scam domains mostly registered in 2025. Our study reveals that
attackers exploit permissive registrars and less common top-level domains, with
86.9% of domains concentrated in just five non-mainstream TLDs and 72.9%
registered via a single provider. We also discover specific registration
patterns, including short bursts of activity that suggest automated,
coordinated attacks, with over half of domains registered in the first quarter
of 2025. This extreme temporal clustering reflects highly synchronized campaign
launches. Additionally, we build a simple predictive model using only domain
registration data to predict which scam domains are likely to be suspended -- a
proxy for confirmed abuse -- achieving 80.4% accuracy, and 92.3% sensitivity.
Our analysis reveals attacker strategies for evading detection -- such as
exploiting obscure TLDs, permissive registrars, and coordinated registration
bursts -- which can inform more targeted interventions by registrars, hosting
providers, and security platforms. However, our results suggest that
registration metadata alone may be insufficient, and incorporating features
from domain URLs and webpage content could further improve detection.

</details>


### [78] [Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks](https://arxiv.org/abs/2510.14283)
*Xinhao Deng,Jingyou Chen,Linxiao Yu,Yixiang Zhang,Zhongyi Gu,Changhao Qiu,Xiyuan Zhao,Ke Xu,Qi Li*

Main category: cs.CR

TL;DR: 本文首次系统评估了网站指纹识别攻击在多种现实条件下的表现，发现许多在孤立环境中表现良好的技术在面对防御机制、流量漂移、多标签浏览等复杂情况时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有的网站指纹识别攻击研究大多局限于单一场景，忽视了真实环境的复杂性，需要系统评估这些技术在实际应用中的表现。

Method: 采用多维评估框架，在多种现实条件下测试现有网站指纹识别技术，包括防御机制、流量漂移、多标签浏览、早期检测、开放世界和少样本场景。

Result: 实验结果显示，许多在孤立设置中性能强劲的网站指纹识别技术，在面对其他条件时性能显著下降。由于真实环境往往结合多种挑战，当前的网站指纹识别攻击难以直接应用于实践。

Conclusion: 本研究揭示了网站指纹识别攻击的局限性，并提出了多维评估框架，为开发更鲁棒和实用的网站指纹识别攻击提供了重要见解。

Abstract: Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to
infer the websites visited by users, posing a serious threat to anonymous
communication systems. Although recent WF techniques achieve over 90% accuracy
in controlled experimental settings, most studies remain confined to single
scenarios, overlooking the complexity of real-world environments. This paper
presents the first systematic and comprehensive evaluation of existing WF
attacks under diverse realistic conditions, including defense mechanisms,
traffic drift, multi-tab browsing, early-stage detection, open-world settings,
and few-shot scenarios. Experimental results show that many WF techniques with
strong performance in isolated settings degrade significantly when facing other
conditions. Since real-world environments often combine multiple challenges,
current WF attacks are difficult to apply directly in practice. This study
highlights the limitations of WF attacks and introduces a multidimensional
evaluation framework, offering critical insights for developing more robust and
practical WF attacks.

</details>


### [79] [BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection](https://arxiv.org/abs/2510.14344)
*Zichen Liu,Shao Yang,Xusheng Xiao*

Main category: cs.CR

TL;DR: BINCTX是一种多模态学习方法，通过结合字节码图像视图、上下文视图和第三方库使用视图来检测移动应用中的不良行为，在真实恶意软件检测中达到94.73%的F1分数，对混淆和对抗样本具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 移动应用市场中存在大量不良行为应用，这些行为难以检测，因为它们不依赖权限保护API，且可通过UI或元数据编辑进行伪装。

Method: 构建多模态应用表示：1）全局字节码图像视图捕获代码语义和家族模式；2）上下文视图显示行为触发方式；3）第三方库使用视图总结调用路径频率。三个视图嵌入融合后训练上下文感知分类器。

Result: 在真实恶意软件和良性应用上达到94.73%的宏F1分数，比强基线至少提升14.92%。在商业混淆后仍保持84%的F1分数，比仅使用字节码的系统更抗对抗样本。

Conclusion: BINCTX通过多模态融合有效检测移动应用中的不良行为，对混淆和对抗攻击具有鲁棒性，显著优于现有方法。

Abstract: Mobile app markets host millions of apps, yet undesired behaviors (e.g.,
disruptive ads, illegal redirection, payment deception) remain hard to catch
because they often do not rely on permission-protected APIs and can be easily
camouflaged via UI or metadata edits. We present BINCTX, a learning approach
that builds multi-modal representations of an app from (i) a global
bytecode-as-image view that captures code-level semantics and family-style
patterns, (ii) a contextual view (manifested actions, components, declared
permissions, URL/IP constants) indicating how behaviors are triggered, and
(iii) a third-party-library usage view summarizing invocation frequencies along
inter-component call paths. The three views are embedded and fused to train a
contextual-aware classifier. On real-world malware and benign apps, BINCTX
attains a macro F1 of 94.73%, outperforming strong baselines by at least
14.92%. It remains robust under commercial obfuscation (F1 84%
post-obfuscation) and is more resistant to adversarial samples than
state-of-the-art bytecode-only systems.

</details>


### [80] [Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries](https://arxiv.org/abs/2510.14384)
*Sebastian Jänich,Merlin Sievers,Johannes Kinder*

Main category: cs.CR

TL;DR: 提出了一种在二进制级别自动修补物联网固件中已知漏洞的方法，无需供应商支持，成功修补了83%-96%的目标漏洞。


<details>
  <summary>Details</summary>
Motivation: 低成本物联网设备由于更新机制不完善而存在安全风险，许多设备运行着过时且已知存在漏洞的开源软件版本。

Method: 采用最小侵入性本地重汇编技术，在二进制级别自动修补已知漏洞，旨在最小化副作用并降低引入破坏性变更的风险。

Result: 在MAGMA基准测试的108个二进制文件中成功修补83%的目标漏洞，在KARONTE数据集的30个真实物联网固件中成功修补96%的目标漏洞。

Conclusion: 该方法能够有效解决物联网设备因更新机制不完善导致的安全问题，为物联网安全提供了一种实用的解决方案。

Abstract: Low-cost Internet of Things (IoT) devices are increasingly popular but often
insecure due to poor update regimes. As a result, many devices run outdated and
known-vulnerable versions of open-source software. We address this problem by
proposing to patch IoT firmware at the binary level, without requiring vendor
support. In particular, we introduce minimally invasive local reassembly, a new
technique for automatically patching known (n-day) vulnerabilities in IoT
firmware. Our approach is designed to minimize side effects and reduce the risk
of introducing breaking changes. We systematically evaluate our approach both
on 108 binaries within the controlled environment of the MAGMA benchmarks, as
well as on 30 real-world Linux-based IoT firmware images from the KARONTE
dataset. Our prototype successfully patches 83% of targeted vulnerabilities in
MAGMA and 96% in the firmware dataset.

</details>


### [81] [Certifying optimal MEV strategies with Lean](https://arxiv.org/abs/2510.14480)
*Massimo Bartoletti,Riccardo Marchesin,Roberto Zunino*

Main category: cs.CR

TL;DR: 本文首次在Lean定理证明器中形式化MEV攻击，提出了一种构建机器验证证明的方法来验证MEV攻击的上界，并证明了自动做市商中三明治攻击的最优性。


<details>
  <summary>Details</summary>
Motivation: MEV攻击已从去中心化应用中提取了数十亿美元价值，对区块链安全产生系统性影响。现有实证研究和手工推理方法无法充分验证MEV攻击的缺失，需要更严格的形式化验证方法。

Method: 在Lean定理证明器中形式化MEV，构建机器检查的证明来验证MEV攻击的上界，建模并分析两个典型DeFi协议的MEV。

Result: 开发了第一个机器验证证明，证明了自动做市商中三明治攻击的最优性，为MEV攻击分析提供了超越现有技术的正确性保证。

Conclusion: 该方法为MEV攻击的形式化验证提供了通用框架，能够为DeFi协议提供更强的安全保证，证明了形式化方法在区块链安全分析中的有效性。

Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized
applications where the adversary profits by manipulating the ordering,
inclusion, or exclusion of transactions in a blockchain. Decentralized Finance
(DeFi) protocols are a primary target of these attacks, as their logic depends
critically on transaction sequencing. To date, MEV attacks have already
extracted billions of dollars in value, underscoring their systemic impact on
blockchain security. Verifying the absence of MEV attacks requires determining
suitable upper bounds, i.e. proving that no adversarial strategy can extract
more value (if any) than expected by protocol designers. This problem is
notoriously difficult: the space of adversarial strategies is extremely vast,
making empirical studies and pen-and-paper reasoning insufficiently rigorous.
In this paper, we present the first mechanized formalization of MEV in the Lean
theorem prover. We introduce a methodology to construct machine-checked proofs
of MEV bounds, providing correctness guarantees beyond what is possible with
existing techniques. To demonstrate the generality of our approach, we model
and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the
first machine-checked proof of the optimality of sandwich attacks in Automated
Market Makers, a fundamental DeFi primitive.

</details>


### [82] [Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration](https://arxiv.org/abs/2510.14522)
*Evangelos Lamprou,Julian Dai,Grigoris Ntousakis,Martin C. Rinard,Nikos Vasilakis*

Main category: cs.CR

TL;DR: Lexo是一个自动学习和重新生成无漏洞版本潜在恶意组件的系统，用于防御软件供应链攻击。它通过生成输入-输出对来建模组件行为，然后合成新版本组件，保持原始功能但避免恶意行为。


<details>
  <summary>Details</summary>
Motivation: 开源软件生态系统中的软件供应链攻击是一个持续关注的重要问题，这些攻击在保持组件标准功能的同时隐藏恶意功能，只在目标环境中激活。

Method: Lexo首先生成一组输入-输出对来建模组件的完整可观察行为，然后使用这些数据合成原始组件的新版本。在整个重新生成过程中，Lexo咨询多个大型语言模型实例，使用正确性和覆盖率指标来引导这些实例，并对结果进行保护。

Result: 在100多个真实世界软件包（包括高知名度的隐蔽供应链攻击）上的评估表明，Lexo能够跨多个领域扩展，平均在100秒内高效重新生成代码，保持兼容性，并成功消除了多个真实世界供应链攻击中的恶意代码，即使在最先进的大型语言模型提示消除恶意代码失败的情况下也能成功。

Conclusion: Lexo提供了一种有效的方法来防御隐蔽的软件供应链攻击，通过自动重新生成无恶意代码的组件版本，在保持功能兼容性的同时消除安全威胁。

Abstract: Software supply-chain attacks are an important and ongoing concern in the
open source software ecosystem. These attacks maintain the standard
functionality that a component implements, but additionally hide malicious
functionality activated only when the component reaches its target environment.
Lexo addresses such stealthy attacks by automatically learning and regenerating
vulnerability-free versions of potentially malicious components. Lexo first
generates a set of input-output pairs to model a component's full observable
behavior, which it then uses to synthesize a new version of the original
component. The new component implements the original functionality but avoids
stealthy malicious behavior. Throughout this regeneration process, Lexo
consults several distinct instances of Large Language Models (LLMs), uses
correctness and coverage metrics to shepherd these instances, and guardrails
their results. Our evaluation on 100+ real-world packages, including high
profile stealthy supply-chain attacks, indicates that Lexo scales across
multiple domains, regenerates code efficiently (<100s on average), maintains
compatibility, and succeeds in eliminating malicious code in several real-world
supply-chain-attacks, even in cases when a state-of-the-art LLM fails to
eliminate malicious code when prompted to do so.

</details>


### [83] [Symbolic verification of Apple's Find My location-tracking protocol](https://arxiv.org/abs/2510.14589)
*Vaishnavi Sundararajan,Rithwik*

Main category: cs.CR

TL;DR: 本文对苹果Find My追踪系统进行形式化安全分析，通过符号建模和自动化验证证明其隐私保护属性


<details>
  <summary>Details</summary>
Motivation: 苹果Find My系统虽然声称安全私密，但代码不公开且可能存在逻辑漏洞，需要独立验证其安全声明

Method: 使用符号模型对Find My协议进行建模，在Tamarin证明器中制定精确的形式化规范并进行自动化机器可验证证明

Result: 通过形式化验证证明了Find My协议满足期望的安全属性

Conclusion: Find My协议在形式化验证下确实具备所声称的隐私和安全保护能力

Abstract: Tracking devices, while designed to help users find their belongings in case
of loss/theft, bring in new questions about privacy and surveillance of not
just their own users, but in the case of crowd-sourced location tracking, even
that of others even orthogonally associated with these platforms. Apple's Find
My is perhaps the most ubiquitous such system which can even locate devices
which do not possess any cellular support or GPS, running on millions of
devices worldwide. Apple claims that this system is private and secure, but the
code is proprietary, and such claims have to be taken on faith. It is well
known that even with perfect cryptographic guarantees, logical flaws might
creep into protocols, and allow undesirable attacks. In this paper, we present
a symbolic model of the Find My protocol, as well as a precise formal
specification of desirable properties, and provide automated, machine-checkable
proofs of these properties in the Tamarin prover.

</details>


### [84] [AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX](https://arxiv.org/abs/2510.14675)
*Nicolas Dutly,Friederike Groschupp,Ivan Puddu,Kari Kostiainen,Srdjan Capkun*

Main category: cs.CR

TL;DR: AEX-NStep是首个针对AEX-Notify的终端计数攻击，证明即使没有确定性单步执行，中断计数攻击仍然可行，并成功实施了ECDSA密钥泄露攻击。


<details>
  <summary>Details</summary>
Motivation: Intel引入AEX-Notify来防止基于中断的步进攻击（如SGX-Step），但需要验证其安全保证是否真正有效。

Method: 开发了AEX-NStep中断计数攻击，包括两种新的概率性中断计数攻击方法，并应用于AEX-Notify启用的SGX终端。

Result: 证明AEX-Notify的安全保证之一——混淆的前向进展——不成立，成功实施了ECDSA密钥泄露攻击。

Conclusion: AEX-Notify不能完全防止中断计数攻击，研究结果扩展了其安全分析并为未来缓解措施设计提供了参考。

Abstract: To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel
introduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent
deterministic single-stepping. In this work, we introduce AEX-NStep, the first
interrupt counting attack on AEX-Notify-enabled Enclaves. We show that
deterministic single-stepping is not required for interrupt counting attacks to
be practical and that, therefore, AEX-Notify does not entirely prevent such
attacks. We specifically show that one of AEX-Notify's security guarantees,
obfuscated forward progress, does not hold, and we introduce two new
probabilistic interrupt counting attacks. We use these attacks to construct a
practical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our
results extend the original security analysis of AEX-Notify and inform the
design of future mitigations.

</details>


### [85] [FibRace: a large-scale benchmark of client-side proving on mobile devices](https://arxiv.org/abs/2510.14693)
*Simon Malatrait,Alex Sirac*

Main category: cs.CR

TL;DR: FibRace是首个在智能手机上使用Cairo M进行客户端证明生成的大规模实验，通过移动游戏形式让玩家证明斐波那契数列并参与排行榜竞争。实验结果显示现代智能手机能在5秒内完成证明，证实移动设备现在能够可靠生成零知识证明。


<details>
  <summary>Details</summary>
Motivation: 测试智能手机客户端证明生成的可行性，为轻量级证明器、证明驱动基础设施和隐私保护移动应用建立实践基准，同时通过游戏形式吸引公众参与。

Method: 开发名为FibRace的移动游戏，玩家在游戏中生成斐波那契数列的零知识证明并参与排行榜竞争。在2025年9月11-30日期间，6047名玩家在1420种不同设备型号上生成了2,195,488个证明。

Result: 大多数现代智能手机能在5秒内完成证明，性能主要与RAM容量和SoC性能相关。拥有至少3GB RAM的设备能够稳定证明，苹果A19 Pro和M系列芯片获得最快证明时间。所有证明都在Hyli区块链上原生验证且无拥堵。

Conclusion: 移动设备现在能够可靠生成零知识证明，无需远程证明器或专用硬件。FibRace提供了迄今为止最全面的移动证明性能数据集，为未来研究建立了实践基准。

Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale
experiment to test client-side proof generation on smartphones using Cairo M.
Presented as a mobile game in which players proved Fibonacci numbers and
climbed a leaderboard, FibRace served a dual purpose: to engage the public and
to provide empirical benchmarking. Over a three-week campaign (September 11-30,
2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420
unique device models. The results show that most modern smartphones can
complete a proof in under 5 seconds, confirming that *mobile devices are now
capable of producing zero-knowledge proofs reliably*, without the need for
remote provers or specialized hardware. Performance was correlated primarily
with RAM capacity and SoC (System on Chip) performance: devices with at least 3
GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the
fastest proving times. Hyli's blockchain natively verified every proof onchain
without congestion. FibRace provides the most comprehensive dataset to date on
mobile proving performance, establishing a practical baseline for future
research in lightweight provers, proof-powered infrastructure, and
privacy-preserving mobile applications.

</details>


### [86] [SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services](https://arxiv.org/abs/2510.14708)
*Ha Xuan Son,Nguyen Quoc Anh,Phat T. Tran-Truong,Le Thanh Tuan,Pham Thanh Nghiem*

Main category: cs.CR

TL;DR: SLIE是一种基于WKD-IBE的新型密码系统，为医疗物联网提供安全轻量级身份加密，显著提升加密效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网的服务导向模式引入了设备管理和通信中的重大安全漏洞，特别是医疗数据的敏感性需要更强的安全保障。

Method: 采用Wildcard密钥派生身份基加密(WKD-IBE)，实现端到端加密、分层访问控制和轻量级密钥管理系统，包含恒定时间操作、内存混淆和基于过期的密钥撤销机制。

Result: SLIE显著优于RSA，1KB数据的加密和解密时间分别为0.936ms和0.217ms，加密速度提升84.54%，解密速度提升99.70%，能效为0.014 J/KB。

Conclusion: SLIE为资源受限设备提供了可扩展的信任和安全全向通信，有效抵御侧信道、中间人和未授权访问攻击，确保符合HIPAA和GDPR标准。

Abstract: The Internet of Medical Things (IoMT) has revolutionized healthcare by
transforming medical operations into standardized, interoperable services.
However, this service-oriented model introduces significant security
vulnerabilities in device management and communication, which are especially
critical given the sensitivity of medical data. To address these risks, this
paper proposes SLIE (Secure and Lightweight Identity Encryption), a novel
cryptosystem based on Wildcard Key Derivation Identity-Based Encryption
(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication
through end-to-end encryption, hierarchical access control, and a lightweight
key management system designed for resource-constrained devices. It
incorporates constant-time operations, memory obfuscation, and expiry-based key
revocation to counter side-channel, man-in-the-middle, and unauthorized access
attacks, thereby ensuring compliance with standards like HIPAA and GDPR.
Evaluations show that SLIE significantly outperforms RSA, with encryption and
decryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement
in encryption speed, a 99.70% improvement in decryption speed, and an energy
efficiency of 0.014 J/KB.

</details>


### [87] [Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning](https://arxiv.org/abs/2510.14894)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To preserve privacy, multi-party computation (MPC) enables executing Machine
Learning (ML) algorithms on secret-shared or encrypted data. However, existing
MPC frameworks are not optimized for sparse data. This makes them unsuitable
for ML applications involving sparse data, e.g., recommender systems or
genomics. Even in plaintext, such applications involve high-dimensional sparse
data, that cannot be processed without sparsity-related optimizations due to
prohibitively large memory requirements.
  Since matrix multiplication is central in ML algorithms, we propose MPC
algorithms to multiply secret sparse matrices. On the one hand, our algorithms
avoid the memory issues of the "dense" data representation of classic secure
matrix multiplication algorithms. On the other hand, our algorithms can
significantly reduce communication costs (some experiments show a factor 1000)
for realistic problem sizes. We validate our algorithms in two ML applications
in which existing protocols are impractical.
  An important question when developing MPC algorithms is what assumptions can
be made. In our case, if the number of non-zeros in a row is a sensitive piece
of information then a short runtime may reveal that the number of non-zeros is
small. Existing approaches make relatively simple assumptions, e.g., that there
is a universal upper bound to the number of non-zeros in a row. This often
doesn't align with statistical reality, in a lot of sparse datasets the amount
of data per instance satisfies a power law. We propose an approach which allows
adopting a safe upper bound on the distribution of non-zeros in rows/columns of
sparse matrices.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [88] [Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach](https://arxiv.org/abs/2510.13819)
*George Stamatelis,Hui Chen,Henk Wymeersch,George C. Alexandropoulos*

Main category: cs.NI

TL;DR: 本文提出了一种基于可重构智能表面(RIS)的用户定位方法，采用基站到用户的反馈链路实现上行链路用户导频传输的动态功率控制，并开发了一种结合神经进化和监督学习的多智能体算法来联合控制RIS相位配置和用户发射功率。


<details>
  <summary>Details</summary>
Motivation: 研究利用RIS辅助用户定位，通过动态功率控制提高定位性能，支持离散响应的RIS元件，并减少反馈开销。

Method: 提出了一种混合方法，结合神经进化(NE)和监督学习，开发多智能体算法来联合控制RIS相位配置和用户发射功率，仅需单比特反馈消息。

Result: 数值结果表明，该方法在性能上优于指纹识别、深度强化学习基准和基于反向传播的位置估计器。

Conclusion: 所提出的方案能够有效利用RIS进行用户定位，在减少反馈开销的同时实现优越的定位性能。

Abstract: This paper studies user localization aided by a Reconfigurable Intelligent
Surface (RIS). A feedback link from the Base Station (BS) to the user is
adopted to enable dynamic power control of the user pilot transmissions in the
uplink. A novel multi-agent algorithm for the joint control of the RIS phase
configuration and the user transmit power is presented, which is based on a
hybrid approach integrating NeuroEvolution (NE) and supervised learning. The
proposed scheme requires only single-bit feedback messages for the uplink power
control, supports RIS elements with discrete responses, and is numerically
shown to outperform fingerprinting, deep reinforcement learning baselines and
backpropagation-based position estimators.

</details>


### [89] [DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G Networks](https://arxiv.org/abs/2510.14111)
*Taekyun Lee,Tommaso Balercia,Heasung Kim,Hyeji Kim,Jeffrey G. Andrews*

Main category: cs.NI

TL;DR: DiffLoc框架使用条件生成扩散模型直接从大规模MIMO信道状态信息实现高精度室外用户定位，在东京城市宏小区环境中达到亚厘米级精度，相比现有方法有数量级提升。


<details>
  <summary>Details</summary>
Motivation: 传统指纹定位方法难以扩展到大型动态室外环境，需要密集且不切实际的数据调查。为克服这些限制，需要直接从原始上行链路SRS指纹学习到连续地理坐标的映射。

Method: 应用条件生成扩散模型处理高维大规模MIMO CSI，学习从原始SRS指纹到连续地理坐标的直接映射，采用一致性训练减少推理步骤。

Result: DiffLoc-CT模型在融合精度上达到0.5厘米，单基站精度1-2厘米，相比监督回归方法（误差超过10米）和基于网格的融合（3米误差）有数量级改进。一致性训练将推理时间从200步减少到2步，保持高精度。

Conclusion: 该框架在高速用户（15-25米/秒）和未见用户轨迹下仍保持优异精度，展示了在实时6G应用中的实际可行性。

Abstract: This paper introduces a novel framework for high-accuracy outdoor user
equipment (UE) positioning that applies a conditional generative diffusion
model directly to high-dimensional massive MIMO channel state information
(CSI). Traditional fingerprinting methods struggle to scale to large, dynamic
outdoor environments and require dense, impractical data surveys. To overcome
these limitations, our approach learns a direct mapping from raw uplink
Sounding Reference Signal (SRS) fingerprints to continuous geographic
coordinates. We demonstrate that our DiffLoc framework achieves unprecedented
sub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm
fusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic,
ray-traced Tokyo urban macro-cell environment. This represents an
order-of-magnitude improvement over existing methods, including supervised
regression approaches (over 10 m error) and grid-based fusion (3 m error). Our
consistency training approach reduces inference time from 200 steps to just 2
steps while maintaining exceptional accuracy even for high-speed users (15-25
m/s) and unseen user trajectories, demonstrating the practical feasibility of
our framework for real-time 6G applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [90] [Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2510.14024)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 提出了一种名为"Pervasive Context Management"的技术，通过将LLM初始化上下文与推理过程解耦，在GPU中保留上下文直到不再需要，从而避免长等待队列和高启动成本。


<details>
  <summary>Details</summary>
Motivation: 当前HPC集群设计无法有效支持集成轻量级LLM与传统高吞吐量应用的新型工作负载，要么导致静态批处理队列的长时间等待，要么因资源抢占而反复支付昂贵的LLM启动成本。

Method: 将LLM初始化上下文与实际LLM推理解耦，在GPU中保留上下文直到不再需要，实现"Pervasive Context Management"技术。

Result: 事实验证应用的执行时间减少了72.1%（从3小时降至48分钟），使用相同数量的GPU；并且能够机会性地扩展到集群中32.8%的所有GPU，进一步将执行时间降至13分钟。

Conclusion: 该技术有效解决了HPC集群支持新型生成式AI工作负载时的性能瓶颈，显著提升了执行效率并实现了更好的资源利用。

Abstract: The rise of Generative AI introduces a new class of HPC workloads that
integrates lightweight LLMs with traditional high-throughput applications to
accelerate scientific discovery. The current design of HPC clusters is
inadequate to support this new class however, either incurring long wait times
on static batch queues or repeatedly paying expensive LLM startup costs upon
resource preemption. To circumvent both the long queues and high startup costs,
we propose to "decouple" the LLM initialization context from the actual LLM
inferences, and retain the context in GPUs until it is no longer needed, a
technique we term "Pervasive Context Management". We transform a fact
verification application to enable this technique, allowing it to reduce its
execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of
GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and
further reduce the execution time to 13 minutes.

</details>


### [91] [Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving](https://arxiv.org/abs/2510.14126)
*Nikos Pagonas,Yeounoh Chung,Kostis Kaffes,Arvind Krishnamurthy*

Main category: cs.DC

TL;DR: Cortex是一个面向智能体工作负载的原型工作流感知服务平台，通过阶段隔离策略为智能体工作流的每个不同阶段分配专用资源池，从而提高性能、吞吐量并减少干扰。


<details>
  <summary>Details</summary>
Motivation: 解决智能体工作流中不同阶段之间的计算和内存干扰问题，改善KV缓存利用率，提供更可预测的性能表现。

Method: 采用阶段隔离策略，为智能体工作流的每个不同阶段配置专用资源池，并定制每个阶段的资源分配和调度机制。

Result: 通过阶段隔离实现了更好的KV缓存利用率、更高的吞吐量和更可预测的性能，减少了阶段间干扰。

Conclusion: Cortex为更先进的智能体原生服务范式奠定了基础，包括可塑性资源管理、工作流分支的推测执行以及共享的多层智能体状态缓存。

Abstract: We introduce Cortex, a prototype workflow-aware serving platform designed for
agentic workloads. The core principle of Cortex is stage isolation: it
provisions dedicated resource pools for each distinct stage of an agentic
workflow. This simple yet powerful strategy mitigates inter-stage interference
in compute and memory, leading to better KV cache utilization, higher
throughput, and more predictable performance. By customizing resource
allocation and scheduling within each distinct stage of agentic workflows,
Cortex lays the groundwork for more advanced, agent-native serving paradigms,
including malleable resource management, speculative execution of workflow
branches, and a shared, multi-tiered cache for "agentic state."

</details>


### [92] [Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction](https://arxiv.org/abs/2510.14147)
*Gabriel Raulet,Dmitriy Morozov,Aydin Buluc,Katherine Yelick*

Main category: cs.DC

TL;DR: 提出了一种可扩展的分布式内存算法，使用覆盖树在通用度量空间中计算固定半径近邻图，在真实世界高维数据集上实现了显著的并行加速。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力和数据获取方法的发展，大型科学数据集需要可扩展的精确近邻图计算解决方案，特别是对于非欧几里得度量空间的需求。

Method: 开发了共享内存的覆盖树构建算法，并引入了两种分布式内存算法：简单点分区策略和空间分区策略，利用覆盖树算法在每个节点上执行。

Result: 在真实世界高维数据集上，使用1024核时实现了678.34倍的加速（平均每个顶点70个邻居），使用4096核时实现了1590.99倍的加速（平均每个顶点500个邻居）。

Conclusion: 该算法在各种真实和合成数据集上表现出良好的并行扩展性，为传统和非传统度量空间提供了高效的近邻图计算解决方案。

Abstract: Computing fixed-radius near-neighbor graphs is an important first step for
many data analysis algorithms. Near-neighbor graphs connect points that are
close under some metric, endowing point clouds with a combinatorial structure.
As computing power and data acquisition methods advance, diverse sources of
large scientific datasets would greatly benefit from scalable solutions to this
common subroutine for downstream analysis. Prior work on parallel nearest
neighbors has made great progress in problems like k-nearest and approximate
nearest neighbor search problems, with particular attention on Euclidean
spaces. Yet many applications need exact solutions and non-Euclidean metrics.
This paper presents a scalable sparsity-aware distributed memory algorithm
using cover trees to compute near-neighbor graphs in general metric spaces. We
provide a shared-memory algorithm for cover tree construction and demonstrate
its competitiveness with state-of-the-art fixed-radius search data structures.
We then introduce two distributed-memory algorithms for the near-neighbor graph
problem, a simple point-partitioning strategy and a spatial-partitioning
strategy, which leverage the cover tree algorithm on each node. Our algorithms
exhibit parallel scaling across a variety of real and synthetic datasets for
both traditional and non-traditional metrics. On real world high dimensional
datasets with one million points, we achieve speedups up to 678.34x over the
state-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on
average), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per
vertex (on average).

</details>


### [93] [Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs](https://arxiv.org/abs/2510.14186)
*Pengkun Ren,Hai Dong,Nasrin Sohrabi,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: AUTIG是一个高性能、可插拔的订单公平性服务，通过非对称架构解决传统BFT共识中订单公平性验证的计算冗余问题。它采用增量图维护和结构化证明验证，在不重新计算公平订单的情况下实现高效验证。


<details>
  <summary>Details</summary>
Motivation: 现有的订单公平共识协议（如Themis）要求每个副本重新运行领导者昂贵的排序计算来进行验证，这种对称冗余范式导致性能低下。需要打破这种对称性来提高性能。

Method: AUTIG采用非对称架构：领导者维护持久化的未确认交易增量图（UTIG）来分摊图构建成本，并发出结构化公平性证明；跟随者无需维护历史状态即可验证证明。关键创新包括增量图维护、解耦流水线和覆盖所有内部对加前沿完整性检查的证明设计。

Result: 实验评估显示，在部分同步条件下，AUTIG相比对称图基线具有更高的吞吐量和更低的端到端延迟，同时保持gamma批次订单公平性。

Conclusion: AUTIG通过非对称验证架构成功解决了订单公平共识中的计算冗余问题，实现了高性能的订单公平性服务，验证公平订单无需重新计算它。

Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on
transaction ordering despite malicious actors, but unconstrained ordering power
enables sophisticated value extraction attacks like front running and sandwich
attacks - a critical threat to blockchain systems. Order-fair consensus curbs
adversarial value extraction by constraining how leaders may order
transactions. While state-of-the-art protocols such as Themis attain strong
guarantees through graph-based ordering, they ask every replica to re-run the
leader's expensive ordering computation for validation - an inherently
symmetric and redundant paradigm. We present AUTIG, a high-performance,
pluggable order-fairness service that breaks this symmetry. Our key insight is
that verifying a fair order does not require re-computing it. Instead,
verification can be reduced to a stateless audit of succinct, verifiable
assertions about the ordering graph's properties. AUTIG realizes this via an
asymmetric architecture: the leader maintains a persistent
Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction
across rounds and emits a structured proof of fairness with each proposal;
followers validate the proof without maintaining historical state. AUTIG
introduces three critical innovations: (i) incremental graph maintenance driven
by threshold-crossing events and state changes; (ii) a decoupled pipeline that
overlaps leader-side collection/update/extraction with follower-side stateless
verification; and (iii) a proof design covering all internal pairs in the
finalized prefix plus a frontier completeness check to rule out hidden external
dependencies. We implement AUTIG and evaluate it against symmetric graph-based
baselines under partial synchrony. Experiments show higher throughput and lower
end-to-end latency while preserving gamma-batch-order-fairness.

</details>


### [94] [JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization](https://arxiv.org/abs/2510.14599)
*Michal Konopa,Jan Fesl,Ladislav Ber ánek*

Main category: cs.DC

TL;DR: JASDA是一个基于拍卖理论和在线优化的去中心化GPU调度框架，通过双向迭代交互实现自适应资源管理，适用于MIG-enabled GPU环境。


<details>
  <summary>Details</summary>
Motivation: 传统集中式调度在MIG-enabled GPU上面对复杂多变的工作负载时面临可扩展性挑战，需要新的调度范式。

Method: 扩展SJA概念，采用完全去中心化的协商过程，作业主动生成和评分可行子作业，调度器执行策略驱动的清算，平衡利用率、公平性和时间响应性。

Result: JASDA将反馈、校准和概率安全性嵌入调度循环，实现自适应和透明的决策制定。

Conclusion: JASDA为市场感知和公平驱动的资源管理提供了可扩展基础，连接了理论调度模型与现代MIG-enabled环境的实际部署。

Abstract: The increasing complexity and temporal variability of workloads on
MIG-enabled GPUs challenge the scalability of traditional centralized
scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel
paradigm that extends SJA from a largely centralized scheduling model toward a
fully decentralized negotiation process. In JASDA, jobs actively generate and
score feasible subjobs in response to scheduler-announced execution windows,
while the scheduler performs policy-driven clearing that balances utilization,
fairness, and temporal responsiveness. This bidirectional, iterative
interaction embeds feedback, calibration, and probabilistic safety directly
into the scheduling loop, enabling adaptive and transparent decision-making. By
coupling principles from auction theory and online optimization with the
temporal granularity of GPU workloads, JASDA provides a scalable foundation for
market-aware and fairness-driven resource management-bridging theoretical
scheduling models with practical deployment in modern MIG-enabled environments
relevant to Artificial Intelligence and Agriculture 4.0.

</details>


### [95] [MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems](https://arxiv.org/abs/2510.14622)
*Miryeong Kwon,Donghyun Gouk,Hyein Woo,Junhee Kim,Jinwoo Baek,Kyungkuk Nam,Sangyoon Ji,Jiseon Kim,Hanyeoreum Bae,Junhyeok Jang,Hyunwoo You,Junseok Moon,Myoungsoo Jung*

Main category: cs.DC

TL;DR: MPI-over-CXL是一种利用CXL技术的新型MPI通信范式，通过共享内存访问替代传统数据拷贝操作，显著降低通信延迟和内存带宽使用。


<details>
  <summary>Details</summary>
Motivation: 传统MPI实现依赖显式内存拷贝操作，导致冗余数据移动和缓冲区管理的开销，严重影响涉及密集处理器间通信的HPC工作负载性能。

Method: 利用CXL提供的跨多主机缓存一致性共享内存，将共享内存区域直接映射到MPI进程的虚拟地址空间，实现基于指针的高效通信，消除冗余拷贝操作。

Result: 使用代表性基准测试进行评估，相比传统MPI系统显示出显著的性能提升。

Conclusion: MPI-over-CXL有潜力在大规模HPC环境中提高效率和可扩展性。

Abstract: MPI implementations commonly rely on explicit memory-copy operations,
incurring overhead from redundant data movement and buffer management. This
overhead notably impacts HPC workloads involving intensive inter-processor
communication. In response, we introduce MPI-over-CXL, a novel MPI
communication paradigm leveraging CXL, which provides cache-coherent shared
memory across multiple hosts. MPI-over-CXL replaces traditional data-copy
methods with direct shared memory access, significantly reducing communication
latency and memory bandwidth usage. By mapping shared memory regions directly
into the virtual address spaces of MPI processes, our design enables efficient
pointer-based communication, eliminating redundant copying operations. To
validate this approach, we implement a comprehensive hardware and software
environment, including a custom CXL 3.2 controller, FPGA-based multi-host
emulation, and dedicated software stack. Our evaluations using representative
benchmarks demonstrate substantial performance improvements over conventional
MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and
scalability in large-scale HPC environments.

</details>


### [96] [xLLM Technical Report](https://arxiv.org/abs/2510.14686)
*Tongxuan Liu,Tao Peng,Peijun Yang,Xiaoyang Zhao,Xiusheng Lu,Weizhe Huang,Zirui Liu,Xiaoyu Chen,Zhiwei Liang,Jun Xiong,Donghe Jin,Minchao Zhang,Jinrong Guo,Yingxu Deng,Xu Zhang,Xianzhe Dong,Siqi Wang,Siyu Wu,Yu Wu,Zihan Tang,Yuting Zeng,Yanshu Wang,Jinguang Liu,Meng Kang,Menxin Li,Yunlong Wang,Yiming Liu,Xiaolong Ma,Yifan Wang,Yichen Zhang,Jinrun Yin,Keyang Zheng,Jiawei Yin,Jun Zhang,Ziyue Wang,Xiaobo Lin,Liangyu Liu,Liwei Lan,Yang Liu,Chunhua Peng,Han Liu,Songcheng Ren,Xuezhu Wang,Yunheng Shen,Yi Wang,Guyue Liu,Hui Chen,Tong Yang,Hailong Yang,Jing Li,Guiguang Ding,Ke Zhang*

Main category: cs.DC

TL;DR: xLLM是一个针对AI加速器优化的高效LLM推理框架，采用解耦的服务-引擎架构，通过智能调度、分布式KV缓存管理和多层级优化，显著提升吞吐量和推理效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模企业级LLM服务中的性能瓶颈和资源利用率问题，需要设计一个能够充分利用AI加速器的高性能推理框架。

Method: 构建解耦的服务-引擎架构：服务层包含智能调度模块、动态Prefill-Decode解耦策略和分布式KV缓存管理；引擎层通过多层级执行管道优化、自适应图模式和xTensor内存管理来饱和计算资源。

Result: 在相同TPOT约束下，xLLM的吞吐量达到MindIE的1.7倍和vLLM-Ascend的2.2倍（使用Qwen系列模型），使用Deepseek系列模型时平均吞吐量为MindIE的1.7倍。

Conclusion: xLLM通过系统与算法的协同优化，在大规模企业级LLM服务中实现了显著优越的性能和资源效率，框架已开源。

Abstract: We introduce xLLM, an intelligent and efficient Large Language Model (LLM)
inference framework designed for high-performance, large-scale enterprise-grade
serving, with deep optimizations for diverse AI accelerators. To address these
challenges, xLLM builds a novel decoupled service-engine architecture. At the
service layer, xLLM-Service features an intelligent scheduling module that
efficiently processes multimodal requests and co-locates online and offline
tasks through unified elastic scheduling to maximize cluster utilization. This
module also relies on a workload-adaptive dynamic Prefill-Decode (PD)
disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation
policy designed for multimodal inputs. Furthermore, it incorporates a
distributed architecture to provide global KV Cache management and robust
fault-tolerant capabilities for high availability. At the engine layer,
xLLM-Engine co-optimizes system and algorithm designs to fully saturate
computing resources. This is achieved through comprehensive multi-layer
execution pipeline optimizations, an adaptive graph mode and an xTensor memory
management. xLLM-Engine also further integrates algorithmic enhancements such
as optimized speculative decoding and dynamic EPLB, collectively serving to
substantially boost throughput and inference efficiency. Extensive evaluations
demonstrate that xLLM delivers significantly superior performance and resource
efficiency. Under identical TPOT constraints, xLLM achieves throughput up to
1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while
maintaining an average throughput of 1.7x that of MindIE with Deepseek-series
models. xLLM framework is publicly available at
https://github.com/jd-opensource/xllm and
https://github.com/jd-opensource/xllm-service.

</details>


### [97] [Balls and Bins and the Infinite Process with Random Deletions](https://arxiv.org/abs/2510.14798)
*Petra Berenbrink,Tom Friedetzky,Peter Kling,Lars Nagel*

Main category: cs.DC

TL;DR: 本文研究了一个无限球入箱过程，包含插入和删除操作。当插入时使用Greedy[2]策略（将球放入两个随机选择的箱中负载较低的那个），删除时随机从非空箱中移除一个球。论文分析了最大负载与平均负载的差异（discrepancy）和超载（overload），证明了discrepancy的上界为O(log(n))且下界匹配，overload的上界为loglog(n)+O(1)。


<details>
  <summary>Details</summary>
Motivation: 研究在动态插入和删除操作下的负载均衡问题，特别是在无限时间尺度上分析系统的负载分布特性，为分布式系统中的资源分配提供理论保证。

Method: 使用分层归纳法（layered induction）和详细的势能分析，通过概率耦合简化分析设置，获得恢复性质，避免复杂的条件分析。

Result: 证明了在任意时间点，超过平均负载的球数为O(n)，discrepancy为O(log(n))且下界匹配，overload为loglog(n)+O(1)。对于"良好"的插入概率序列，discrepancy可进一步限制为loglog(n)+O(1)。

Conclusion: 该无限球入箱过程在动态插入删除操作下能够保持良好的负载均衡特性，最大负载与平均负载的差异和超载都有理论保证的上界，为实际系统设计提供了理论依据。

Abstract: We consider an infinite balls-into-bins process with deletions where in each
discrete step $t$ a coin is tossed as to whether, with probability $\beta(t)
\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places
the ball in the lower loaded of two bins sampled uniformly at random) or, with
remaining probability $1-\beta(t)$, a ball is deleted from a non-empty bin
chosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total
load at time $t$. We are interested in bounding the discrepancy $x_{\max}(t) -
m(t)/n$ (current maximum load relative to current average) and the overload
$x_{\max}(t) - m_{\max}(t)/n$ (current maximum load relative to highest average
observed so far).
  We prove that at an arbitrarily chosen time $t$ the total number of balls
above the average is $O(n)$ and that the discrepancy is $ O(\log(n))$. For the
discrepancy, we provide a matching lower bound. Furthermore we prove that at an
arbitrarily chosen time $t$ the overload is $\log\log(n)+O(1)$. For "good"
insertion probability sequences (in which the average load of time intervals
with polynomial length increases in expectation) we show that even the
discrepancy is bounded by $\log\log(n)+O(1)$.
  One of our main analytical tools is a layered induction, as per [ABKU99].
Since our model allows for rather more general scenarios than what was
previously considered, the formal analysis requires some extra ingredients as
well, in particular a detailed potential analysis. Furthermore, we simplify the
setup by applying probabilistic couplings to obtain certain "recovery"
properties, which eliminate much of the need for intricate and careful
conditioning elsewhere in the analysis.

</details>
