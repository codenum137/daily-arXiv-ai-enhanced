<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 37]
- [cs.IT](#cs.IT) [Total: 9]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.LG](#cs.LG) [Total: 142]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Hierarchical Multi-Modal Threat Intelligence Fusion Without Aligned Data: A Practical Framework for Real-World Security Operations](https://arxiv.org/abs/2510.15953)
*Sisir Doppalapudi*

Main category: cs.CR

TL;DR: HM-TIF是一个专门为非对齐多模态安全数据设计的威胁检测框架，通过分层交叉注意力和时间相关协议，在不依赖对齐数据的情况下实现88.7%的检测准确率，并将误报率降低32%。


<details>
  <summary>Details</summary>
Motivation: 解决多模态威胁检测中安全工具孤立运行、数据流自然对齐缺失的现实挑战，为实际安全操作中常见的异构、非协调数据源提供解决方案。

Method: 采用分层交叉注意力机制（动态权重适应数据可用性和威胁上下文）和新型时间相关协议（保持统计独立性），避免数据泄露问题。

Result: 在UNSW-NB15、CSE-CIC-IDS2018和CICBell-DNS2021数据集上评估，达到88.7%准确率，误报率降低32%，且在模态缺失时保持鲁棒性。

Conclusion: HM-TIF是首个专门为非对齐数据设计的框架，证明多模态融合即使没有完美对齐也能提供操作优势，为安全团队提供实用部署指南。

Abstract: Multi-modal threat detection faces a fundamental challenge that involves
security tools operating in isolation, and this creates streams of network,
email, and system data with no natural alignment or correlation. We present
Hierarchical Multi-Modal Threat Intelligence Fusion (HM-TIF), a framework
explicitly designed for this realistic scenario where naturally aligned
multi-modal attack data does not exist. Unlike prior work that assumes or
creates artificial alignment, we develop principled methods for correlating
independent security data streams while maintaining operational validity. Our
architecture employs hierarchical cross-attention with dynamic weighting that
adapts to data availability and threat context, coupled with a novel temporal
correlation protocol that preserves statistical independence. Evaluation on
UNSW-NB15, CSE-CIC-IDS2018, and CICBell-DNS2021 datasets demonstrates that
HM-TIF achieves 88.7% accuracy with a critical 32% reduction in false positive
rates, even without true multi-modal training data. The framework maintains
robustness when modalities are missing, making it immediately deployable in
real security operations where data streams frequently have gaps. Our
contributions include: (i) the first multi-modal security framework explicitly
designed for non-aligned data, (ii) a temporal correlation protocol that avoids
common data leakage pitfalls, (iii) empirical validation that multi-modal
fusion provides operational benefits even without perfect alignment, and (iv)
practical deployment guidelines for security teams facing heterogeneous,
uncoordinated data sources. Index Terms: multi-modal learning, threat
intelligence, non-aligned data, operational security, cross-attention
mechanisms, practical deployment

</details>


### [2] [Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts](https://arxiv.org/abs/2510.15973)
*Tiarnaigh Downey-Webb,Olamide Jogunola,Oluwaseun Ajao*

Main category: cs.CR

TL;DR: 该论文对四个主流大语言模型（Phi-2、Llama-2-7B-Chat、GPT-3.5-Turbo、GPT-4）进行了系统性安全评估，测试了四种对抗攻击方法，发现模型安全性存在显著差异，Llama-2安全性最高，Phi-2最脆弱。


<details>
  <summary>Details</summary>
Motivation: 评估不同大语言模型对各种对抗攻击的鲁棒性，了解跨模型安全漏洞，为开发针对性防御机制提供依据。

Method: 使用SALAD-Bench数据集的1200个分层提示，测试四种攻击方法（人工编写提示、AutoDAN、GCG、TAP），涵盖六个危害类别，并进行统计显著性分析。

Result: Llama-2整体安全性最高（平均攻击成功率3.4%），Phi-2最脆弱（7.0%）。GCG和TAP攻击对目标模型无效，但转移到其他模型时成功率显著提高（GPT-4达17%）。恶意使用提示的攻击成功率最高（10.71%）。

Conclusion: 大语言模型安全性存在显著差异，攻击具有跨模型转移性，需要针对不同危害类别开发专门的防御机制。

Abstract: This paper presents a systematic security assessment of four prominent Large
Language Models (LLMs) against diverse adversarial attack vectors. We evaluate
Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack
categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),
and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs
1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six
harm categories. Results demonstrate significant variations in model
robustness, with Llama-2 achieving the highest overall security (3.4% average
attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%
average attack success rate). We identify critical transferability patterns
where GCG and TAP attacks, though ineffective against their target model
(Llama-2), achieve substantially higher success rates when transferred to other
models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals
significant differences in vulnerability across harm categories ($p < 0.001$),
with malicious use prompts showing the highest attack success rates (10.71%
average). Our findings contribute to understanding cross-model security
vulnerabilities and provide actionable insights for developing targeted defense
mechanisms

</details>


### [3] [Meta-Guardian: An Early Evaluation of an On-device Application to Mitigate Psychography Data Leakage in Immersive Technologies](https://arxiv.org/abs/2510.15989)
*Keshav Sood,Sanjay Selvaraj,Youyang Qu*

Main category: cs.CR

TL;DR: 提出了一个名为Meta-Guardian的隐私保护系统架构，可在VR头显内实时识别和过滤生物特征信号，防止敏感数据传输或存储。


<details>
  <summary>Details</summary>
Motivation: 沉浸式技术（XR）需要收集生物特征数据来创造沉浸式体验，但这些实时反馈信息（包括生物特征）由于数据的敏感性而引发严重的隐私担忧。现有文献大多忽视了头戴式显示系统中实时生物特征数据过滤的复杂性。

Method: 开发了一个模块化的Unity软件开发工具包（SDK），与主要沉浸式平台兼容，采用机器学习模型进行信号分类，并使用过滤机制来阻止敏感数据。

Result: 实现了一个隐私保护框架，使开发者能够在各种头显和应用程序中嵌入隐私设计原则。

Conclusion: Meta-Guardian系统为沉浸式技术提供了一种有效的隐私保护解决方案，通过实时生物特征信号过滤解决了关键的隐私问题。

Abstract: The use of Immersive Technologies has shown its potential to revolutionize
many sectors such as health, entertainment, education, and industrial sectors.
Immersive technologies such as Virtual Reality (VR), Augmented reality (AR),
and Mixed Reality (MR) have redefined user interaction through real-time
biometric and behavioral tracking. Although Immersive Technologies (XR)
essentially need the collection of the biometric data which acts as a baseline
to create immersive experience, however, this ongoing feedback information
(includes biometrics) poses critical privacy concerns due to the sensitive
nature of the data collected. A comprehensive review of recent literature
explored the technical dimensions of related problem; however, they largely
overlook the challenge particularly the intricacies of real-time biometric data
filtering within head-mounted display system. Motivated from this, in this
work, we propose a novel privacy-preserving system architecture that identifies
and filters biometric signals (within the VR headset) in real-time before
transmission or storage. Implemented as a modular Unity Software-development
Kit (SDK) compatible with major immersive platforms, our solution (named
Meta-Guardian) employs machine learning models for signal classification and a
filtering mechanism to block sensitive data. This framework aims to enable
developers to embed privacy-by-design principles into immersive experiences on
various headsets and applications.

</details>


### [4] [Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers](https://arxiv.org/abs/2510.16005)
*Giacomo Bertollo,Naz Bodemir,Jonah Burgess*

Main category: cs.CR

TL;DR: 对500名CTF参与者分析显示，简单AI防护易被绕过，但多层多步防御仍构成显著挑战，为构建更安全AI系统提供具体见解


<details>
  <summary>Details</summary>
Motivation: 研究AI系统的安全防护机制在实际攻击场景中的有效性，探索如何构建更安全的AI系统

Method: 通过分析500名CTF参与者的攻击行为，测试不同复杂度的AI防护机制

Result: 参与者能够轻松绕过简单的AI防护措施，但多层多步防御系统仍然构成显著挑战

Conclusion: 多层多步防御策略在保护AI系统安全方面具有重要价值，为构建更安全的AI系统提供了具体指导

Abstract: Analyzing 500 CTF participants, this paper shows that while participants
readily bypassed simple AI guardrails using common techniques, layered
multi-step defenses still posed significant challenges, offering concrete
insights for building safer AI systems.

</details>


### [5] [On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation](https://arxiv.org/abs/2510.16024)
*Abdulrahman Alhaidari,Balaji Palanisamy,Prashant Krishnamurthy*

Main category: cs.CR

TL;DR: 提出了首个去中心化、完全链上的学习框架，通过在Layer-2执行高gas消耗计算，将验证后的模型更新传播到Layer-1，并在智能合约内实现gas限制的低延迟推理。


<details>
  <summary>Details</summary>
Motivation: DeFi平台每年因业务逻辑或会计漏洞导致的交易损失数十亿美元，现有防御方法无法防止通过私有中继或恶意合约在同一区块内执行的攻击。

Method: 使用Proof-of-Improvement协议管理训练过程，通过量化技术和循环展开技术实现逻辑回归、SVM、MLP、CNN和门控RNN在以太坊区块gas限制内的推理。

Result: 收集了298个独特的真实世界攻击案例（2020-2025年），涉及402次攻击交易，覆盖8个EVM链，总损失达37.4亿美元。

Conclusion: 该框架能够有效检测和防止DeFi平台中的漏洞利用，通过链上学习机制提供实时保护。

Abstract: Billions of dollars are lost every year in DeFi platforms by transactions
exploiting business logic or accounting vulnerabilities. Existing defenses
focus on static code analysis, public mempool screening, attacker contract
detection, or trusted off-chain monitors, none of which prevents exploits
submitted through private relays or malicious contracts that execute within the
same block. We present the first decentralized, fully on-chain learning
framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce
cost, (ii) propagates verified model updates to Layer-1, and (iii) enables
gas-bounded, low-latency inference inside smart contracts. A novel
Proof-of-Improvement (PoIm) protocol governs the training process and verifies
each decentralized micro update as a self-verifying training transaction.
Updates are accepted by \textit{PoIm} only if they demonstrably improve at
least one core metric (e.g., accuracy, F1-score, precision, or recall) on a
public benchmark without degrading any of the other core metrics, while
adversarial proposals get financially penalized through an adaptable test set
for evolving threats. We develop quantization and loop-unrolling techniques
that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs
(with support for formally verified decision tree inference) within the
Ethereum block gas limit, while remaining bit-exact to their off-chain
counterparts, formally proven in Z3. We curate 298 unique real-world exploits
(2020 - 2025) with 402 exploit transactions across eight EVM chains,
collectively responsible for \$3.74 B in losses.

</details>


### [6] [A Multi-Cloud Framework for Zero-Trust Workload Authentication](https://arxiv.org/abs/2510.16067)
*Saurabh Deochake,Ryan Murphy,Jeremiah Gearheart*

Main category: cs.CR

TL;DR: 提出一个使用工作负载身份联盟和OpenID Connect的多云框架，实现无密钥认证，通过加密验证的临时令牌替代静态长期凭证，显著降低攻击面。


<details>
  <summary>Details</summary>
Motivation: 静态长期凭证存在安全风险，违反零信任原则，需要解决凭证盗窃问题。

Method: 采用工作负载身份联盟和OpenID Connect构建多云框架，使用加密验证的临时令牌进行认证，避免持久私钥的使用。

Result: 在企业级Kubernetes环境中验证了该框架，显著减少了攻击面。

Conclusion: 该模型提供了跨不同云平台统一管理工作负载身份的解决方案，为未来实现基于属性的访问控制奠定了基础。

Abstract: Static, long-lived credentials for workload authentication create untenable
security risks that violate Zero-Trust principles. This paper presents a
multi-cloud framework using Workload Identity Federation (WIF) and OpenID
Connect (OIDC) for secretless authentication. Our approach uses
cryptographically-verified, ephemeral tokens, allowing workloads to
authenticate without persistent private keys and mitigating credential theft.
We validate this framework in an enterprise-scale Kubernetes environment, which
significantly reduces the attack surface. The model offers a unified solution
to manage workload identities across disparate clouds, enabling future
implementation of robust, attribute-based access control.

</details>


### [7] [Membership Inference over Diffusion-models-based Synthetic Tabular Data](https://arxiv.org/abs/2510.16037)
*Peini Cheng,Amir Bahmani*

Main category: cs.CR

TL;DR: 本研究调查了基于扩散的合成表格数据生成方法存在的隐私风险，重点关注其对成员推理攻击(MIAs)的脆弱性。研究发现TabDDPM比TabSyn更容易受到此类攻击。


<details>
  <summary>Details</summary>
Motivation: 评估扩散模型在合成表格数据生成中的隐私影响，了解不同模型对成员推理攻击的脆弱性。

Method: 开发基于逐步误差比较方法的查询式成员推理攻击，对TabDDPM和TabSyn两个最新模型进行测试。

Result: TabDDPM对这些攻击表现出更高的脆弱性，而TabSyn则对我们的攻击模型表现出弹性。

Conclusion: 研究强调了评估扩散模型隐私影响的重要性，并鼓励进一步研究用于合成数据生成的鲁棒隐私保护机制。

Abstract: This study investigates the privacy risks associated with diffusion-based
synthetic tabular data generation methods, focusing on their susceptibility to
Membership Inference Attacks (MIAs). We examine two recent models, TabDDPM and
TabSyn, by developing query-based MIAs based on the step-wise error comparison
method. Our findings reveal that TabDDPM is more vulnerable to these attacks.
TabSyn exhibits resilience against our attack models. Our work underscores the
importance of evaluating the privacy implications of diffusion models and
encourages further research into robust privacy-preserving mechanisms for
synthetic data generation.

</details>


### [8] [PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)
*Zheng Hui,Yijiang River Dong,Sanhanat Sivapiromrat,Ehsan Shareghi,Nigel Collier*

Main category: cs.CR

TL;DR: 本文提出PrivacyPAD框架，使用强化学习动态路由文本块，在隐私保护和任务性能之间实现最优平衡。


<details>
  <summary>Details</summary>
Motivation: 用户在向大语言模型提交查询时面临两难选择：使用强大的专有LLM可能泄露敏感数据，而使用本地小模型虽能保证隐私但性能下降。现有静态方法会破坏语言连贯性并无差别删除隐私信息。

Method: 将隐私意识委托重新表述为顺序决策问题，引入基于强化学习的PrivacyPAD框架，训练智能体动态路由文本块，学习区分可替换PII（本地处理）和任务关键PII（远程处理）的策略。

Result: 在隐私-效用前沿上达到新的最先进水平，验证了在敏感环境中部署LLM时需要学习自适应策略的必要性。

Conclusion: PrivacyPAD框架通过强化学习实现了隐私保护与任务性能的最佳平衡，为敏感环境中的LLM部署提供了有效解决方案。

Abstract: When users submit queries to Large Language Models (LLMs), their prompts can
often contain sensitive data, forcing a difficult choice: Send the query to a
powerful proprietary LLM providers to achieving state-of-the-art performance
and risk data exposure, or relying on smaller, local models guarantees data
privacy but often results in a degradation of task performance. Prior
approaches have relied on static pipelines that use LLM rewriting, which
shatters linguistic coherence and indiscriminately removes privacy-sensitive
information, including task-critical content. We reformulate this challenge
(Privacy-Conscious Delegation) as a sequential decision-making problem and
introduce a novel reinforcement learning (RL) framework called PrivacyPAD to
solve it. Our framework trains an agent to dynamically route text chunks,
learning a policy that optimally balances the trade-off between privacy leakage
and task performance. It implicitly distinguishes between replaceable
Personally Identifiable Information (PII) (which it shields locally) and
task-critical PII (which it strategically sends to the remote model for maximal
utility). To validate our approach in complex scenarios, we also introduce a
new medical dataset with high PII density. Our framework achieves a new
state-of-the-art on the privacy-utility frontier, demonstrating the necessity
of learned, adaptive policies for deploying LLMs in sensitive environments.

</details>


### [9] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出了一种实用的卡上人脸验证匹配设计，使用64/128位紧凑模板，通过PCA-ITQ离线生成模板，在卡上使用恒定时间汉明距离进行比较。


<details>
  <summary>Details</summary>
Motivation: 满足ISO/IEC传输约束和隐私目标，通过短二进制模板、固定负载决策APDU和恒定时间匹配来实现安全高效的人脸验证。

Method: 使用PCA-ITQ生成紧凑模板，在卡上通过恒定时间汉明距离进行匹配，采用ISO/IEC 7816-4和14443-4命令APDU，包含固定长度负载和仅决策状态字。

Result: 在9.6 kbps速率下，64位验证时间43.9ms，128位52.3ms；在38.4 kbps下均小于14ms。在FAR=1%时，两种长度均达到TPR=0.836，128位相比64位降低了EER。

Conclusion: 该设计满足ISO/IEC传输约束，具有宽裕的时间余量，并与ISO/IEC 24745隐私目标一致。但存在单数据集评估和设计级时序的局限性，下一步将扩展到AgeDB/CFP-FP和卡上微基准测试。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [10] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 该论文系统比较了生成式和判别式分类器在成员推理攻击(MIA)中的脆弱性，发现显式建模联合概率P(X,Y)的完全生成式分类器最容易遭受成员信息泄露，揭示了分类器设计中固有的效用-隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式和判别式分类器在成员推理攻击中的系统性比较有限，需要填补这一空白，为隐私敏感应用中的分类器部署提供指导。

Method: 首先提供生成式分类器对MIA更敏感的理论动机，然后通过综合实证评估验证这些见解，涵盖判别式、生成式和伪生成式文本分类器，在9个基准数据集上使用多种MIA策略进行评估。

Result: 完全生成式分类器（显式建模联合似然P(X,Y)）对成员信息泄露最为脆弱，且生成式分类器中常用的典型推理方法显著放大了这种隐私风险。

Conclusion: 分类器设计存在固有的效用-隐私权衡，在隐私敏感应用中部署生成式分类器时需要格外谨慎，这为开发既能保持效用又能减轻成员推理漏洞的隐私保护生成式分类器指明了未来研究方向。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [11] [Prompt injections as a tool for preserving identity in GAI image descriptions](https://arxiv.org/abs/2510.16128)
*Kate Glazko,Jennifer Mankoff*

Main category: cs.CR

TL;DR: 该论文提出将提示注入作为一种工具，使间接用户能够保护其内容在生成式AI系统中的身份特征，特别是保留图像所有者的性别和残疾身份。


<details>
  <summary>Details</summary>
Motivation: 生成式AI存在偏见和缺乏代表性的风险，影响那些不直接与系统交互但内容被使用的间接用户。现有缓解方法大多需要自上而下或外部干预，需要一种更赋权的替代方案。

Method: 采用提示注入策略，将其重新定义为内容所有者抵抗的工具，而非恶意攻击向量。通过案例研究展示如何通过提示注入在GAI描述图像时保留所有者的性别和残疾身份。

Result: 提示注入能够有效帮助间接用户减轻对其的伤害，使他们能够从自己的内容内部进行自我保护。

Conclusion: 提示注入可以作为一种赋权工具，使间接用户能够主动保护其身份特征在生成式AI系统中的表达，提供了一种自下而上的危害缓解方法。

Abstract: Generative AI risks such as bias and lack of representation impact people who
do not interact directly with GAI systems, but whose content does: indirect
users. Several approaches to mitigating harms to indirect users have been
described, but most require top down or external intervention. An emerging
strategy, prompt injections, provides an empowering alternative: indirect users
can mitigate harm against them, from within their own content. Our approach
proposes prompt injections not as a malicious attack vector, but as a tool for
content/image owner resistance. In this poster, we demonstrate one case study
of prompt injections for empowering an indirect user, by retaining an image
owner's gender and disabled identity when an image is described by GAI.

</details>


### [12] [SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection](https://arxiv.org/abs/2510.16219)
*Yang Feng,Xudong Pan*

Main category: cs.CR

TL;DR: SentinelNet是一个去中心化框架，用于在多智能体系统中主动检测和缓解恶意行为，通过信用评估和对比学习训练检测器，在实验中实现了接近完美的恶意智能体检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法存在反应式设计或集中式架构的缺陷，可能导致单点故障，无法有效应对恶意智能体对多智能体系统可靠性和决策能力的威胁。

Method: 为每个智能体配备基于信用的检测器，通过对比学习在增强的对抗性辩论轨迹上训练，采用bottom-k消除进行动态邻居排名以抑制恶意通信，并生成对抗性轨迹解决攻击数据稀缺问题。

Result: 在多智能体系统基准测试中，SentinelNet在两轮辩论内实现接近100%的恶意智能体检测率，从受损基线恢复95%的系统准确率，并在跨领域和攻击模式上表现出强泛化能力。

Conclusion: SentinelNet为保护协作多智能体系统建立了一个新的范式，通过去中心化架构和主动检测机制有效应对恶意威胁。

Abstract: Malicious agents pose significant threats to the reliability and
decision-making capabilities of Multi-Agent Systems (MAS) powered by Large
Language Models (LLMs). Existing defenses often fall short due to reactive
designs or centralized architectures which may introduce single points of
failure. To address these challenges, we propose SentinelNet, the first
decentralized framework for proactively detecting and mitigating malicious
behaviors in multi-agent collaboration. SentinelNet equips each agent with a
credit-based detector trained via contrastive learning on augmented adversarial
debate trajectories, enabling autonomous evaluation of message credibility and
dynamic neighbor ranking via bottom-k elimination to suppress malicious
communications. To overcome the scarcity of attack data, it generates
adversarial trajectories simulating diverse threats, ensuring robust training.
Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection
of malicious agents, close to 100% within two debate rounds, and recovers 95%
of system accuracy from compromised baselines. By exhibiting strong
generalizability across domains and attack patterns, SentinelNet establishes a
novel paradigm for safeguarding collaborative MAS.

</details>


### [13] [C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations](https://arxiv.org/abs/2510.16229)
*Vienna Li,Justin Villa,Dan Diessner,Jayson Clifford,Laxima Niure Kandel*

Main category: cs.CR

TL;DR: 本文提出了一种基于卫星载噪比(C/N₀)变化的GPS欺骗检测方法，通过分析天线在不同方向（水平、右倾、左倾）时的C/N₀变化模式来识别欺骗信号。


<details>
  <summary>Details</summary>
Motivation: GPS欺骗对航空安全构成日益严重的威胁，需要开发有效的检测方法来保护飞机导航系统免受误导。

Method: 使用u-blox EVK-M8U接收器和GPSG-1000卫星模拟器，在真实天空（非欺骗）和欺骗环境下收集三种天线方向（水平、右倾、左倾）的C/N₀数据。

Result: 非欺骗信号下C/N₀随方向自然波动，而欺骗信号显示独特模式：水平方向（正对欺骗天线）C/N₀最高，倾斜方向因与欺骗源错位而C/N₀降低。

Conclusion: 简单的机动动作（如短暂倾斜）引起的C/N₀变化可为通用航空和无人机系统提供GPS欺骗的早期检测线索。

Abstract: GPS spoofing poses a growing threat to aviation by falsifying satellite
signals and misleading aircraft navigation systems. This paper demonstrates a
proof-of-concept spoofing detection strategy based on analyzing satellite
Carrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static
antenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite
simulator, C/N$_0$ data is collected under three antenna orientations flat,
banked right, and banked left) in both real-sky (non-spoofed) and spoofed
environments. Our findings reveal that under non-spoofed signals, C/N$_0$
values fluctuate naturally with orientation, reflecting true geometric
dependencies. However, spoofed signals demonstrate a distinct pattern: the flat
orientation, which directly faces the spoofing antenna, consistently yielded
the highest C/N$_0$ values, while both banked orientations showed reduced
C/N$_0$ due to misalignment with the spoofing source. These findings suggest
that simple maneuvers such as brief banking to induce C/N$_0$ variations can
provide early cues of GPS spoofing for general aviation and UAV systems.

</details>


### [14] [Efficient and Privacy-Preserving Binary Dot Product via Multi-Party Computation](https://arxiv.org/abs/2510.16331)
*Fatemeh Jafarian Dehkordi,Elahe Vedadi,Alireza Feizbakhsh,Yasaman Keshtkarjahromi,Hulya Seferoglu*

Main category: cs.CR

TL;DR: 本文提出了一种新颖的二进制多方计算框架BiMPC，专门针对树型垂直联邦学习中的位运算场景，解决了现有隐私保护技术在二进制数据操作方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器学习中，平衡数据隐私保护和协同计算是一个关键挑战。虽然联邦学习的隐私保护技术已广泛发展，但涉及位运算的场景（如树型垂直联邦学习）仍缺乏有效方法。传统的秘密共享和多方计算机制对二进制数据的位运算不够优化。

Method: 提出了BiMPC框架，核心是DoMA方法，使用常规和模加法进行高效的二进制点积计算。通过在高阶域中使用随机掩码进行线性计算，以及使用三方不经意传输协议进行非线性二进制运算来确保隐私。

Result: BiMPC框架的隐私保证经过严格分析，证明其在分布式环境中具有高效性和可扩展性。

Conclusion: BiMPC框架有效解决了二进制多方计算中的隐私保护问题，特别适用于树型垂直联邦学习等需要位运算的场景。

Abstract: Striking a balance between protecting data privacy and enabling collaborative
computation is a critical challenge for distributed machine learning. While
privacy-preserving techniques for federated learning have been extensively
developed, methods for scenarios involving bitwise operations, such as
tree-based vertical federated learning (VFL), are still underexplored.
Traditional mechanisms, including Shamir's secret sharing and multi-party
computation (MPC), are not optimized for bitwise operations over binary data,
particularly in settings where each participant holds a different part of the
binary vector. This paper addresses the limitations of existing methods by
proposing a novel binary multi-party computation (BiMPC) framework. The BiMPC
mechanism facilitates privacy-preserving bitwise operations, with a particular
focus on dot product computations of binary vectors, ensuring the privacy of
each individual bit. The core of BiMPC is a novel approach called Dot Product
via Modular Addition (DoMA), which uses regular and modular additions for
efficient binary dot product calculation. To ensure privacy, BiMPC uses random
masking in a higher field for linear computations and a three-party oblivious
transfer (triot) protocol for non-linear binary operations. The privacy
guarantees of the BiMPC framework are rigorously analyzed, demonstrating its
efficiency and scalability in distributed settings.

</details>


### [15] [EditMark: Watermarking Large Language Models based on Model Editing](https://arxiv.org/abs/2510.16367)
*Shuai Li,Kejiang Chen,Jun Jiang,Jie Zhang,Qiyi Yao,Kai Zeng,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: EditMark是一种基于模型编辑的LLM水印方法，无需训练即可嵌入水印，具有高效、隐蔽且不影响模型性能的特点。


<details>
  <summary>Details</summary>
Motivation: 保护LLM版权并追踪未经授权使用，现有水印方法需要重新训练模型，成本高且影响性能，水印文本不自然。

Method: 利用模型编辑技术，为多答案问题分配唯一水印，通过自适应多轮稳定编辑策略和噪声矩阵注入来嵌入水印。

Result: 在20秒内嵌入32位水印，水印提取成功率100%，相比微调方法(6875秒)效率显著提升，具有良好的保真度、隐蔽性和抗攻击能力。

Conclusion: EditMark提供了一种训练免费、隐蔽且性能无损的LLM水印方案，有效解决了现有方法的成本和性能问题。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, but
their training requires extensive data and computational resources, rendering
them valuable digital assets. Therefore, it is essential to watermark LLMs to
protect their copyright and trace unauthorized use or resale. Existing methods
for watermarking LLMs primarily rely on training LLMs with a watermarked
dataset, which entails burdensome training costs and negatively impacts the
LLM's performance. In addition, their watermarked texts are not logical or
natural, thereby reducing the stealthiness of the watermark. To address these
issues, we propose EditMark, the first watermarking method that leverages model
editing to embed a training-free, stealthy, and performance-lossless watermark
for LLMs. We observe that some questions have multiple correct answers.
Therefore, we assign each answer a unique watermark and update the weights of
LLMs to generate corresponding questions and answers through the model editing
technique. In addition, we refine the model editing technique to align with the
requirements of watermark embedding. Specifically, we introduce an adaptive
multi-round stable editing strategy, coupled with the injection of a noise
matrix, to improve both the effectiveness and robustness of the watermark
embedding. Extensive experiments indicate that EditMark can embed 32-bit
watermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a
watermark extraction success rate of 100%, which demonstrates its effectiveness
and efficiency. External experiments further demonstrate that EditMark has
fidelity, stealthiness, and a certain degree of robustness against common
attacks.

</details>


### [16] [$ρ$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching](https://arxiv.org/abs/2510.16544)
*Weijie Chen,Shan Tang,Yulin Tang,Xiapu Luo,Yinqian Zhang,Weizhong Qiang*

Main category: cs.CR

TL;DR: ρHammer是一种新的Rowhammer攻击框架，通过地址映射逆向工程、预取式锤击和反推测执行技术，有效突破了最新Intel架构的防御机制，在Raptor Lake等最新平台上成功恢复了Rowhammer攻击能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于负载的Rowhammer攻击在Intel Alder和Raptor Lake等最新架构上变得高度无效，需要开发新的攻击方法来克服现代内存控制器的防御机制。

Method: 1) 使用选择性成对测量和结构化推导的高效DRAM地址映射逆向工程方法；2) 基于预取指令的异步特性和多bank并行化的新型锤击范式；3) 利用控制流混淆和优化的NOP伪屏障的反推测执行锤击技术。

Result: 在四个最新Intel架构上的评估显示：在2小时攻击模式模糊测试中诱导超过20万额外比特翻转，在Comet和Rocket Lake上的翻转率比基线高112倍，首次在Raptor Lake上实现稳定翻转率2291/分钟和快速端到端利用。

Conclusion: ρHammer框架成功克服了现代架构的三个核心挑战，证明了Rowhammer威胁在最新硬件上仍然存在，需要更强的防御措施。

Abstract: Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)
that continues to pose a significant threat to various systems. However, we
find that conventional load-based attacks are becoming highly ineffective on
the most recent architectures such as Intel Alder and Raptor Lake. In this
paper, we present $\rho$Hammer, a new Rowhammer framework that systematically
overcomes three core challenges impeding attacks on these new architectures.
First, we design an efficient and generic DRAM address mapping
reverse-engineering method that uses selective pairwise measurements and
structured deduction, enabling recovery of complex mappings within seconds on
the latest memory controllers. Second, to break through the activation rate
bottleneck of load-based hammering, we introduce a novel prefetch-based
hammering paradigm that leverages the asynchronous nature of x86 prefetch
instructions and is further enhanced by multi-bank parallelism to maximize
throughput. Third, recognizing that speculative execution causes more severe
disorder issues for prefetching, which cannot be simply mitigated by memory
barriers, we develop a counter-speculation hammering technique using
control-flow obfuscation and optimized NOP-based pseudo-barriers to maintain
prefetch order with minimal overhead. Evaluations across four latest Intel
architectures demonstrate $\rho$Hammer's breakthrough effectiveness: it induces
up to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes
and has a 112x higher flip rate than the load-based hammering baselines on
Comet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on
the latest Raptor Lake architecture, where baselines completely fail, achieving
stable flip rates of 2,291/min and fast end-to-end exploitation.

</details>


### [17] [Toward Understanding Security Issues in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2510.16558)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 本文首次对MCP生态系统进行全面的安全分析，揭示了由于缺乏输出验证机制和服务器审查流程，恶意服务器可以操纵模型行为并引发各种安全威胁，包括敏感数据泄露。


<details>
  <summary>Details</summary>
Motivation: MCP生态系统发展迅速但缺乏系统性的安全研究，需要分析其架构和相关安全风险。

Method: 将MCP生态系统分解为主机、注册中心和服务器三个核心组件，研究它们之间的交互和信任关系，收集并分析来自6个公共注册中心的67,057个服务器数据集。

Result: 定性分析显示主机缺乏LLM生成输出的验证机制，定量分析表明大量服务器可被攻击者劫持，发现广泛漏洞使攻击者能够劫持服务器。

Conclusion: 提出了针对MCP主机、注册中心和用户的实用防御策略，并向受影响方负责任地披露了发现。

Abstract: The Model Context Protocol (MCP) is an emerging open standard that enables
AI-powered applications to interact with external tools through structured
metadata. A rapidly growing ecosystem has formed around MCP, including a wide
range of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP
registries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),
and thousands of community-contributed MCP servers. Although the MCP ecosystem
is gaining traction, there has been little systematic study of its architecture
and associated security risks. In this paper, we present the first
comprehensive security analysis of the MCP ecosystem. We decompose MCP
ecosystem into three core components: hosts, registries, and servers, and study
the interactions and trust relationships among them. Users search for servers
on registries and configure them in the host, which translates LLM-generated
output into external tool invocations provided by the servers and executes
them. Our qualitative analysis reveals that hosts lack output verification
mechanisms for LLM-generated outputs, enabling malicious servers to manipulate
model behavior and induce a variety of security threats, including but not
limited to sensitive data exfiltration. We uncover a wide range of
vulnerabilities that enable attackers to hijack servers, due to the lack of a
vetted server submission process in registries. To support our analysis, we
collect and analyze a dataset of 67,057 servers from six public registries. Our
quantitative analysis demonstrates that a substantial number of servers can be
hijacked by attackers. Finally, we propose practical defense strategies for MCP
hosts, registries, and users. We responsibly disclosed our findings to affected
hosts and registries.

</details>


### [18] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: Patronus是一个保护文本到图像模型免受白盒攻击的防御框架，通过内部调节器和不可微调的学习机制来防止恶意内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有的安全措施在面对知道模型参数并能调整参数的白盒攻击者时失效，需要一种能抵御此类攻击的全面保护方案。

Method: 设计内部调节器将不安全输入特征解码为零向量，同时保持良性输入特征的解码性能；采用不可微调的学习机制增强模型对齐，防止恶意微调。

Result: 实验验证了在安全内容生成上的性能完整性，以及拒绝不安全内容生成的有效性，确认了Patronus对各种白盒微调攻击的抵御能力。

Conclusion: Patronus框架为T2I模型提供了全面的白盒攻击防护，确保模型在安全内容生成上的性能同时有效阻止不安全内容的生成。

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


### [19] [DESTinE Block: Private Blockchain Based Data Storage Framework for Power System](https://arxiv.org/abs/2510.16593)
*Khandaker Akramul Haque,Katherine R. Davis*

Main category: cs.CR

TL;DR: DESTinE Block是一个基于区块链的数据存储框架，专为电力系统设计，优化用于资源受限环境如单板计算机。它结合IPFS存储大文件和自定义区块链存储元数据，采用双区块链抽象和PoA共识机制，实现高效安全的去中心化数据存储。


<details>
  <summary>Details</summary>
Motivation: 为电力系统在资源受限的边缘设备上提供安全、防篡改的数据存储解决方案，解决传统集中式存储的安全性和可靠性问题。

Method: 采用双区块链抽象架构，使用IPFS存储大文件，自定义DESTinE区块链存储元数据（CID、上传者身份、管理员验证、时间戳）。基于PoA共识机制，需要管理员和上传者共同协作创建区块，确保计算效率。

Result: 在x86和ARM64设备（包括树莓派5）上成功测试，与基于Multichain的类似框架相比表现良好，证明了在分布式电力系统基础设施中实现防篡改数据保留的可行性。

Conclusion: DESTinE Block为智能电网应用提供了一个有前景的安全、去中心化日志和测量存储解决方案，同时保持最低硬件要求。

Abstract: This paper presents DESTinE Block, a blockchain-based data storage framework
designed for power systems and optimized for resource-constrained environments,
including grid-edge devices such as single-board computers. The proposed
architecture leverages the InterPlanetary File System (IPFS) for storing large
files while maintaining secure and traceable metadata on a custom blockchain
named DESTinE Block. The metadata, comprising the IPFS Content Identifier
(CID), uploader identity, administrator verification, and timestamp; is
immutably recorded on-chain to ensure authenticity and integrity. DESTinE Block
adopts a dual-blockchain abstraction, where the blockchain remains unaware of
the IPFS storage layer to enhance security and limit the exposure of sensitive
file data. The consensus mechanism is based on Proof of Authority (PoA), where
both an administrator and an uploader with distinct cryptographic key pairs are
required to create a block collaboratively. Each block contains verified
signatures of both parties and is designed to be computationally efficient,
enabling deployment on devices like the Raspberry Pi 5. The framework was
tested on both an x86-based device and an ARM64-based Raspberry Pi,
demonstrating its potential for secure, decentralized logging and measurement
storage in smart grid applications. Moreover, DESTinE Block is compared with a
similar framework based on Multichain. The results indicate that DESTinE Block
provides a promising solution for tamper-evident data retention in distributed
power system infrastructure while maintaining minimal hardware requirements.

</details>


### [20] [DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](https://arxiv.org/abs/2510.16716)
*Asmita Mohanty,Gezheng Kang,Lei Gao,Murali Annavaram*

Main category: cs.CR

TL;DR: DistilLock是一个基于可信执行环境(TEE)的边缘设备微调框架，通过安全黑盒教师模型实现隐私保护的知识蒸馏，同时保护数据隐私和模型知识产权。


<details>
  <summary>Details</summary>
Motivation: 解决LLM微调中数据隐私和模型知识产权保护的两难问题：云端微调需要上传敏感数据，边缘微调需要传输专有模型存在IP泄露风险。

Method: 在数据所有者设备的TEE enclave中执行专有基础模型作为安全黑盒教师，采用模型混淆机制将混淆权重卸载到不可信加速器进行高效知识蒸馏。

Result: DistilLock能够防止未经授权的知识蒸馏过程和模型窃取攻击，同时保持高计算效率。

Conclusion: DistilLock为基于边缘的LLM个性化提供了一个安全实用的解决方案，同时保护了数据隐私和模型IP。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
diverse tasks, but fine-tuning them typically relies on cloud-based,
centralized infrastructures. This requires data owners to upload potentially
sensitive data to external servers, raising serious privacy concerns. An
alternative approach is to fine-tune LLMs directly on edge devices using local
data; however, this introduces a new challenge: the model owner must transfer
proprietary models to the edge, which risks intellectual property (IP) leakage.
To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning
framework that enables privacy-preserving knowledge distillation on the edge.
In DistilLock, a proprietary foundation model is executed within a trusted
execution environment (TEE) enclave on the data owner's device, acting as a
secure black-box teacher. This setup preserves both data privacy and model IP
by preventing direct access to model internals. Furthermore, DistilLock employs
a model obfuscation mechanism to offload obfuscated weights to untrusted
accelerators for efficient knowledge distillation without compromising
security. We demonstrate that DistilLock prevents unauthorized knowledge
distillation processes and model-stealing attacks while maintaining high
computational efficiency, but offering a secure and practical solution for
edge-based LLM personalization.

</details>


### [21] [Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022](https://arxiv.org/abs/2510.16744)
*Srinivas Vivek*

Main category: cs.CR

TL;DR: 本文展示了对Xie等人提出的隐私保护叫车服务（PP-RHS）协议的被动攻击，该攻击允许服务提供商完全恢复每次叫车请求中乘客和司机的精确位置。


<details>
  <summary>Details</summary>
Motivation: Xie等人在NSS 2022提出的PP-RHS协议旨在保护乘客和司机的位置隐私，但本文发现该协议存在严重安全漏洞，需要揭示这些漏洞以改进协议安全性。

Method: 采用被动攻击方法，攻击者（服务提供商）能够在不被检测的情况下，通过分析协议交互数据完全恢复乘客和司机的精确位置信息。

Result: 攻击成功实现了对乘客和司机位置的完全恢复，且攻击效率很高，不受安全参数的限制。

Conclusion: Xie等人提出的PP-RHS协议存在严重的位置隐私泄露漏洞，需要重新设计或改进以提供真正的隐私保护。

Abstract: Ride-Hailing Services (RHS) match a ride request initiated by a rider with a
suitable driver responding to the ride request. A Privacy-Preserving RHS
(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'
and drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie
et al. proposed a PP-RHS. In this work, we demonstrate a passive attack on
their PP-RHS protocol. Our attack allows the SP to completely recover the
locations of the rider as well as that of the responding drivers in every ride
request. Further, our attack is very efficient as it is independent of the
security parameter.

</details>


### [22] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: Verifiable Fine Tuning是一种协议和系统，通过零知识证明验证模型是从公共初始化模型按照声明的训练程序和可审计数据集承诺进行微调得到的。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型微调发布实践在数据使用和更新计算方面提供的保证较弱，存在信任缺口，特别是在受监管和去中心化部署场景中。

Method: 结合五个要素：数据集承诺绑定、可验证采样器、参数高效微调更新电路、递归证明聚合、来源绑定和可信执行属性卡。

Result: 在英语和双语指令混合数据集上，该方法在严格预算内保持实用性，同时实现实用的证明性能。策略配额零违规，私有采样窗口无索引泄漏。

Conclusion: 端到端可验证微调对于真实的参数高效管道是可行的，为受监管和去中心化部署填补了关键的信任缺口。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [23] [Addendum: Systematic Evaluation of Randomized Cache Designs against Cache Occupancy](https://arxiv.org/abs/2510.16871)
*Anirban Chakraborty,Nimish Mishra,Sayandeep Saha,Sarani Bhattacharya,Debdeep Mukhopadhyay*

Main category: cs.CR

TL;DR: 本文是对USENIX Security 2025主论文的补充说明，讨论了随机化缓存设计中缓存占用率在性能和安全性方面的作用，并回应了[2]中关于L1d缓存大小影响攻击成功率以及MIRAGE修复版本防止AES密钥泄漏的观察。


<details>
  <summary>Details</summary>
Motivation: 系统分析随机化缓存设计中缓存占用率的作用，从性能和安全性两个角度进行公平比较和威胁评估，并回应相关研究的新发现。

Method: 提出统一的基准测试策略来公平比较不同随机化缓存设计；从安全角度提出三种威胁假设：隐蔽信道、进程指纹侧信道和AES密钥恢复攻击。

Result: 发现设计在效率和安全性上与现代化组相联LLC相当的随机化缓存仍然是一个开放性问题；[2]的研究表明L1d缓存大小影响攻击成功率，修复版MIRAGE能防止AES密钥泄漏。

Conclusion: 设计既高效又能抵抗基于争用和基于占用率攻击的随机化缓存仍然是一个未解决的开放问题，需要进一步研究。

Abstract: In the main text published at USENIX Security 2025, we presented a systematic
analysis of the role of cache occupancy in the design considerations for
randomized caches (from the perspectives of performance and security). On the
performance front, we presented a uniform benchmarking strategy that allows for
a fair comparison among different randomized cache designs. Likewise, from the
security perspective, we presented three threat assumptions: (1) covert
channels; (2) process fingerprinting side-channel; and (3) AES key recovery.
The main takeaway of our work is an open problem of designing a randomized
cache of comparable efficiency with modern set-associative LLCs, while still
resisting both contention-based and occupancy-based attacks. This note is meant
as an addendum to the main text in light of the observations made in [2]. To
summarize, the authors in [2] argue that (1) L1d cache size plays a role in
adversarial success, and that (2) a patched version of MIRAGE with randomized
initial seeding of global eviction map prevents leakage of AES key. We discuss
the same in this addendum.

</details>


### [24] [On the Credibility of Deniable Communication in Court](https://arxiv.org/abs/2510.16873)
*Jacob Leiken,Sunoo Park*

Main category: cs.CR

TL;DR: 该论文挑战了密码学可否认系统在法庭证据中的传统理解，提出了更广泛的"可信度"概念，强调现实世界中证据认证涉及密码学模型之外的社会技术因素。


<details>
  <summary>Details</summary>
Motivation: 作者认为密码学文献中对可否认系统的理解过于狭窄，主要关注法庭场景中的证据否认，而忽视了现实世界中证据认证涉及的多维度因素。

Method: 通过分析现实世界证据认证过程，提出了可信度模型，该模型包含三个关键方面：伪造品必须达到的可信度阈值、创建通过该阈值伪造品的难易程度，以及系统默认保留策略。

Result: 研究发现密码学可否认性概念与现实应用存在差距，可信度模型能够更好地捕捉现实威胁模型中的安全需求。

Conclusion: 可信度模型有助于设计更安全的通信系统，支持在特定法律和社会技术背景下对密码学保证的优缺点进行更细致的讨论。

Abstract: Over time, cryptographically deniable systems have come to be associated in
computer-science literature with the idea of "denying" evidence in court -
specifically, with the ability to convincingly forge evidence in courtroom
scenarios and an inability to authenticate evidence in such contexts.
Evidentiary processes in courts, however, have been developed over centuries to
account for the reality that evidence has always been forgeable, and relies on
factors outside of cryptographic models to seek the truth "as well as possible"
while acknowledging that all evidence is imperfect. We argue that deniability
does not and need not change this paradigm.
  Our analysis highlights a gap between technical deniability notions and their
application to the real world. There will always be factors outside a
cryptographic model that influence perceptions of a message's authenticity, in
realistic situations. We propose the broader concept of credibility to capture
these factors. The credibility of a system is determined by (1) a threshold of
quality that a forgery must pass to be "believable" as an original
communication, which varies based on sociotechnical context and threat model,
(2) the ease of creating a forgery that passes this threshold, which is also
context- and threat-model-dependent, and (3) default system retention policy
and retention settings. All three aspects are important for designing secure
communication systems for real-world threat models, and some aspects of (2) and
(3) may be incorporated directly into technical system design. We hope that our
model of credibility will facilitate system design and deployment that
addresses threats that are not and cannot be captured by purely technical
definitions and existing cryptographic models, and support more nuanced
discourse on the strengths and limitations of cryptographic guarantees within
specific legal and sociotechnical contexts.

</details>


### [25] [UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks](https://arxiv.org/abs/2510.16923)
*Mansi Phute,Matthew Hull,Haoran Wang,Alec Helbling,ShengYun Peng,Willian Lunardi,Martin Andreoni,Wenke Lee,Polo Chau*

Main category: cs.CR

TL;DR: UNDREAM是一个软件框架，首次将照片级逼真模拟器与可微分渲染器结合，支持对3D物体进行端到端的对抗性扰动优化，可在自动驾驶等安全关键应用中测试深度学习模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器不可微分，导致研究人员无法在模拟环境中集成环境因素创建对抗攻击，降低了攻击成功率。

Method: 通过UNDREAM框架，用户可以完全控制天气、光照、背景、相机角度、轨迹以及真实的人类和物体运动，从而创建多样化场景。

Result: 展示了UNDREAM能够快速生成多种不同的物理上合理的对抗性物体，并在不同可配置环境中进行探索。

Conclusion: 照片级逼真模拟与可微分优化的结合为物理对抗攻击研究开辟了新途径。

Abstract: Deep learning models deployed in safety critical applications like autonomous
driving use simulations to test their robustness against adversarial attacks in
realistic conditions. However, these simulations are non-differentiable,
forcing researchers to create attacks that do not integrate simulation
environmental factors, reducing attack success. To address this limitation, we
introduce UNDREAM, the first software framework that bridges the gap between
photorealistic simulators and differentiable renderers to enable end-to-end
optimization of adversarial perturbations on any 3D objects. UNDREAM enables
manipulation of the environment by offering complete control over weather,
lighting, backgrounds, camera angles, trajectories, and realistic human and
object movements, thereby allowing the creation of diverse scenes. We showcase
a wide array of distinct physically plausible adversarial objects that UNDREAM
enables researchers to swiftly explore in different configurable environments.
This combination of photorealistic simulation and differentiable optimization
opens new avenues for advancing research of physical adversarial attacks.

</details>


### [26] [Efficient derandomization of differentially private counting queries](https://arxiv.org/abs/2510.16959)
*Surendra Ghentiyala*

Main category: cs.CR

TL;DR: 本文提出了一种多项式时间机制，用于差分隐私下的d个计数查询，仅需O(log d)比特随机性，改进了之前非多项式时间的方法。


<details>
  <summary>Details</summary>
Motivation: 2020年人口普查的差分隐私需要90TB随机性，成本过高。研究旨在降低差分隐私中计数查询的随机性复杂度。

Method: 通过随机偏移每个计数查询的答案，观察到许多查询结果在添加噪声前后保持不变，从而避免对许多查询添加噪声步骤。

Result: 构建了多项式时间机制，实现了与之前非多项式时间方法几乎相同的随机性复杂度与准确性权衡。

Conclusion: 新机制提供了更清晰的视角来理解通过批处理d个计数查询可以获得的随机性节省来源，且不依赖复杂的舍入方案。

Abstract: Differential privacy for the 2020 census required an estimated 90 terabytes
of randomness [GL20], an amount which may be prohibitively expensive or
entirely infeasible to generate. Motivated by these practical concerns, [CSV25]
initiated the study of the randomness complexity of differential privacy, and
in particular, the randomness complexity of $d$ counting queries. This is the
task of outputting the number of entries in a dataset that satisfy predicates
$\mathcal{P}_1, \dots, \mathcal{P}_d$ respectively. They showed the rather
surprising fact that though any reasonably accurate,
$\varepsilon$-differentially private mechanism for one counting query requires
$1-O(\varepsilon)$ bits of randomness in expectation, there exists a fairly
accurate mechanism for $d$ counting queries which requires only $O(\log d)$
bits of randomness in expectation.
  The mechanism of [CSV25] is inefficient (not polynomial time) and relies on a
combinatorial object known as rounding schemes. Here, we give a polynomial time
mechanism which achieves nearly the same randomness complexity versus accuracy
tradeoff as that of [CSV25]. Our construction is based on the following simple
observation: after a randomized shift of the answer to each counting query, the
answer to many counting queries remains the same regardless of whether we add
noise to that coordinate or not. This allows us to forgo the step of adding
noise to the result of many counting queries. Our mechanism does not make use
of rounding schemes. Therefore, it provides a different -- and, in our opinion,
clearer -- insight into the origins of the randomness savings that can be
obtained by batching $d$ counting queries. Therefore, it provides a different
-- and, in our opinion, clearer -- insight into the origins of the randomness
savings that can be obtained by batching $d$ counting queries.

</details>


### [27] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 本文提出了一个信息论框架来分析大型语言模型(LLM)中对抗性攻击的信息泄露问题，通过计算观测信号与目标属性之间的互信息来量化信息泄露程度，并推导出攻击所需查询次数与泄露率的关系。


<details>
  <summary>Details</summary>
Motivation: 当前LLM面临恶意用户的对抗性攻击，攻击者通过观察模型回复中的各种信号来推断未知目标属性。然而信息泄露的程度缺乏系统性分析，审计者缺乏原则性指导，防御者难以权衡透明度与风险。

Method: 采用信息论框架，将观测信号Z与目标属性T之间的互信息I(Z;T)作为每次查询泄露的比特数，推导出达到误差ε所需的最少查询次数为log(1/ε)/I(Z;T)。

Result: 在7个LLM上进行的系统提示泄露、越狱和重新学习攻击实验验证了理论：仅暴露答案token需要约1000次查询；添加logits可减少到约100次；显示完整思考过程仅需几十次查询。

Conclusion: 该研究为LLM部署中透明度与安全性的平衡提供了首个原则性衡量标准，表明即使适度增加信息披露也会显著降低攻击成本。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [28] [Watermark Robustness and Radioactivity May Be at Odds in Federated Learning](https://arxiv.org/abs/2510.17033)
*Leixu Huang,Zedian Shao,Teodora Baluta*

Main category: cs.CR

TL;DR: 本文提出在联邦学习中应用LLM水印技术进行数据溯源，发现水印具有放射性（训练后仍可检测），但服务器可通过鲁棒聚合过滤水印更新，揭示了放射性、鲁棒性和效用之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习中越来越多使用LLM生成的数据，数据溯源对于问责制和透明度变得至关重要。需要开发能够在分布式环境中追踪数据来源的方法。

Method: 将LLM水印技术适配到联邦学习环境中，让部分客户端在水印数据上计算本地更新，服务器对所有更新进行平均得到全局LLM。同时研究服务器作为主动对抗者使用鲁棒聚合过滤水印更新的情况。

Result: 水印具有放射性，即使在仅6.6%数据被水印的情况下，p值仍可达10^-24。但服务器通过强鲁棒聚合可以有效过滤水印信号，所有评估的放射性水印都无法抵抗这种主动过滤。

Conclusion: 研究揭示了在联邦学习环境中数据溯源技术面临的基本权衡：水印的放射性、对抗攻击的鲁棒性以及模型效用之间存在不可调和的矛盾。

Abstract: Federated learning (FL) enables fine-tuning large language models (LLMs)
across distributed data sources. As these sources increasingly include
LLM-generated text, provenance tracking becomes essential for accountability
and transparency. We adapt LLM watermarking for data provenance in FL where a
subset of clients compute local updates on watermarked data, and the server
averages all updates into the global LLM. In this setup, watermarks are
radioactive: the watermark signal remains detectable after fine-tuning with
high confidence. The $p$-value can reach $10^{-24}$ even when as little as
$6.6\%$ of data is watermarked. However, the server can act as an active
adversary that wants to preserve model utility while evading provenance
tracking. Our observation is that updates induced by watermarked synthetic data
appear as outliers relative to non-watermark updates. Our adversary thus
applies strong robust aggregation that can filter these outliers, together with
the watermark signal. All evaluated radioactive watermarks are not robust
against such an active filtering server. Our work suggests fundamental
trade-offs between radioactivity, robustness, and utility.

</details>


### [29] [Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability](https://arxiv.org/abs/2510.17087)
*Ziqing Zhu*

Main category: cs.CR

TL;DR: 本文提出了一种面向虚拟电厂(VPP)的量子密钥感知调度框架，将稀缺的量子密钥作为优先级调度资源进行管理，解决了传统PKI在高频跨域通信中的延迟和量子安全威胁问题。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟电厂运营向分钟级和秒级反馈发展，通信安全从合规要求转变为运营约束。传统PKI和密钥轮换方案难以应对跨域高频消息传递，且面临长期量子威胁。量子密钥分发(QKD)虽然提供信息论安全的新鲜密钥，但其密钥产量稀缺且随机，与VPP突发流量不匹配。

Method: 提出密钥感知优先级和配额框架，结合：(1)预测驱动的长期配额和短期令牌；(2)密钥感知的赤字轮询仲裁；(3)抢占式应急密钥储备；(4)通过加密模式切换和非关键流量受控降采样的优雅降级。采用漂移加惩罚分析建立强稳定性保证。

Result: 在IEEE 33和123总线VPP系统上的实验表明，相比FIFO、固定优先级和静态配额基线，所提方案显著减少关键消息的尾部延迟和被动超时，提高每比特密钥效用，在密钥稀缺和状态切换期间增强功率跟踪可靠性。

Conclusion: 该密钥感知调度框架为虚拟电厂提供了可量化的运营保证，在量子密钥稀缺条件下实现了高效、安全的通信调度，为未来电网的量子安全通信提供了可行解决方案。

Abstract: Virtual power plants (VPPs) are becoming a cornerstone of future grids,
aggregating distributed PV, wind, storage, and flexible loads for market
participation and real-time balancing. As operations move to minute-- and
second--level feedback, communication security shifts from a compliance item to
an operational constraint: latency, reliability, and confidentiality jointly
determine whether dispatch, protection, and settlement signals arrive on time.
Conventional PKI and key-rotation schemes struggle with cross-domain,
high-frequency messaging and face long-term quantum threats. Quantum key
distribution (QKD) offers information-theoretic key freshness, but its key
yield is scarce and stochastic, often misaligned with bursty VPP traffic. This
paper proposes a key-aware priority and quota framework that treats quantum
keys as first-class scheduling resources. The design combines (i)
forecast-driven long-term quotas and short-term tokens, (ii) key-aware
deficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and
(iv) graceful degradation via encryption-mode switching and controlled
down-sampling for non-critical traffic. A drift-plus-penalty analysis
establishes strong stability under average supply--demand balance with
quantifiable bounds on backlog and tail latency, providing interpretable
operating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus
VPP systems and evaluate normal, degraded, and outage regimes with
industry-consistent message classes and TTLs. Against FIFO, fixed-priority, and
static-quota baselines, the proposed scheme consistently reduces tail delay and
passive timeouts for critical messages, improves per-bit key utility, and
enhances power-tracking reliability during key scarcity and regime switches.

</details>


### [30] [QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR](https://arxiv.org/abs/2510.17175)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 本文提出QR"iS方法，通过分析QR码的结构特征来检测钓鱼攻击，相比传统黑盒方法具有更好的可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: 当前QR码钓鱼攻击(Quishing)日益严重，现有防御方法多依赖黑盒技术，缺乏可解释性和透明度，存在信任度、责任归属和偏见检测等问题。

Method: 开发简单算法从QR码布局模式中提取24个结构特征，然后训练机器学习模型进行分类，并开发移动应用进行实际验证。

Result: 在40万样本数据集上获得最高83.18%的准确率，并通过与相关研究的对比分析验证了方法的有效性。

Conclusion: QR"iS方法通过结构分析实现了透明、可复现的QR码分类，为防范钓鱼攻击提供了可行的解决方案。

Abstract: Globally, individuals and organizations employ Quick Response (QR) codes for
swift and convenient communication. Leveraging this, cybercriminals embed
falsify and misleading information in QR codes to launch various phishing
attacks which termed as Quishing. Many former studies have introduced defensive
approaches to preclude Quishing such as by classifying the embedded content of
QR codes and then label the QR codes accordingly, whereas other studies
classify them using visual features (i.e., deep features, histogram density
analysis features). However, these approaches mainly rely on black-box
techniques which do not clearly provide interpretability and transparency to
fully comprehend and reproduce the intrinsic decision process; therefore,
having certain obvious limitations includes the approaches' trust,
accountability, issues in bias detection, and many more. We proposed QR\"iS,
the pioneer method to classify QR codes through the comprehensive structural
analysis of a QR code which helps to identify phishing QR codes beforehand. Our
classification method is clearly transparent which makes it reproducible,
scalable, and easy to comprehend. First, we generated QR codes dataset (i.e.
400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike
black-box models, we developed a simple algorithm to extract 24 structural
features from layout patterns present in QR codes. Later, we train the machine
learning models on the harvested features and obtained accuracy of up to
83.18%. To further evaluate the effectiveness of our approach, we perform the
comparative analysis of proposed method with relevant contemporary studies.
Lastly, for real-world deployment and validation, we developed a mobile app
which assures the feasibility of the proposed solution in real-world scenarios
which eventually strengthen the applicability of the study.

</details>


### [31] [Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography](https://arxiv.org/abs/2510.17220)
*Giulia Giusti*

Main category: cs.CR

TL;DR: 该论文探讨了线性逻辑在编程范式中的应用，分为ADLL和CryptoBLL两部分。ADLL将线性逻辑应用于自动微分，连接JAX类型系统与线性逻辑理论；CryptoBLL使用线性逻辑表达计算密码学中对手的复杂性约束。


<details>
  <summary>Details</summary>
Motivation: 线性在数学和计算机科学中具有互补但不同的含义。数学中线性支撑函数和向量空间，计算机科学中线性涉及资源敏感计算。线性逻辑能够建模必须恰好使用一次的假设，为跟踪计算资源提供自然框架。

Method: 论文采用两部分方法：1) ADLL：将线性逻辑应用于自动微分，建模实数上的线性函数和转置操作；2) CryptoBLL：使用线性逻辑表达计算密码学中对手的复杂性约束，提出自动分析协议的框架。

Result: ADLL连接了JAX的类型系统与线性逻辑理论，弥合了自动微分中基于证明理论的理论方法和JAX实现的实践方法之间的差距。CryptoBLL解决了密码学框架在表达性和简单性之间的权衡问题。

Conclusion: 线性逻辑为编程语言、类型系统和形式模型提供了表达计算复杂性和可组合性的框架。通过桥接数学和计算机科学中的线性概念，能够为分析和验证复杂系统提供严谨而实用的方法论。

Abstract: The concept of linearity plays a central role in both mathematics and
computer science, with distinct yet complementary meanings. In mathematics,
linearity underpins functions and vector spaces, forming the foundation of
linear algebra and functional analysis. In computer science, it relates to
resource-sensitive computation. Linear Logic (LL), for instance, models
assumptions that must be used exactly once, providing a natural framework for
tracking computational resources such as time, memory, or data access. This
dual perspective makes linearity essential to programming languages, type
systems, and formal models that express both computational complexity and
composability. Bridging these interpretations enables rigorous yet practical
methodologies for analyzing and verifying complex systems.
  This thesis explores the use of LL to model programming paradigms based on
linearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to
Automatic Differentiation (AD), modeling linear functions over the reals and
the transposition operation. The latter uses LL to express complexity
constraints on adversaries in computational cryptography.
  In AD, two main approaches use linear type systems: a theoretical one
grounded in proof theory, and a practical one implemented in JAX, a Python
library developed by Google for machine learning research. In contrast,
frameworks like PyTorch and TensorFlow support AD without linear types. ADLL
aims to bridge theory and practice by connecting JAX's type system to LL.
  In modern cryptography, several calculi aim to model cryptographic proofs
within the computational paradigm. These efforts face a trade-off between
expressiveness, to capture reductions, and simplicity, to abstract probability
and complexity. CryptoBLL addresses this tension by proposing a framework for
the automatic analysis of protocols in computational cryptography.

</details>


### [32] [Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks](https://arxiv.org/abs/2510.17277)
*Xinkai Wang,Beibei Li,Zerui Shao,Ao Liu,Shouling Ji*

Main category: cs.CR

TL;DR: 该论文提出了PolyJailbreak方法，通过强化学习在多模态大语言模型中实现越狱攻击，利用视觉对齐导致的安全不对称性来绕过安全约束。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在现实应用中存在安全漏洞，容易受到越狱攻击，导致产生不道德响应。研究发现视觉对齐在不同模态间施加了不均匀的安全约束，形成了多模态安全不对称性。

Method: 开发了基于强化学习的黑盒越狱方法PolyJailbreak：首先探测模型的注意力动态和潜在表示空间，评估视觉输入如何重塑跨模态信息流；然后系统化为可泛化和可重用的操作规则，构建原子策略原语库；最后通过多智能体优化过程自动调整输入对抗目标模型。

Result: 在多种开源和闭源MLLMs上进行了全面评估，证明PolyJailbreak优于现有最先进的基线方法。

Conclusion: 多模态安全不对称性是MLLMs的重要安全漏洞，PolyJailbreak方法有效利用了这种不对称性，实现了高效的越狱攻击，揭示了MLLMs安全机制需要进一步改进。

Abstract: Multimodal large language models (MLLMs) have demonstrated significant
utility across diverse real-world applications. But MLLMs remain vulnerable to
jailbreaks, where adversarial inputs can collapse their safety constraints and
trigger unethical responses. In this work, we investigate jailbreaks in the
text-vision multimodal setting and pioneer the observation that visual
alignment imposes uneven safety constraints across modalities in MLLMs, thereby
giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a
black-box jailbreak method grounded in reinforcement learning. Initially, we
probe the model's attention dynamics and latent representation space, assessing
how visual inputs reshape cross-modal information flow and diminish the model's
ability to separate harmful from benign inputs, thereby exposing exploitable
vulnerabilities. On this basis, we systematize them into generalizable and
reusable operational rules that constitute a structured library of Atomic
Strategy Primitives, which translate harmful intents into jailbreak inputs
through step-wise transformations. Guided by the primitives, PolyJailbreak
employs a multi-agent optimization process that automatically adapts inputs
against the target models. We conduct comprehensive evaluations on a variety of
open-source and closed-source MLLMs, demonstrating that PolyJailbreak
outperforms state-of-the-art baselines.

</details>


### [33] [Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values](https://arxiv.org/abs/2510.17284)
*Jiri Gavenda,Petr Svenda,Stanislav Bobon,Vladimir Sedlacek*

Main category: cs.CR

TL;DR: 本文分析了比特币CoinJoin协议的隐私保护效果，发现主要协议（Whirlpool、Wasabi 1.x和2.x）的匿名集大小平均减少10-50%，并开发了更精确的隐私评估方法。


<details>
  <summary>Details</summary>
Motivation: CoinJoin协议旨在通过协作交易提升比特币隐私性，但隐私增益评估因多种影响因素和计算复杂性而成为未解难题。

Method: 改进BlockSci链上分析软件以分析CoinJoin交易，设计考虑混币费用、实现限制和用户后混币行为的并行化隐私评估方法。

Result: 所有三种主要CoinJoin设计的匿名集大小平均减少10-50%，减少幅度在混币后第一天最高，一年后可忽略。对真实Wasabi 2.x混币的分析表明，即使考虑用户不良后混币行为，正确归属硬币仍非常困难。

Conclusion: 尽管用户后混币行为不理想，但使用改进的分析算法正确识别硬币所有者仍然极其困难，CoinJoin协议仍能提供有效的隐私保护。

Abstract: A coinjoin protocol aims to increase transactional privacy for Bitcoin and
Bitcoin-like blockchains via collaborative transactions, by violating
assumptions behind common analysis heuristics. Estimating the resulting privacy
gain is a crucial yet unsolved problem due to a range of influencing factors
and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions,
demonstrating a significant (10-50%) average post-mix anonymity set size
decrease for all three major designs with a central coordinator: Whirlpool,
Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and
negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method,
which takes into account coinjoin fees, implementation-specific limitations and
users' post-mix behavior. We evaluate our method in detail on a set of emulated
and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world
coinjoins with hundreds of inputs and outputs. We conclude that despite the
users' undesirable post-mix behavior, correctly attributing the coins to their
owners is still very difficult, even with our improved analysis algorithm.

</details>


### [34] [Single-Shuffle Full-Open Card-Based Protocols for Any Function](https://arxiv.org/abs/2510.17308)
*Reo Eriguchi,Kazumasa Shinagawa*

Main category: cs.CR

TL;DR: 本文首次证明了所有函数都存在单次洗牌全公开协议，提出了两种在卡片数量和洗牌复杂度之间权衡的构造方法，并展示了仅公开部分卡片的协议变体。


<details>
  <summary>Details</summary>
Motivation: 单次洗牌全公开协议是卡牌安全计算的最小模型，但之前已知可实现的函数类别有限，仅限于少数小例子。本文旨在证明所有函数都可以通过这种最小模型实现安全计算。

Method: 通过建立单次洗牌全公开协议与密码学原语"私有同时消息协议"之间的新颖联系，提出了两种构造方法：一种在卡片数量和洗牌复杂度之间提供权衡，另一种仅公开部分卡片以减少洗牌复杂度。

Result: 首次证明了所有函数都存在单次洗牌全公开协议，并提供了具体的构造方法。与现有协议相比，提出的变体协议显著降低了洗牌操作的复杂度。

Conclusion: 这项研究扩展了卡牌安全计算的能力边界，证明了即使是最小模型也能实现任意函数的安全计算，为卡牌密码学提供了新的理论基础和实用构造。

Abstract: A card-based secure computation protocol is a method for $n$ parties to
compute a function $f$ on their private inputs $(x_1,\ldots,x_n)$ using
physical playing cards, in such a way that the suits of revealed cards leak no
information beyond the value of $f(x_1,\ldots,x_n)$. A \textit{single-shuffle
full-open} protocol is a minimal model of card-based secure computation in
which, after the parties place face-down cards representing their inputs, a
single shuffle operation is performed and then all cards are opened to derive
the output. Despite the simplicity of this model, the class of functions known
to admit single-shuffle full-open protocols has been limited to a few small
examples. In this work, we prove for the first time that every function admits
a single-shuffle full-open protocol. We present two constructions that offer a
trade-off between the number of cards and the complexity of the shuffle
operation. These feasibility results are derived from a novel connection
between single-shuffle full-open protocols and a cryptographic primitive known
as \textit{Private Simultaneous Messages} protocols, which has rarely been
studied in the context of card-based cryptography. We also present variants of
single-shuffle protocols in which only a subset of cards are revealed. These
protocols reduce the complexity of the shuffle operation compared to existing
protocols in the same setting.

</details>


### [35] [The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment](https://arxiv.org/abs/2510.17311)
*Eduard Marin,Jinwoo Kim,Alessio Pavoni,Mauro Conti,Roberto Di Pietro*

Main category: cs.CR

TL;DR: 本文首次对公共存储库中的无服务器组件安全状况进行全面分析，发现存在系统性漏洞，包括过时软件包、敏感参数误用、可被利用的部署配置、易受域名抢注攻击等问题。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算已成为重要云范式，公共无服务器存储库对加速无服务器应用开发至关重要，但其日益流行使其成为攻击者目标，而这些存储库的安全状况尚未得到充分研究，使开发者和组织面临潜在风险。

Method: 分析了来自5个广泛使用的公共存储库的2,758个无服务器组件，以及跨3个广泛使用的IaC框架的125,936个基础设施即代码模板。

Result: 分析揭示了系统性漏洞，包括过时软件包、敏感参数误用、可被利用的部署配置、易受域名抢注攻击以及在压缩无服务器组件中嵌入恶意行为的机会。

Conclusion: 最后提供了实用的建议来缓解这些威胁。

Abstract: Serverless computing has rapidly emerged as a prominent cloud paradigm,
enabling developers to focus solely on application logic without the burden of
managing servers or underlying infrastructure. Public serverless repositories
have become key to accelerating the development of serverless applications.
However, their growing popularity makes them attractive targets for
adversaries. Despite this, the security posture of these repositories remains
largely unexplored, exposing developers and organizations to potential risks.
In this paper, we present the first comprehensive analysis of the security
landscape of serverless components hosted in public repositories. We analyse
2,758 serverless components from five widely used public repositories popular
among developers and enterprises, and 125,936 Infrastructure as Code (IaC)
templates across three widely used IaC frameworks. Our analysis reveals
systemic vulnerabilities including outdated software packages, misuse of
sensitive parameters, exploitable deployment configurations, susceptibility to
typo-squatting attacks and opportunities to embed malicious behaviour within
compressed serverless components. Finally, we provide practical recommendations
to mitigate these threats.

</details>


### [36] [CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks](https://arxiv.org/abs/2510.17687)
*Xu Zhang,Hao Li,Zhichao Lu*

Main category: cs.CR

TL;DR: ImpForge是一个自动化红队管道，使用强化学习生成多样化的隐式攻击样本，CrossGuard是基于此开发的意图感知安全防护系统，能有效防御显式和隐式威胁。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLMs)在推理和感知方面表现出色，但越来越容易受到越狱攻击。现有研究主要关注显式攻击，而隐式攻击（良性文本和图像输入共同表达不安全意图）难以检测且研究不足，主要由于高质量隐式数据稀缺。

Method: 提出ImpForge自动化红队管道，利用强化学习配合定制奖励模块在14个领域生成多样化隐式样本。基于此数据集开发CrossGuard意图感知安全防护系统。

Result: 在安全和不安全基准测试、隐式和显式攻击以及多个域外设置上的广泛实验表明，CrossGuard显著优于现有防御方法，包括先进的MLLMs和防护机制，在保持高实用性的同时实现更强的安全性。

Conclusion: 这项工作为增强MLLMs对抗现实世界多模态威胁的鲁棒性提供了一个平衡且实用的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong reasoning and
perception capabilities but are increasingly vulnerable to jailbreak attacks.
While existing work focuses on explicit attacks, where malicious content
resides in a single modality, recent studies reveal implicit attacks, in which
benign text and image inputs jointly express unsafe intent. Such joint-modal
threats are difficult to detect and remain underexplored, largely due to the
scarcity of high-quality implicit data. We propose ImpForge, an automated
red-teaming pipeline that leverages reinforcement learning with tailored reward
modules to generate diverse implicit samples across 14 domains. Building on
this dataset, we further develop CrossGuard, an intent-aware safeguard
providing robust and comprehensive defense against both explicit and implicit
threats. Extensive experiments across safe and unsafe benchmarks, implicit and
explicit attacks, and multiple out-of-domain settings demonstrate that
CrossGuard significantly outperforms existing defenses, including advanced
MLLMs and guardrails, achieving stronger security while maintaining high
utility. This offers a balanced and practical solution for enhancing MLLM
robustness against real-world multimodal threats.

</details>


### [37] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V是一个基于变分推理的多模态越狱发现框架，通过学习文本-图像提示对的联合后验分布，生成隐蔽的对抗性输入来绕过视觉语言模型的防护机制。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态红队方法依赖脆弱的模板，关注单攻击设置，只能暴露有限的漏洞。需要更系统的方法来发现多模态模型的潜在安全风险。

Method: 采用变分推理框架，将多模态越狱发现建模为学习文本-图像提示对的联合后验分布。集成三种互补策略：基于排版的文本提示、基于扩散的图像合成和结构化干扰器。

Result: 在HarmBench和HADES基准测试中，VERA-V在开源和前沿VLM上持续优于现有基线，在GPT-4o上攻击成功率比最佳基线高出53.75%。

Conclusion: VERA-V提供了一个概率框架来系统发现多模态模型的漏洞，证明了变分推理在多模态安全评估中的有效性。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [38] [A Semantic Generalization of Shannon's Information Theory and Applications](https://arxiv.org/abs/2510.15871)
*Chenguang Lu*

Main category: cs.IT

TL;DR: 本文提出了一种语义信息理论（G理论），通过用语义约束替代失真约束来推广香农信息理论，并展示了其在多个领域的应用。


<details>
  <summary>Details</summary>
Motivation: 探讨语义通信是否需要独立的语义信息理论，还是可以通过推广香农信息理论来实现语义通信。

Method: 使用一组真值函数作为语义通道，将失真约束替换为语义约束，从而表达语义失真、语义信息度量和语义信息损失。

Result: 最大语义信息准则等价于最大似然准则，类似于正则化最小二乘准则。G理论可应用于日常和电子语义通信、机器学习、约束控制等多个领域。

Conclusion: G理论成功推广了香农信息理论用于语义通信，但存在表示复杂数据语义的局限性。

Abstract: Does semantic communication require a semantic information theory parallel to
Shannon's information theory, or can Shannon's work be generalized for semantic
communication? This paper advocates for the latter and introduces a semantic
generalization of Shannon's information theory (G theory for short). The core
idea is to replace the distortion constraint with the semantic constraint,
achieved by utilizing a set of truth functions as a semantic channel. These
truth functions enable the expressions of semantic distortion, semantic
information measures, and semantic information loss. Notably, the maximum
semantic information criterion is equivalent to the maximum likelihood
criterion and similar to the Regularized Least Squares criterion. This paper
shows G theory's applications to daily and electronic semantic communication,
machine learning, constraint control, Bayesian confirmation, portfolio theory,
and information value. The improvements in machine learning methods involve
multilabel learning and classification, maximum mutual information
classification, mixture models, and solving latent variables. Furthermore,
insights from statistical physics are discussed: Shannon information is similar
to free energy; semantic information to free energy in local equilibrium
systems; and information efficiency to the efficiency of free energy in
performing work. The paper also proposes refining Friston's minimum free energy
principle into the maximum information efficiency principle. Lastly, it
compares G theory with other semantic information theories and discusses its
limitation in representing the semantics of complex data.

</details>


### [39] [Enhancing Channel Estimation in RIS-aided Systems via Observation Matrix Design](https://arxiv.org/abs/2510.16576)
*Zijian Zhang,Mingyao Cui*

Main category: cs.IT

TL;DR: 本文提出了一种基于贝叶斯优化的可重构智能表面信道估计观测矩阵设计方法，采用交替黎曼流形优化算法来更新接收机组合器和RIS相移矩阵，并通过自适应核训练策略迭代优化信道协方差矩阵。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面(RIS)通过密集天线阵列增强无线通信性能，但需要准确的信道估计来充分发挥其潜力。现有方法在观测矩阵设计方面存在不足，需要更有效的信道估计器。

Method: 采用贝叶斯优化框架设计观测矩阵以最大化接收导频信号与RIS信道之间的互信息，开发交替黎曼流形优化算法交替更新接收机组合器和RIS相移矩阵，并引入自适应核训练策略迭代优化信道协方差矩阵。

Result: 仿真结果表明，所提出的ARMO增强估计器在估计精度方面相比最先进方法实现了显著提升。

Conclusion: 提出的基于贝叶斯优化和交替黎曼流形优化的观测矩阵设计方法能够有效提升RIS信道估计性能，为RIS技术在无线通信中的应用提供了更准确的信道估计解决方案。

Abstract: Reconfigurable intelligent surfaces (RISs) have emerged as a promising
technology for enhancing wireless communications through dense antenna arrays.
Accurate channel estimation is critical to unlocking their full performance
potential. To enhance RIS channel estimators, this paper proposes a novel
observation matrix design scheme. Bayesian optimization framework is adopted to
generate observation matrices that maximize the mutual information between
received pilot signals and RIS channels. To solve the formulated problem
efficiently, we develop an alternating Riemannian manifold optimization (ARMO)
algorithm to alternately update the receiver combiners and RIS phase-shift
matrices. An adaptive kernel training strategy is further introduced to
iteratively refine the channel covariance matrix without requiring additional
pilot resources. Simulation results demonstrate that the proposed ARMO-enhanced
estimator achieves substantial gains in estimation accuracy over
state-of-the-art methods.

</details>


### [40] [Feedback Lunch: Deep Feedback Codes for Wiretap Channels](https://arxiv.org/abs/2510.16620)
*Yingyao Zhou,Natasha Devroye,Onur Günlü*

Main category: cs.IT

TL;DR: 本文研究了反向退化窃听信道，提出了一种结合通用哈希函数和学习反馈码的模块化编码设计，在信道输出反馈下实现正保密速率。


<details>
  <summary>Details</summary>
Motivation: 针对反向退化窃听信道在没有信道反馈时保密容量为零的问题，探索如何利用信道反馈来克服窃听者的安全优势。

Method: 采用种子模块化编码设计，结合通用哈希函数保障安全性，学习反馈码保障可靠性，在信道输出反馈下实现保密通信。

Result: 研究表明反馈机制使得合法双方能够协商共享密钥，从而克服窃听者的安全优势，实现正保密速率。

Conclusion: 反馈机制为窃听信道提供了安全通信的可能性，并为下一代集成感知与通信方法中的感知辅助安全通信编码设计提供了启示。

Abstract: We consider reversely-degraded wiretap channels, for which the secrecy
capacity is zero if there is no channel feedback. This work focuses on a seeded
modular code design for the Gaussian wiretap channel with channel output
feedback, combining universal hash functions for security and learned
feedback-based codes for reliability to achieve positive secrecy rates. We
study the trade-off between communication reliability and information leakage,
illustrating that feedback enables agreeing on a secret key shared between
legitimate parties, overcoming the security advantage of the wiretapper. Our
findings also motivate code designs for sensing-assisted secure communication,
to be used in next-generation integrated sensing and communication methods.

</details>


### [41] [Non-Orthogonal Pilot Sequence Design for Multi-Cells Interference Networks](https://arxiv.org/abs/2510.16792)
*Zhi Gu,Wai Ho Mow*

Main category: cs.IT

TL;DR: 本文推导了多小区系统中扩展总平方相关(ETSC)的闭式下界，这是对Welch界和扩展Welch界的推广，并提出了基于MM优化框架的ETSC-MM算法来生成低ETSC序列集。


<details>
  <summary>Details</summary>
Motivation: 在无线通信中，当用户数超过序列长度时，非正交序列集的性能显著影响多用户干扰水平。在多小区系统中，考虑到来自本小区和邻小区信道的强度差异，需要设计新的序列设计准则。

Method: 提出了扩展总平方相关(ETSC)作为新的序列设计准则，推导了其闭式下界，并开发了基于Majorization-Minimization(MM)优化框架的ETSC-MM算法来生成低ETSC序列集。

Result: 获得了ETSC在多小区系统中的闭式下界表达式，当干扰功率因子矩阵正定时可以轻松获得最优序列集，并提出了有效的序列生成算法。

Conclusion: 该工作为多小区系统中的非正交序列设计提供了理论框架和实用算法，扩展了传统的Welch界理论，能够有效降低多用户干扰。

Abstract: In wireless communications, the performance of non-orthogonal sequence sets
significantly affects the level of multi-user interference when the number of
users surpasses the sequence length. The design of non-orthogonal sequences
plays a crucial role in both the non-orthogonality of the pilots in multi-cell
systems and the signature sequences in overloaded code-division multiple-access
(CDMA) systems. In multi-cell systems, considering the strength disparity
between channels originating from the home cell and the neighboring cells, the
extended total squared correlation (ETSC) is proposed as a new sequence design
criterion, which is defined as the sum of squares of the weighted correlations
among sequences. In this paper, we derive a closed-form expression for the
lower bound of ETSC for multi-cell systems with a given sequence length $\tau$,
where $\tau \leq K$ and $K$ is the number of users per cell. This can be
regarded as a generalization of the well-known Welch bound (Welch, 1974, IEEE
TIT) and the extended Welch bound (Wang et al., 2021, IEEE TWC). Additionally,
from the necessary conditions of the bound, the optimal sequence set can be
easily obtained when the interference power factor matrix is positive definite.
On the other hand, to address the lack of sequence generation methods under
certain parameter conditions, we propose the ETSC-MM algorithm, which generates
sequence sets with low ETSC based on a Majorization-Minimization (MM)
optimization framework.

</details>


### [42] [Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude](https://arxiv.org/abs/2510.16948)
*Ruiming Guo,Ayush Bhandari*

Main category: cs.IT

TL;DR: 论文提出了一种基于无限制感知框架(USF)的模数编码方法，用于解决传统数字采集在强-弱幅度差异脉冲恢复中的局限性，实现了超越传统限制的幅度和时间超分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统数字采集在处理强-弱幅度差异的脉冲时，会导致强分量削波或弱分量被量化噪声淹没，这种信息损失在固定比特预算下是不可避免的。

Method: 采用无限制感知框架(USF)中的模数编码来增强测量精度，开发了适用于非带限核的新理论结果和鲁棒的离网格稀疏恢复算法。

Result: 在飞行时间成像应用中，数值模拟和硬件实验验证了该方法在低位量化下的有效性，能够实现幅度和时间上的超分辨率。

Conclusion: 模数编码在USF框架内能够克服传统数字采集的基本限制，通过提高测量精度实现数字超分辨率，解锁超越传统限制的时间超分辨率。

Abstract: The recovery of Dirac impulses, or spikes, from filtered measurements is a
classical problem in signal processing. As the spikes lie in the continuous
domain while measurements are discrete, this task is known as super-resolution
or off-the-grid sparse recovery. Despite significant theoretical and
algorithmic advances over the past decade, these developments often overlook
critical challenges at the analog-digital interface. In particular, when spikes
exhibit strong-weak amplitude disparity, conventional digital acquisition may
result in clipping of strong components or loss of weak ones beneath the
quantization noise floor. This motivates a broader perspective:
super-resolution must simultaneously resolve both amplitude and temporal
structure. Under a fixed bit budget, such information loss is unavoidable. In
contrast, the emerging theory and practice of the Unlimited Sensing Framework
(USF) demonstrate that these fundamental limitations can be overcome. Building
on this foundation, we demonstrate that modulo encoding within USF enables
digital super-resolution by enhancing measurement precision, thereby unlocking
temporal super-resolution beyond conventional limits. We develop new
theoretical results that extend to non-bandlimited kernels commonly encountered
in practice and introduce a robust algorithm for off-the-grid sparse recovery.
To demonstrate practical impact, we instantiate our framework in the context of
time-of-flight imaging. Both numerical simulations and hardware experiments
validate the effectiveness of our approach under low-bit quantization, enabling
super-resolution in amplitude and time.

</details>


### [43] [Channel Capacity for FMCW-based Optical Wireless Integrated Sensing and Communication: Asymptotic Analysis and Envelope Design](https://arxiv.org/abs/2510.17093)
*Yunfeng Wen,Fang Yang,Jian Song,Zhu Han*

Main category: cs.IT

TL;DR: 本文分析了基于FMCW的相干光学无线集成感知与通信系统的信道容量，推导了感知约束下的容量上下界，并提供了脉冲幅度调制的包络设计指导。


<details>
  <summary>Details</summary>
Motivation: 光学无线集成感知与通信作为射频技术的补充和增强正在快速发展，需要分析信道容量来指导系统设计。

Method: 将FMCW-based OW-ISAC系统模型重新表述为信息论形式，施加谐波均值约束以确保感知性能，推导信道容量上下界，分析低高信噪比区域的渐近表达式。

Result: 数值结果展示了脉冲幅度调制的容量实现能力，仿真揭示了通信与感知功能之间的权衡关系。

Conclusion: 在感知约束下的信道容量分析为OW-ISAC设计的最优性和实用性提供了重要见解。

Abstract: Optical wireless integrated sensing and communication (OW-ISAC) is rapidly
burgeoning as a complement and augmentation to its radio-frequency counterpart.
In this paper, the channel capacity is analyzed to guide the design of a
coherent OW-ISAC system based on frequency-modulated continuous wave (FMCW).
Firstly, the system model of FMCW-based OW-ISAC is recast into an
information-theoretic formulation, where an additional harmonic-mean constraint
is imposed to ensure the sensing performance. Subsequently, both lower and
upper bounds for channel capacity are derived under the imposed sensing
constraint, based on which asymptotic expressions for channel capacity are
presented for both low and high signal-to-noise-ratio regions. Moreover, the
analysis of channel capacity provides guidance for the envelope design based on
pulse amplitude modulation, whose capacity-achieving capabilities are
demonstrated by numerical results. Furthermore, simulations reveal the
trade-off between communication and sensing functionalities. In summary, the
analysis of channel capacity under the sensing constraint provides insights
into both the optimality and the practicality of OW-ISAC design.

</details>


### [44] [Delay-Doppler Pulse Shaping in Zak-OTFS Using Hermite Basis Functions](https://arxiv.org/abs/2510.17466)
*Fathima Jesbin,Ananthanarayanan Chockalingam*

Main category: cs.IT

TL;DR: 本文提出了一个系统化的延迟-多普勒脉冲设计框架，通过Hermite基函数的线性组合来优化Zak-OTFS调制性能，解决了脉冲形状在时频定位和正交性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: Zak-OTFS调制性能严重依赖于延迟-多普勒域脉冲形状滤波器的选择。Balian-Low定理限制了L²(ℝ)脉冲设计，必须在时频定位和正交性之间进行权衡，既要满足输入输出关系估计的定位需求，又要保证无时间或带宽扩展时的可靠数据检测正交性。

Method: 提出将脉冲表示为Hermite基函数的线性组合，通过求解约束优化问题（使用奇异值分解）获得最小化符号间干扰能量的最优系数，并推导了Zak-OTFS中输入输出关系和噪声协方差的闭式表达式。

Result: 在Vehicular-A信道中进行的仿真表明，优化脉冲形状的误码率性能显著优于经典的sinc和高斯脉冲，与最先进的GS脉冲性能相当，同时提供了更好的ISI和旁瓣能量控制灵活性。

Conclusion: 所提出的Hermite脉冲框架为Zak-OTFS提供了系统化的脉冲设计方法，在保持高性能的同时提供了更大的设计灵活性，有效解决了时频定位与正交性之间的权衡问题。

Abstract: The performance of Zak-OTFS modulation is critically dependent on the choice
of the delay-Doppler (DD) domain pulse shaping filter. The design of pulses for
$L^2(\mathbb{R})$ is constrained by the Balian-Low Theorem, which imposes an
inescapable trade-off between time-frequency localization and orthogonality for
spectrally efficient systems. In Zak-OTFS, this trade-off requires balancing
the need for localization for input/output (I/O) relation estimation with the
need for orthogonality for reliable data detection when operating without time
or bandwidth expansion. The well-known sinc and Gaussian pulse shapes represent
the canonical extremes of this trade-off, while composite constructions such as
the Gaussian-sinc (GS) pulse shape offer a good compromise. In this work, we
propose a systematic DD pulse design framework for Zak-OTFS that expresses the
pulse as a linear combination of Hermite basis functions. We obtain the optimal
coefficients for the Hermite basis functions that minimize the inter-symbol
interference (ISI) energy at the DD sampling points by solving a constrained
optimization problem via singular value decomposition. For the proposed class
of Hermite pulses, we derive closed-form expressions for the I/O relation and
noise covariance in Zak-OTFS. Simulation results of Zak-OTFS with embedded
pilot and model-free I/O relation estimation in Vehicular-A channels with
fractional DDs demonstrate that the optimized pulse shape achieves a bit error
rate performance that is significantly superior compared to those of the
canonical sinc and Gaussian pulses and is on par with that of the
state-of-the-art GS pulse, validating the proposed framework which provides
greater design flexibility in terms of control of ISI and sidelobe energies.

</details>


### [45] [Multihead Finite-State Compression](https://arxiv.org/abs/2510.17544)
*Neil Lutz*

Main category: cs.IT

TL;DR: 本文开发了多头有限状态压缩模型，作为有限状态压缩的推广，与Huang等人(2025)的多头有限状态维度互补。该模型使用多个有限状态读头压缩无限符号序列，主要定理证明对于任意序列和正整数h，h头有限状态无损压缩器达到的压缩比率下确界等于该序列的h头有限状态预维度。


<details>
  <summary>Details</summary>
Motivation: 扩展有限状态压缩理论，引入多头读头机制，以更全面地描述序列的可压缩性，并与现有的多头有限状态维度理论形成互补。

Method: 采用多头有限状态压缩模型，使用固定数量的有限状态读头在序列中向前移动，基于读头读取的符号按照有限状态规则产生输出。

Result: 主要定理证明：对于任意序列和正整数h，h头有限状态无损压缩器达到的压缩比率下确界等于该序列的h头有限状态预维度。

Conclusion: 该工作建立了多头有限状态压缩与多头有限状态维度之间的等价关系，为序列压缩性分析提供了新的理论框架。

Abstract: This paper develops multihead finite-state compression, a generalization of
finite-state compression, complementary to the multihead finite-state
dimensions of Huang, Li, Lutz, and Lutz (2025). In this model, an infinite
sequence of symbols is compressed by a compressor that produces outputs
according to finite-state rules, based on the symbols read by a constant number
of finite-state read heads moving forward obliviously through the sequence. The
main theorem of this work establishes that for every sequence and every
positive integer $h$, the infimum of the compression ratios achieved by
$h$-head finite-state information-lossless compressors equals the $h$-head
finite-state predimension of the sequence. As an immediate corollary, the
infimum of these ratios over all $h$ is the multihead finite-state dimension of
the sequence.

</details>


### [46] [On the Capacity of Erasure-prone Quantum Storage with Erasure-prone Entanglement Assistance](https://arxiv.org/abs/2510.17781)
*Hua Sun,Syed A. Jafar*

Main category: cs.IT

TL;DR: 本文研究了量子纠删码的容量问题，其中量子消息被编码到N个存储节点和NB个纠缠辅助节点中，要求从任意K个存储节点和任意KB个纠缠辅助节点中恢复消息。作者在大多数情况下完全刻画了容量函数，并引入了相应的经典存储问题作为关键工具。


<details>
  <summary>Details</summary>
Motivation: 研究量子存储系统中存在纠缠辅助时的纠删码容量问题，探索在存储节点和纠缠辅助节点都可能发生擦除的情况下，量子消息的最大可传输容量。

Method: 引入了带有共享随机性辅助的经典存储问题作为关键工具，识别了一组约束条件，使得经典线性码构造可以转化为量子存储码，并对两种设置使用了相似的逆界证明方法。

Result: 在大多数情况下完全刻画了容量作为N,K,NB,KB,λB函数的精确表达式，除了在λB的中间范围内，当严格多数的N个存储节点和严格非零少数的NB个纠缠辅助节点被擦除时，容量问题仍然开放。

Conclusion: 量子存储码和经典存储码在容量已确定的情况下具有相同的容量特征，经典线性码构造可以转化为量子存储码，这为量子纠删码的研究提供了新的工具和视角。

Abstract: A quantum message is encoded into $N$ storage nodes (quantum systems
$Q_1\dots Q_N$) with assistance from $N_B$ maximally entangled bi-partite
quantum systems $A_1B_1, \dots, A_{N_B}B_{N_B}$, that are prepared in advance
such that $B_1\dots B_{N_B}$ are stored separately as entanglement assistance
(EA) nodes, while $A_1\dots A_{N_B}$ are made available to the encoder. Both
the storage nodes and EA nodes are erasure-prone. The quantum message must be
recoverable given any $K$ of the $N$ storage nodes along with any $K_B$ of the
$N_B$ EA nodes. The capacity for this setting is the maximum size of the
quantum message, given that the size of each EA node is $\lambda_B$. All node
sizes are relative to the size of a storage node, which is normalized to unity.
The exact capacity is characterized as a function of $N,K,N_B,K_B, \lambda_B$
in all cases, with one exception. The capacity remains open for an intermediate
range of $\lambda_B$ values when a strict majority of the $N$ storage nodes,
and a strict non-zero minority of the $N_B$ EA nodes, are erased. As a key
stepping stone, an analogous classical storage (with shared-randomness
assistance) problem is introduced. A set of constraints is identified for the
classical problem, such that classical linear code constructions translate to
quantum storage codes, and the converse bounds for the two settings utilize
similar insights. In particular, the capacity characterizations for the
classical and quantum settings are shown to be identical in all cases where the
capacity is settled.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [47] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: 本文提出了两种基于MPI的并行自助法算法策略，通过局部统计量聚合和同步伪随机数生成来解决大规模数据集自助法计算中的通信开销和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 自助法作为强大的统计重采样技术，在大数据集或大量重采样时计算成本过高，需要设计高效的并行算法来解决通信开销和内存约束问题。

Method: 提出了两种策略：1) 局部统计量聚合，通过传输充分统计量而非完整重采样数据集来大幅减少通信；2) 同步伪随机数生成，在无法将整个数据集存储在单个进程时实现分布式重采样。

Result: 开发了通信和计算复杂度的分析模型，与朴素基线方法相比，所提方法显著减少了通信量和内存使用。

Conclusion: 所提出的方法能够在大规模系统上实现可扩展的并行自助法计算，为处理大规模统计推断问题提供了有效解决方案。

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [48] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: FourierCompress是一种基于傅里叶变换的LLM激活压缩框架，通过在频域保留低频系数来大幅减少通信带宽需求，同时保持接近无损的推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上协作LLM推理中的通信瓶颈问题，传统激活压缩方法难以同时实现高压缩比、低重构误差和计算效率。

Method: 利用LLM激活在频域的稀疏性，通过FFT将激活转换到频域，仅保留紧凑的低频系数块，在服务器端利用共轭对称性重构信号。

Result: 在Llama 3和Qwen2.5模型上的实验显示，平均减少7.6倍激活大小，准确率损失小于0.3%，压缩时间比Top-k减少32倍以上。

Conclusion: FourierCompress在通信效率、近无损推理和快速压缩之间取得了良好平衡，适用于边缘设备LLM推理。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [49] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: Celeris是一种面向机器学习工作负载的RDMA传输协议，通过移除重传和顺序交付机制，利用ML对数据丢失的容忍性来降低尾延迟，提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式机器学习工作负载扩展到数千个GPU，集体通信中的尾延迟成为主要瓶颈。传统RDMA设计的严格可靠性和顺序交付机制在ML场景下引入了不必要的复杂性和延迟。

Method: Celeris在RDMA NIC中移除重传和顺序交付功能，采用尽力而为的传输方式，保留拥塞控制，通过软件级机制（如自适应超时和数据优先级）管理通信，将丢失恢复转移到ML流水线（如使用Hadamard变换）。

Result: Celeris将第99百分位延迟降低高达2.3倍，BRAM使用量减少67%，NIC对故障的恢复能力几乎翻倍。

Conclusion: Celeris为集群规模的机器学习工作负载提供了一个弹性、可扩展的传输解决方案，通过利用ML对数据丢失的容忍性来优化性能。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [50] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 提出了一种基于C++ Noarr库的MPI新抽象，实现了布局无关的MPI应用设计，并通过分布式GEMM内核案例展示了其可用性和性能。


<details>
  <summary>Details</summary>
Motivation: MPI作为分布式高性能计算技术已有数十年历史，但其纯C接口缺乏现代语言特性如类型检查和泛型代码设计，限制了开发效率。

Method: 将MPI抽象实现为C++ Noarr库的扩展，遵循Noarr范式（一等布局和遍历抽象），提供布局无关的MPI应用设计方法。

Result: 通过分布式GEMM内核案例验证，该抽象在保持与现有MPI C++绑定相当性能的同时，提供了更灵活的分布式应用设计能力。

Conclusion: 提出的MPI抽象在性能相当的前提下，显著提升了分布式应用设计的灵活性和开发效率。

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [51] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: 提出了一种集成容错架构，通过构建稳定性指标识别可靠机器并进行周期性诊断，实现永久故障隔离和自适应任务调度，无需额外硬件。相比基准TMR减少约30%任务负载，显著提高可靠性和能效。


<details>
  <summary>Details</summary>
Motivation: 解决传统两阶段三模冗余（TMR）在永久故障下失效的问题，以及反应式TMR（R-TMR）因依赖额外硬件而增加系统复杂性和降低容错能力的问题。

Method: 构建稳定性指标识别可靠机器，进行周期性诊断，实现永久故障隔离和自适应任务调度，无需额外硬件。

Result: 相比基准TMR减少约30%任务负载，实现优异的故障覆盖率和隔离精度。

Conclusion: 该方法显著提高了互联多核系统的可靠性和能效，为关键系统提供了有效的容错解决方案。

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [52] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [53] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: 提出了一种基于eBPF的GPU尾延迟监控系统，通过关联主机指标和GPU内部事件，在共享计算环境中实现高性能的根因分析。


<details>
  <summary>Details</summary>
Motivation: 现有监控工具在共享计算环境中缺乏足够的粒度来进行根因分析，无法有效诊断GPU尾延迟尖峰问题。

Method: 开发eBPF遥测系统，统一监控GPU工作负载，将eBPF获取的主机指标与GPU内部事件关联，实现全系统可观测性。

Result: 系统达到81-88%的诊断准确率，5秒内检测到延迟尖峰，6-8秒完成根因分析，在100Hz采样下仅产生1.21%的CPU开销。

Conclusion: 该系统能够识别NIC争用、PCIe压力和CPU干扰等根因，为多租户GPU基础设施提供操作调试能力，无需集群范围检测。

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [54] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: 本文提出了一种训练语言模型的方法，使其能够在推理过程中与性能工具交互，以解决代码性能优化任务中语言模型难以理解环境与代码性能交互的问题。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在软件工程中广泛应用，但在代码性能相关任务（如优化）中表现不佳，因为这些任务依赖于环境、硬件等复杂数据，而这些信息无法直接从源代码中获取。

Method: 提出一种训练方法，让语言模型在推理过程中能够与性能工具进行交互，从而更好地理解环境与代码性能的关系。

Result: 使用该方法训练出了一个最先进的GPU内核优化模型。

Conclusion: 通过让语言模型与性能工具交互，可以有效提升其在代码性能优化任务中的表现，解决了传统方法难以理解环境因素的问题。

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [55] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 该论文研究了分布式图算法中轮消除固定点技术的通用性问题，证明了轮消除技术对于带输入的问题不是通用的，但对于无输入的问题可能是通用的。


<details>
  <summary>Details</summary>
Motivation: 研究轮消除固定点技术是否能够作为证明分布式图算法下界的通用方法，解决先前存在的障碍并发现新的限制。

Method: 使用三势输入(tripotent inputs)构建轮消除下界，并开发新的系统化技术来构造轮消除下界证明。

Result: 证明了同态问题确实可以通过轮消除固定点证明下界，但同时也发现了轮消除技术对于带输入的问题不是通用的。

Conclusion: 轮消除技术不能作为带输入问题的通用下界证明方法，但对于无输入问题可能是通用的，并提出了首个适用于任何问题的通用下界定理。

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个针对Lean和mathlib的语义搜索引擎，通过理解数学家意图、分析公开讨论、微调文本嵌入和多样化反馈信号，显著提升了搜索效果，比现有引擎和GPT-4o有30%以上的相对改进。


<details>
  <summary>Details</summary>
Motivation: 现有Lean搜索引擎主要依赖形式化陈述的非正式翻译，忽视了与现实用户查询的匹配问题，导致定理证明进展缓慢且劳动密集。

Method: 分析并聚类公开Lean讨论的语义，在模拟用户意图的合成查询上微调文本嵌入，使用多样化反馈信号与数学家偏好对齐。

Result: 在真实世界查询、非正式化陈述和证明状态上的评估显示，Lean Finder相比之前搜索引擎和GPT-4o实现了超过30%的相对改进。

Conclusion: Lean Finder是一个用户中心的语义搜索工具，有效解决了数学家在使用Lean时的搜索难题，并与基于LLM的定理证明器兼容，连接了检索与形式推理。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [57] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: 本文提出LS-OGD框架，通过动态调整学习率和模态融合权重来应对概念漂移，确保多模态学习系统在非平稳环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中面临概念漂移的挑战，特别是模态特定的漂移和缺乏持续稳定适应机制的问题。

Method: 使用在线控制器动态调整模型学习率和不同数据模态之间的融合权重，以响应检测到的漂移和预测误差变化。

Result: 在有限漂移条件下，LS-OGD系统的预测误差被证明是最终一致有界的，如果漂移停止则收敛到零；自适应融合策略能有效隔离和减轻严重模态特定漂移的影响。

Conclusion: LS-OGD为开发可靠且持续适应的多模态学习系统提供了理论基础，确保系统韧性和容错能力。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [58] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，通过实时更新奖励分布的后验信念来决定何时停止生成新样本，在保持响应质量的同时最多减少80%的平均采样量。


<details>
  <summary>Details</summary>
Motivation: 多响应采样是提高LLM输出质量的常用方法，但会带来额外的计算成本。关键挑战是如何平衡准确率提升与效率，决定何时停止生成新样本。

Method: 基于序列搜索和贝叶斯学习，BEACON顺序生成策略LLM的响应，实时更新奖励分布的后验信念，通过权衡预期收益与计算成本来决定停止时机。

Result: BEACON在保持响应质量的同时，平均采样量最多减少80%，并展示了在成本效益偏好数据生成中的实用性。

Conclusion: BEACON提供了理论最优性保证和实际可操作性，为未来研究人员提供了可行的扩展思路和实用见解。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [59] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: PatMD是一种新颖的有害表情包检测方法，通过学习和主动缓解潜在误判风险来提升检测性能。该方法构建误判风险模式知识库，动态指导多模态大语言模型避免已知误判陷阱。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法（包括基于MLLM的技术）在处理表情包中的隐含表达（如讽刺和隐喻）时存在困难，导致频繁误判。需要超越表面内容匹配，识别潜在的误判风险模式。

Method: 首先构建知识库，将每个表情包解构为误判风险模式，解释其可能被误判的原因（漏判有害潜台词或过度解读良性内容）。对于目标表情包，检索相关模式并动态指导MLLM推理。

Result: 在5个有害检测任务的6,626个表情包基准测试中，PatMD优于最先进的基线方法，F1分数平均提升8.30%，准确率提升7.71%。

Conclusion: PatMD展示了强大的泛化能力和改进的有害表情包检测能力，通过主动识别和缓解误判风险模式，有效提升了检测性能。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [60] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 提出基于击键动力学的帕金森病筛查和远程监测新方法，使用深度学习方法在外部验证中取得超过90%的AUC-ROC和70%以上的F1-Score。


<details>
  <summary>Details</summary>
Motivation: 帕金森病全球患者超过1000万且预计2040年翻倍，早期诊断困难，传统临床评估存在局限性，需要非侵入性、可扩展的生物标志物。

Method: 三阶段方法：数据预处理（提取4个时间信号，比较3种类不平衡处理方法）；预训练8种深度学习架构；在中等数据集上微调并在独立队列进行外部验证。

Result: 混合卷积-循环和基于Transformer的模型在外部验证中表现优异，AUC-ROC超过90%，F1-Score超过70%，其中时间卷积模型外部验证AUC-ROC达91.14%。

Conclusion: 击键动力学作为帕金森病的可靠数字生物标志物具有巨大潜力，为早期检测和持续监测提供了有前景的途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [61] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的Ensemble Score Filter（EnSF）在野火蔓延实时预测数据同化中的应用，该方法通过整合观测数据和数值模型预测，显著提高了野火蔓延预测的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着野火破坏性增强和控制成本上升，需要准确、实时的火势蔓延预测来进行有效管理。数据同化通过整合观测数据（如遥感数据）和数值模型预测，对提高活跃火势预测准确性至关重要。

Method: 应用Ensemble Score Filter（EnSF）这一基于扩散模型的滤波算法，利用基于分数的生成扩散模型来处理野火蔓延模型的高维非线性滤波问题。

Result: 数值研究表明，EnSF在准确性、稳定性和计算效率方面表现出优越性能，成为野火数据同化的稳健实用方法。

Conclusion: EnSF为实时野火蔓延预测提供了一个准确、稳定且计算高效的数据同化解决方案，相关代码已公开可用。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [62] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: LAMI是一个新颖的联合图-语言建模框架，用于检测青少年和年轻成年人的非法药物使用，并通过挖掘变量间的潜在关系来解释行为风险因素。


<details>
  <summary>Details</summary>
Motivation: 现有建模方法将调查变量独立处理，忽略了变量间的潜在关联结构，这限制了模型对药物使用风险因素的深入理解。

Method: LAMI将个体回答表示为关系图，通过专门的图结构学习层学习潜在连接，并集成大语言模型生成基于图结构和调查语义的自然语言解释。

Result: 在YRBS和NSDUH数据集上的实验表明，LAMI在预测准确性上优于竞争基线。可解释性分析显示LAMI能揭示有意义的行为子结构和心理社会路径。

Conclusion: LAMI框架不仅能准确预测非法药物使用，还能提供与已知风险因素一致的可解释性见解，如家庭动态、同伴影响和学校相关困扰。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [63] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 提出Long Exposure系统，通过解决微调中特有的Shadowy Sparsity问题，加速参数高效微调(PEFT)过程，实现最高2.49倍的端到端微调加速。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型在下游任务中的参数高效微调存在效率低下问题，带来时间和成本挑战，而微调过程中特有的Shadowy Sparsity形式尚未得到充分解决。

Method: Long Exposure系统包含三个核心组件：Shadowy-sparsity Exposer使用长感知范围捕获更多稀疏细节；Sequence-oriented Predictor提供高效准确的预测处理大序列输入和动态参数；Dynamic-aware Operator优化计算模式和内存访问。

Result: 广泛评估表明，Long Exposure在端到端微调中实现了最高2.49倍的加速，优于现有最先进方法。

Conclusion: Long Exposure为加速LLMs的参数高效微调提供了有前景的进展，有效解决了微调过程中的效率瓶颈问题。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [64] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种名为Deadlock Attack的资源耗尽攻击方法，通过训练恶意对抗嵌入来劫持大型推理模型的生成控制流，诱导模型陷入永久推理循环而无法输出最终答案。


<details>
  <summary>Details</summary>
Motivation: 现代大型推理模型通过思维链推理展现出色的多步问题解决能力，但这种迭代思维机制引入了新的安全漏洞面。研究者旨在暴露LRMs在推理效率方面的关键安全漏洞。

Method: 采用优化的对抗嵌入来鼓励推理步骤后出现过渡性标记（如“Wait”、“But”），防止模型得出结论。通过后门植入策略克服连续到离散投影的挑战，确保通过特定触发标记可靠激活攻击。

Result: 在四个先进LRMs（Phi-RM、Nemotron-Nano、R1-Qwen、R1-Llama）和三个数学推理基准测试中实现了100%的攻击成功率，迫使模型生成达到最大标记限制。攻击具有隐蔽性，对良性用户输入造成的效用损失可忽略，并能抵抗现有缓解过度思考问题的策略。

Conclusion: 研究揭示了LRMs在推理效率方面存在关键且未被充分探索的安全漏洞，Deadlock Attack暴露了这种新型威胁，对模型安全性提出了重要警示。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [65] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出了一种细粒度联邦域自适应方法Gains，用于处理开放世界中新客户端不断加入的联邦学习场景，通过知识发现和知识适应机制来整合新知识，同时保持源域性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的联邦学习场景存在新客户端不断加入的问题，这些客户端带来新知识，需要检测和整合这些知识，同时避免牺牲源域性能和适应效率。

Method: 将模型分为编码器和分类器，基于特征对域偏移敏感、分类器参数对类别增量敏感的特性，开发细粒度知识发现和贡献驱动聚合技术，并设计抗遗忘机制。

Result: 在三个典型数据偏移场景的多域数据集上实验表明，Gains在源域和目标域客户端性能上均显著优于其他基线方法。

Conclusion: Gains方法有效解决了开放世界联邦学习中的新知识发现和适应问题，实现了源域和目标域性能的平衡提升。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [66] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: SAU-FNO结合自注意力机制、U-Net和FNO，有效捕捉长程依赖和局部高频特征，通过迁移学习减少对高保真数据的依赖，在3D IC热管理中实现842倍加速。


<details>
  <summary>Details</summary>
Motivation: 3D IC热管理面临功率密度增加的挑战，传统PDE方法速度慢，机器学习方法存在高频信息丢失和高保真数据依赖问题。

Method: 提出SAU-FNO框架，结合自注意力机制、U-Net和傅里叶神经算子，采用迁移学习微调低保真数据。

Result: SAU-FNO在热预测精度上达到最先进水平，相比传统FEM方法实现842倍加速。

Conclusion: SAU-FNO是先进3D IC热仿真的高效工具，显著提升计算效率。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [67] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 本文提出使用持久同调分析训练数据的几何结构，发现数据表示的丰富性和冗余消除对模型性能有重要影响，为AI系统训练数据优化提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据是机器学习和人工智能的基础，但数据的几何结构对模型性能的影响尚未充分探索。作者认为数据的表示丰富性和冗余消除对学习结果有重要影响。

Method: 使用持久同调方法从度量空间中的数据提取拓扑特征，提供了一种基于原则的多样性量化方法，超越了传统的基于熵的度量。

Result: 研究发现持久同调是分析和增强驱动AI系统的训练数据的强大工具。

Conclusion: 持久同调为训练数据的几何结构分析提供了有效框架，有助于优化AI系统的数据质量。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [68] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: PALE框架通过提示引导的数据增强和对比马氏距离评分，有效检测大语言模型的幻觉现象，无需人工标注即可实现优越的检测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在产生误导或虚构信息的幻觉问题，而幻觉检测面临标注数据稀缺的挑战，需要开发无需人工标注的实用检测方法。

Method: 提出PALE框架，利用提示引导从LLMs生成真实和幻觉数据作为数据增强；引入对比马氏距离评分(CM Score)，在激活空间中建模真实和幻觉数据的分布，采用矩阵分解方法捕捉分布结构。

Result: 大量实验表明PALE在幻觉检测方面表现优异，比竞争基线显著提升6.55%的性能。

Conclusion: PALE框架提供了一种无需人工标注、具有强泛化性和实用性的幻觉检测解决方案，为LLM生成内容的可靠性提供了有效保障。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [69] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 提出DAWP框架，通过人工智能数据同化模块在观测空间中初始化天气预测模型，解决了传统AIWP依赖再分析数据的局限性，实现了基于不规则高分辨率观测数据的全球天气预报。


<details>
  <summary>Details</summary>
Motivation: 传统人工智能天气预测方法依赖再分析数据，存在数据同化偏差和时间不一致性问题。观测预测作为一种变革性范式，旨在将AIWP从再分析数据中解放出来，但面临不同测量系统间时空动态学习的挑战。

Method: 提出DAWP框架，包含两个核心模块：1) AIDA模块使用掩码多模态自编码器同化不规则卫星观测数据；2) 时空解耦Transformer结合跨区域边界条件，在观测空间中学习动态，实现基于子图像的全球观测预测。

Result: 实验表明AIDA初始化显著提高了AIWP的展开和效率，DAWP在全局降水预测中展现出良好应用潜力。

Conclusion: DAWP框架成功实现了在完整观测空间中运行AIWP，解决了传统方法对再分析数据的依赖问题，为基于直接观测的天气预报提供了创新解决方案。

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [70] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出了Cog-Rethinker，一种分层元认知强化学习框架，通过分解零准确率问题和参考错误答案来改进LLM推理任务的样本利用率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定提示模板激活LLM推理能力，但这对弱LLM造成严重采样效率问题，大多数问题在推理任务中产生无效输出，导致样本浪费。

Method: 分层元认知两阶段框架：1) 将零准确率问题分解为子问题生成最终推理结果；2) 参考先前错误答案精炼答案。使用监督微调确保训练测试一致性。

Result: 在多个数学推理基准测试中表现优异，相比基线方法提高了样本效率并加速收敛。

Conclusion: Cog-Rethinker通过分层元认知方法有效解决了LLM推理训练中的样本效率问题，显著提升了性能。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [71] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 该论文提出了AMiD框架，通过α-混合辅助分布来解决大型语言模型知识蒸馏中的容量差距和训练不稳定性问题，提供了一个统一的、理论基础的辅助分布空间。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型虽然性能优异但计算和内存成本高，知识蒸馏可以缓解这一问题，但现有方法面临容量差距和训练不稳定性问题，特别是由高维输出导致的近零概率问题。

Method: 提出了α-混合辅助分布，引入新的分布设计变量α来连续扩展辅助分布，并基于最优性推广了与辅助分布一起使用的散度族，形成了统一的AMiD知识蒸馏框架。

Result: 通过大量实验证明，AMiD通过利用更广泛且理论基础的辅助分布空间，提供了优越的性能和训练稳定性。

Conclusion: AMiD框架通过系统化的辅助分布设计和散度推广，有效解决了知识蒸馏中的关键挑战，为大型语言模型的高效压缩提供了新思路。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [72] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 该论文提出了MEET-Sepsis框架，通过多内源视图表示增强机制和级联双卷积时间序列注意力模块，显著提升了脓毒症早期预测性能，仅需20%的ICU监测时间即可达到竞争性预测准确率。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是ICU中死亡率很高的威胁生命的感染综合征，早期准确预测对及时干预至关重要。现有AI方法难以捕捉微弱的早期时间信号，需要改进早期预测能力。

Method: 提出MEET-Sepsis框架，包含多内源视图表示增强(MERE)机制构建丰富特征视图，以及级联双卷积时间序列注意力(CDTA)模块进行多尺度时间表示学习。

Result: 该框架仅需SOTA方法20%的ICU监测时间即可达到竞争性预测准确率，显著推进了脓毒症的早期预测能力。

Conclusion: MEET-Sepsis框架通过创新的表示增强和时间序列学习方法，有效解决了脓毒症早期预测的挑战，为临床实践提供了更高效的预测工具。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [73] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出一种基于聚类的可解释人工智能方法，用于根据睡眠障碍特征对患者进行分组，并通过可解释方法识别影响这些疾病的关键因素。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍严重影响患者健康和生活质量，但由于症状多样性导致诊断复杂。技术进步和医疗数据分析为更好理解这些疾病提供了新视角，特别是可解释人工智能（XAI）旨在使AI模型决策对用户可理解和可解释。

Method: 采用基于聚类的可解释人工智能方法，将患者按不同睡眠障碍特征进行分组，并整合可解释方法识别关键影响因素。

Result: 在匿名真实数据上的实验证明了该方法的有效性和相关性。

Conclusion: 该方法能够有效识别睡眠障碍患者的不同特征分组，并通过可解释分析揭示影响这些疾病的关键因素，为睡眠障碍的诊断和理解提供了新的技术途径。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [74] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 该论文提出了一个框架来追踪和引导大语言模型中的算法原语，通过将推理轨迹与内部激活模式关联，并评估算法原语对推理步骤和任务性能的影响。研究发现LLM推理可能由算法原语的组合几何支持，这些原语可以跨任务和跨模型转移，推理微调增强了跨领域的算法泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何通过潜在计算和推理时间计算来解决多步推理问题，探索模型推理背后的算法原语及其组合性质。

Method: 通过聚类神经激活并标记匹配的推理轨迹来操作化原语，应用函数向量方法推导可重用的推理构建块原语向量，通过加法、减法、标量操作组合原语向量，在激活空间中揭示几何逻辑。

Result: 跨任务和跨模型评估显示存在共享和任务特定的原语，推理微调后的模型表现出更系统的验证和路径生成原语使用，在基础模型中注入相关原语向量可诱导出与推理微调模型相关的行为特征。

Conclusion: LLM推理可能由算法原语的组合几何支持，原语可以跨任务和跨模型转移，推理微调增强了跨领域的算法泛化能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [75] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO算法作为强化学习可验证奖励的主要方法，在提升大语言模型推理能力时存在不一致性。研究发现GRPO本质上是保守的重新加权方案，受限于基础模型分布，无法发现全新解决方案，只能强化预训练偏见。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO算法在不同推理领域表现不一致的问题，探索其在什么条件下能提升推理能力和实现分布外泛化。

Method: 从数据分布角度进行理论分析，证明GRPO是保守的重新加权方案，并通过从头训练transformer进行控制实验，评估在推理深度、输入长度、token表示和组合性等方面的泛化能力。

Result: GRPO的分布外改进仅在目标任务与模型预训练偏见一致时出现，而在分布内任务上的收益会随着性能饱和而减少。

Conclusion: GRPO不是通用的推理增强器，而是强化预训练偏见的工具，未来需要开发能扩展模型能力超越预训练起源的算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [76] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos是一个端到端的LLM蒸馏管道，能自动选择服务器和模型、进行知识蒸馏以及在分布式云环境中部署，旨在满足用户定义的模型性能和系统预算约束。


<details>
  <summary>Details</summary>
Motivation: 工业对定制化和成本效益高的大型语言模型的需求日益增长，现有蒸馏框架需要人工干预且难以满足复杂的用户定义蒸馏需求。

Method: Stratos自动选择Pareto最优服务器，动态匹配师生对，并根据任务复杂度调整蒸馏策略以优化云托管。

Result: 实验显示，Stratos产生的学生模型在罕见的领域特定麻将推理任务上，通过反向合成数据和知识注入，达到了其GPT-4o教师基线四倍的准确率，同时在不牺牲准确性的情况下降低了延迟和成本。

Conclusion: 这些结果突显了Stratos在垂直领域LLM部署中的潜力。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [77] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 该论文提出使用Kolmogorov-Smirnov检验来监测和量化机器学习系统中的分布偏移问题，特别是在智能交通领域的应用。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统中训练数据与测试数据概率分布不一致的问题，这种分布偏移会导致预测误差，在安全关键应用中尤为严重。

Method: 采用Kolmogorov-Smirnov检验来测量分布偏移，使用KS距离来量化分布偏移及其对AI智能体性能的影响。

Result: 研究表明，即使KS距离仅为0.02，也会导致强化学习智能体在单个交叉路口的通行时间增加约50%，影响显著。

Conclusion: KS检验和KS距离可作为实时监测AI智能体性能退化的有价值统计工具，帮助AI系统更有效地应对分布偏移问题。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [78] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 本文提出STAR模块，用于增强时间序列基础模型在处理包含离散状态变量的多元时间序列异常检测时的能力，通过状态感知编码和条件适配器来有效利用状态信息。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在处理同时包含数值变量和离散状态变量的工业时间序列时，往往忽视状态变量的分类特性及其作为条件的关键作用，导致检测性能下降。

Method: 提出STAR模块，包含三个核心组件：身份引导状态编码器、条件瓶颈适配器和数值-状态匹配模块，通过可学习状态记忆和条件低秩适配参数来增强模型对状态变量的建模能力。

Result: 在真实世界数据集上的广泛实验表明，STAR能够显著提升现有时间序列基础模型在多元时间序列异常检测中的性能。

Conclusion: STAR作为一个即插即用模块，有效解决了时间序列基础模型在处理状态变量时的局限性，为工业场景中的时间序列异常检测提供了更强大的解决方案。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [79] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: AirDbM是一种专门用于翼型优化的设计变形方法，通过从UIUC数据库中精选12个基准翼型，显著降低设计空间维度，在保持高重构精度的同时实现更高效的多目标优化和强化学习应用。


<details>
  <summary>Details</summary>
Motivation: 翼型几何优化需要探索多样设计空间，同时尽可能减少设计变量数量。现有方法存在设计空间维度高、优化效率低的问题。

Method: 从UIUC数据库1600多个翼型中系统选择12个最优基准翼型，通过设计变形方法重构翼型几何形状，评估设计容量提升效果。

Result: 用12个基准翼型重构了99%的数据库，平均绝对误差低于0.005；在多目标气动优化中实现快速收敛，获得更大超体积的帕累托前沿，发现具有改进升阻比的新最优解；在强化学习中表现出优于传统参数化方法的适应性。

Conclusion: AirDbM通过系统减少设计空间维度，在保持重构精度的同时显著提升优化效率和机器学习驱动设计的潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [80] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于特征驱动的强化学习方法，用于光伏发电商在日内连续市场中的交易决策，通过整合数据驱动特征，在马尔可夫决策过程框架下学习投标策略。


<details>
  <summary>Details</summary>
Motivation: 光伏运营商面临发电量和短期电价的高度不确定性，日内连续市场允许生产商实时调整头寸，可能提高收入并减少不平衡成本。

Method: 将问题建模为马尔可夫决策过程，使用近端策略优化（PPO）算法，采用主要线性的可解释策略，在历史市场数据上训练。

Result: 在样本外评估中，该策略在各种场景下持续优于基准方法，验证显示快速收敛、实时推理和透明决策规则。

Conclusion: 特征驱动的强化学习为光伏生产商积极参与日内交易提供了实用、数据高效且可操作部署的途径。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [81] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 本文提出了一种信息瓶颈引导的微调方法（IB-FT），用于解决大语言模型在代码生成任务中存在的记忆障碍问题，该方法通过压缩记忆特征来提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调在代码生成任务中存在记忆障碍问题，即模型过度记忆下游代码数据而无法有效学习新的可泛化知识。

Method: 提出IB-FT方法，在代码数据的隐藏表示上应用信息瓶颈惩罚，压缩冗余的记忆特征同时保留任务相关信息。

Result: 在两个代码基准测试（OriGen和Evol-CodeAlpaca-V1）上的实验表明，IB-FT显著缓解了记忆障碍，提高了top-1性能，并在更严格的多样本指标下获得更稳定的提升。

Conclusion: IB-FT方法能有效克服代码生成中的记忆障碍问题，相比传统微调方法具有更好的性能和稳定性。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [82] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出了PolyConFM，首个聚合物基础模型，通过构象中心的生成预训练统一聚合物建模与设计，解决了现有方法忽视全局构象信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅使用单体级描述符表示整个聚合物，忽视了聚合物构象中的全局结构信息，限制了实际性能。同时该领域缺乏能有效支持多样化下游任务的通用基础模型。

Method: 将聚合物构象分解为局部构象序列，采用条件生成范式预训练PolyConFM，通过掩码自回归建模重建局部构象，并生成其方向变换以恢复相应聚合物构象。构建首个高质量聚合物构象数据集。

Result: 实验表明PolyConFM在多样化下游任务上持续优于代表性任务特定方法。

Conclusion: PolyConFM为聚合物科学提供了一个通用且强大的工具。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [83] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 提出一个可推广的因果机器学习流程，用于从大规模电子健康记录中发现潜在因果源并量化其对临床结果的影响。


<details>
  <summary>Details</summary>
Motivation: 处理不完善的多模态临床数据，发现其中的潜在因果源，并量化这些源对临床结果的因果效应，以支持大规模医学发现。

Method: 处理多模态临床数据，将其分解为概率独立的潜在源，然后训练任务特定的因果模型来估计个体因果效应。

Result: 该方法已在两个真实世界应用中验证，展示了其在医学发现中的多功能性和实用性。

Conclusion: 所提出的因果机器学习流程能够有效处理不完善的临床数据，发现潜在因果源并量化其影响，为大规模医学发现提供了有力工具。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [84] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant是一种新型浮点量化方法，首次探索非整数位宽量化，通过尾数位共享和自适应搜索技术，将模型量化为FP-5.33-e2m3和FP4.25-e2m2格式，在保持精度损失可忽略的同时，显著加速LLM推理速度（2.8倍和3.2倍）。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数量巨大（十亿甚至万亿级别）带来存储和推理效率瓶颈，浮点量化能够通过减少内存占用和数据移动来加速LLM推理。

Method: 提出两种新技术：1）尾数位共享：将k个量化权重分组共享最低有效尾数位，进一步逼近最小量化位宽；2）自适应搜索：采用离线优化策略最小化共享带来的精度损失。同时开发了高效的CUDA线性核，将内存节省转化为实际延迟降低。

Result: 在大规模数据集和模型上的实验表明，AMS-Quant能够将模型量化为FP-5.33-e2m3和FP4.25-e2m2格式，相比FP16推理显著加速解码速度（2.8倍和3.2倍），且精度损失可忽略不计。

Conclusion: AMS-Quant首次将浮点量化从整数位宽扩展到非整数位宽，通过创新的尾数位共享和自适应搜索技术，在保持精度的同时实现了显著的推理加速，为LLM高效部署提供了有效解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [85] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 本文系统评估了5个时间序列基础模型和2个基线模型的校准特性，发现时间序列基础模型比基线模型校准更好，且不会系统性地过度自信或不足自信。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测性能上达到最先进水平，但其校准特性相对未被充分探索，而校准则对许多实际应用至关重要。

Method: 进行了一系列系统评估，包括模型校准（过度或不足自信）、不同预测头的影响以及长期自回归预测下的校准情况。

Result: 时间序列基础模型比基线模型校准得更好，且不会系统性地过度自信或不足自信，这与深度学习中常见的过度自信现象形成对比。

Conclusion: 时间序列基础模型具有良好的校准特性，这为实际应用提供了重要优势。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [86] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出了FedPURIN框架，通过整数规划识别关键参数进行传输，结合稀疏聚合方案，在保持模型性能的同时显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中现有方法通信效率不足的问题，特别是在数据异构环境下，通信负担阻碍了实际部署。

Method: 基于参数解耦范式，通过整数规划策略识别关键传输参数，并集成到稀疏聚合方案中。

Result: 在标准图像分类基准测试中，在多种非IID条件下实现了与最先进方法竞争的性能，并通过稀疏聚合实现了可量化的通信减少。

Conclusion: FedPURIN为通信高效的个性化联邦学习建立了新范式，特别适用于具有异构数据源的边缘智能系统。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [87] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出了多尺度神经算子（MNO），一种用于三维非结构化点云上计算流体力学的新型架构，通过显式三尺度分解显著提升了神经算子在复杂流体问题上的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在求解偏微分方程时仍存在精度有限和可扩展性不足的问题，特别是在不规则域上处理多尺度流体结构时表现不佳。

Method: MNO架构包含三个尺度模块：全局维度收缩注意力模块处理长程依赖，局部图注意力模块处理邻域交互，微观点级注意力模块处理细粒度细节。

Result: 在四个不同基准测试中，MNO在稳态和非稳态流动场景下（最多30万点）均优于现有最佳基线，预测误差降低5%-40%，在挑战性3D CFD问题中表现出更强的鲁棒性。

Conclusion: 显式多尺度设计对神经算子至关重要，MNO为在不规则域上学习复杂流体动力学提供了一个可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [88] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 提出基于随机矩阵理论的Transformer训练动态分析框架，通过自注意力矩阵的谱密度演化识别训练三阶段，并开发无需验证集的早停准则。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer训练动态的底层机制，为性能改进提供理论依据，并建立原则性的早停标准。

Method: 利用随机矩阵理论分析浅层自注意力矩阵V的谱密度演化，使用幂律拟合作为探针划分训练阶段，提出定量指标和谱特征作为早停准则。

Result: 观察到自注意力矩阵谱密度始终演化为重尾分布，识别出三个训练阶段：结构探索、重尾结构稳定化和收敛饱和，提出的早停准则与训练进展高度一致。

Conclusion: 随机矩阵理论为监控和诊断Transformer模型训练进展提供了有效工具，提出的早停准则具有一致性和无需验证集的优势。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [89] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出了BPL（Bias-adaptive Preference distillation Learning）框架，通过双蒸馏策略在事实和反事实测试环境中实现高性能推荐系统。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差问题，导致收集的反馈无法完全揭示用户偏好。现有去偏学习主要专注于反事实测试环境，但在基于实际用户-物品交互的事实测试环境中准确率显著下降。需要一种能在两种测试环境中都表现良好的模型。

Method: 采用双蒸馏策略：1）从有偏模型进行师生蒸馏，保留与收集反馈一致的准确偏好知识；2）通过可靠性过滤的自蒸馏，在训练过程中迭代精炼知识。

Result: 综合实验验证了BPL在事实和反事实测试中的有效性。

Conclusion: BPL框架能够逐步揭示用户偏好，在两种测试环境中都实现高性能，解决了推荐系统偏差问题。

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [90] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种因子化引导的语义恢复框架（FSRF），用于解决多模态情感分析中模态缺失的问题，通过去冗余同质-异质因子化模块和分布对齐自蒸馏模块来恢复缺失语义。


<details>
  <summary>Details</summary>
Motivation: 现实应用中由于遮挡、隐私约束和设备故障等原因导致模态缺失，而现有研究主要关注完整多模态数据的交互和融合，忽略了模态缺失问题，导致泛化能力差。

Method: 提出去冗余同质-异质因子化模块将模态分解为同质、异质和噪声表示，并设计约束范式；设计分布对齐自蒸馏模块通过双向知识转移充分恢复缺失语义。

Result: 在两个数据集上的综合实验表明，FSRF在不确定模态缺失情况下相比先前方法具有显著性能优势。

Conclusion: FSRF框架有效缓解了多模态情感分析中的模态缺失问题，提高了模型的泛化能力。

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [91] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE是一个门控持续自编辑框架，使用LoRA参数高效微调来约束顺序更新中的遗忘问题，通过三种指标评估编辑稳定性，在Qwen-2.5-7B模型上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要持续适应机制，但顺序更新会导致灾难性遗忘，新编辑会降低先前获得的知识。

Method: 使用LoRA进行参数高效微调，通过三种指标（精确匹配下降、比特增加、KL散度）评估候选编辑的稳定性，超过阈值时重新缩放或拒绝LoRA更新。

Result: 实验表明门控机制有效减轻遗忘同时保持适应性，基于EM的门控在短持续学习序列中实现了最高累积性能。

Conclusion: 不同门控策略可以产生可比较的分布偏移但产生不同的准确度结果，突显了持续适应中门控设计的重要性，为持续模型编辑提供了原则性方法。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [92] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 提出MemCom方法，通过层间压缩来改进大语言模型上下文学习中的多示例提示压缩，在保持高准确率的同时显著减少内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 上下文学习中增加示例数量能提升性能但带来高昂的内存和计算成本，现有提示压缩方法对多示例压缩效果不佳，需要更有效的压缩方案。

Method: 提出MemCom层间压缩方法，使用更强参数的压缩器模型，在transformer的每一层进行细粒度压缩，为每层提供独立的压缩表示。

Result: 在多种模型大小、架构和压缩比下，MemCom在所有压缩比上均优于基线方法，在高压缩比下性能下降小于10%，而基线方法下降超过20-30%。

Conclusion: MemCom通过层间压缩实现了高效的多示例提示压缩，在保持高准确率的同时显著提升了内存和计算效率。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [93] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 本文提出了一种基于相似性搜索和随机表示的无训练世界模型，与Dreamer家族的PlaNet模型进行比较，在潜在重建质量和长时程预测方面表现相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 世界模型在强化学习中广泛使用，能够提高样本效率。本文旨在探索无需训练过程的世界模型构建方法。

Method: 利用相似性搜索和随机表示来近似世界模型，避免传统训练过程，并与PlaNet模型进行对比评估。

Result: 搜索式世界模型在潜在重建质量和感知相似性方面与基于训练的世界模型表现相当，在长时程预测方面表现更优。

Conclusion: 基于搜索的世界模型能够在不进行训练的情况下达到与训练模型相当的性能，特别是在长时程预测任务中表现更佳。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [94] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 本文首次对时变学习策略下的Q-learning算法进行了有限时间分析，仅假设存在一个能在状态空间上诱导不可约马尔可夫链的策略。证明了Q函数的收敛速率和样本复杂度，揭示了在线Q-learning相比离线版本具有较弱的探索性但更强的利用性优势。


<details>
  <summary>Details</summary>
Motivation: 现有的Q-learning分析大多基于离线采样或固定学习策略，而实际应用中学习策略通常是时变的。本文旨在填补这一空白，为具有时变学习策略的Q-learning提供理论保证。

Method: 采用改进的分析方法，利用泊松方程将马尔可夫噪声分解为鞅差项和残差项。通过泊松方程解对Q函数估计和学习策略的敏感性分析来控制时变非齐次性下的残差项。

Result: 建立了Q函数的最终迭代收敛速率，样本复杂度为O(1/ε²)，与离线Q-learning相同但在探索相关参数上依赖更差。同时推导了学习策略下Q函数与最优Q函数差异的显式速率。

Conclusion: 在线Q-learning在探索性上弱于离线版本，但在利用性上具有优势，因为其策略会收敛到最优策略而非保持固定。所开发的分析工具可进一步促进具有快速时变学习策略的一般强化学习算法的分析。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [95] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种连接零阶优化和锐度感知最小化(SAM)的新方法，通过指数倾斜目标在平均损失和最大损失公式之间平滑过渡，开发了解决软SAM目标的零阶算法。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法优化平滑后的函数(随机扰动参数下的期望目标)，而SAM方法关注邻域内的最大损失以获得平坦最小值。本文旨在明确连接这两种方法。

Method: 提出指数倾斜目标作为平均损失和最大损失公式之间的平滑过渡，开发新的零阶算法来求解由倾斜参数t参数化的软SAM目标。

Result: 该方法在分类、多项选择QA和语言生成等下游任务上，相比普通零阶基线实现了更好的泛化性能。

Conclusion: 该方法可作为SAM变体的无梯度和内存高效替代方案，在多种任务中展现出优越的泛化能力。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [96] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: 提出了GRUwE模型，基于RNN架构处理不规则采样多变量时间序列，通过指数基函数和两种重置机制支持连续时间预测，在多个真实世界基准测试中达到或超越SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样多变量时间序列建模的挑战，探索简单高效的RNN架构是否仍能与复杂架构竞争，提供更易实现和部署的解决方案。

Method: 基于GRU架构，引入指数基函数，采用两种重置机制：观测触发重置和时间触发重置，通过可学习的指数衰减更新GRU状态，支持连续时间预测。

Result: 在多个真实世界基准测试中，GRUwE在下一观测和下一事件预测任务上达到竞争性甚至优于最新SOTA方法的性能。

Conclusion: GRUwE证明了简单RNN架构通过适当修改仍能保持竞争力，具有实现简单、超参数调优少、计算开销低等优势，适合在线部署。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [97] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 本文对三种代表性生成模型（AtomGPT、CDVAE、FlowMM）在超导材料数据集上进行了系统性基准测试，评估它们在晶体结构重建方面的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在材料发现中日益重要，但缺乏对其性能的严格比较评估。本研究旨在填补这一空白，系统比较不同架构生成模型在材料数据集上的表现。

Method: 使用三种代表性生成模型（基于Transformer的AtomGPT、晶体扩散变分自编码器CDVAE、黎曼流匹配模型FlowMM），在两个公开超导数据集（JARVIS Supercon 3D和Alexandria的DS A/B）上进行训练和评估。

Result: 性能评估使用KL散度和晶格常数MAE指标，结果显示CDVAE表现最佳，其次是AtomGPT，然后是FlowMM。

Conclusion: CDVAE在晶体结构重建任务中表现最优，为材料生成模型的比较提供了基准，相关代码和配置将公开提供。

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [98] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 本文通过层间因果修补分析语言模型对齐机制，发现对齐过程在空间上是局部化的，主要由中间层激活决定奖励一致性行为，而早期和晚期层基本不受影响。


<details>
  <summary>Details</summary>
Motivation: 尽管基于人类反馈的强化学习（RLHF）已成为语言模型偏好微调的流行方法，但其内部对齐机制仍然不透明，需要系统分析对齐是如何实现的。

Method: 在Llama-3.2-1B模型上，通过层间因果修补方法，比较基础模型与其调优版本在人类偏好对上的差异，并使用LASSO回归分析激活距离与奖励增益的关系。

Result: 发现对齐是空间局部化的：中间层激活编码了决定奖励一致性行为的独特子空间，而早期和晚期层基本不受影响；只有少数层具有非零系数连接激活距离与奖励增益。

Conclusion: 基于人类偏好的语言模型对齐是一个定向、低秩的过程，而非扩散和参数化的过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [99] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 该论文批评当前强化学习研究过度关注性能表现而忽视学习动态理解，主张应更注重科学理解和基准测试的数学形式化映射。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过度强调展示智能体性能，导致基准测试过拟合，难以将技术迁移到新问题，同时削弱了对学习动态理解的研究价值。

Method: 以Arcade Learning Environment (ALE)为例，说明尽管该基准被认为"饱和"，但仍可有效用于发展对强化学习的理解，并促进RL技术在现实问题中的部署。

Result: 论文提出了两个核心观点：RL研究应停止仅关注性能展示，而应更注重科学理解；需要更精确地定义基准测试与底层数学形式化之间的映射关系。

Conclusion: 强化学习研究需要平衡性能展示与科学理解，通过更精确的基准测试映射来促进RL技术的实际应用和发展。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [100] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于运行时监控语言（RML）的新型语言奖励机，能够处理非正则、非马尔可夫任务，解决了传统奖励机表达能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励函数通常被当作黑盒映射，缺乏解释性，且传统奖励机只能处理正则语言，无法表达计数或参数化条件等复杂行为。

Method: 基于运行时监控语言（RML）开发新型语言奖励机，利用RML的内置内存来指定非正则、非马尔可夫任务的奖励函数。

Result: 实验证明了该方法在表达能力上的优势，在灵活事件处理和任务规范方面优于现有基于奖励机的方法。

Conclusion: 基于RML的语言奖励机为强化学习提供了更强大的奖励函数表达能力，能够处理更复杂的非正则任务。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [101] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文介绍了ECML-PKDD 2025高能物理发现鲁棒学习挑战赛Task 1的获胜解决方案，采用多轮梯度攻击策略，在最小化扰动的同时最大化分类错误率。


<details>
  <summary>Details</summary>
Motivation: 挑战赛要求设计对抗性攻击，针对提供的分类模型，在最小化扰动的同时最大化误分类率。

Method: 使用多轮梯度攻击策略，利用模型的可微分结构，结合随机初始化和样本混合技术来提高攻击效果。

Result: 该攻击方法在扰动大小和欺骗成功率方面取得了最佳结果，在竞赛中获得第一名。

Conclusion: 提出的多轮梯度攻击策略结合随机初始化和样本混合技术，在对抗性攻击任务中表现优异，证明了该方法的有效性。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [102] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习与对象中心表示的新框架，能够处理结构化和非结构化数据，并通过主动向人类专家查询指导来增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理结构化问题时忽略了问题的内在结构，而关系强化学习虽然能处理结构化问题但对问题结构有强假设限制。需要一种能同时处理结构化和非结构化数据的方法。

Method: 结合关系强化学习与对象中心表示，通过显式建模策略不确定性，允许系统主动向人类专家查询指导。

Result: 实证评估表明所提出的方法具有有效性和高效性。

Conclusion: 该框架成功解决了传统强化学习和关系强化学习的局限性，通过结合对象中心表示和主动学习机制，实现了更好的泛化能力和学习效率。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [103] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 本文研究了一个非平稳多臂赌博机问题，其中奖励取决于动作和潜在状态，状态动态由未知线性动力学控制且受动作影响。作者提出了一个探索-承诺算法，通过随机探索估计线性动态参数，然后设计优化动作序列，实现了O(T^{2/3})的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 研究动作和潜在状态共同影响奖励的非平稳赌博机问题，其中状态动态也受动作影响，导致短期和长期奖励之间的权衡问题。

Method: 提出探索-承诺算法：探索阶段使用随机Rademacher动作估计线性动态的马尔可夫参数；承诺阶段使用估计参数设计优化动作序列以最大化长期奖励。

Result: 算法实现了O(T^{2/3})的遗憾上界，解决了时间相关奖励学习和设计最优长期奖励动作序列两个关键挑战。

Conclusion: 通过系统识别和不定二次优化问题的等价性证明，提供了实用的半定松弛方法，为处理动作影响状态动态的复杂赌博机问题提供了有效解决方案。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [104] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: ControlValve防御系统通过生成允许的控制流图并强制执行符合这些图的执行，防止多智能体系统中的控制流劫持攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御（如LlamaFirewall）依赖对齐检查，但无法有效防止控制流劫持攻击，因为多智能体系统的安全性和功能性目标存在根本冲突。

Method: 提出ControlValve防御，基于控制流完整性和最小权限原则，生成允许的控制流图并强制执行符合这些图的执行，同时为每个智能体调用生成零样本上下文规则。

Result: ControlValve能够有效防御规避现有对齐检查的控制流劫持攻击。

Conclusion: ControlValve提供了一种更可靠的多智能体系统安全防御机制，解决了现有对齐检查方法的局限性。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [105] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 本文对标签噪声检测方法进行了全面基准测试，通过将方法分解为标签一致性函数、聚合方法和信息收集方法三个基本组件，系统比较了不同方法在视觉和表格数据集上的表现。研究发现使用平均概率聚合和logit边界作为标签一致性函数的样本内信息收集方法在大多数场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中的标签噪声问题普遍存在，影响模型训练和验证性能。虽然已有多种检测噪声标签的技术，但缺乏对最优方法的明确共识，需要系统性的比较研究。

Method: 将噪声检测方法分解为三个基本组件：标签一致性函数、聚合方法和信息收集方法（样本内vs样本外）。提出统一的基准任务：检测与数据集噪声率相等的训练样本比例，并引入新的评估指标：固定操作点下的假阴性率。

Result: 在视觉和表格数据集上的评估表明，使用平均概率聚合和logit边界作为标签一致性函数的样本内信息收集方法在大多数合成和真实噪声条件下表现最佳。

Conclusion: 研究结果为设计新的检测方法和为特定应用选择技术提供了实用指导，识别了在不同场景下表现最优的噪声检测方法组合。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [106] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 本研究应用机器学习方法分析欧洲绿色协议中气候政策的进展过程，通过比较不同文本表示方法和结合元数据特征，预测政策的推进状态。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效的立法行动来缓解其影响，本研究旨在探索机器学习如何帮助理解气候政策从宣布到采纳的进展过程。

Method: 构建包含165项政策的文本和元数据数据集，比较TF-IDF、BERT和ClimateBERT等文本表示方法，并评估元数据特征对预测性能的影响。

Result: 仅使用文本特征时，ClimateBERT表现最佳（RMSE = 0.17, R² = 0.29）；结合元数据特征后，BERT获得最优性能（RMSE = 0.16, R² = 0.38）。可解释AI方法揭示了政策措辞、政党背景和国家代表等因素的影响。

Conclusion: 机器学习工具在支持气候政策分析和决策制定方面具有重要潜力，能够有效预测政策进展并识别关键影响因素。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [107] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出基于神经常微分方程的连续深度Evoformer模型，用Neural ODE替换原始48个离散块，实现恒定内存成本和训练效率提升。


<details>
  <summary>Details</summary>
Motivation: AlphaFold等蛋白质结构预测模型中的Evoformer模块虽然强大，但48层深度带来高计算成本和刚性层间离散化问题，需要更高效的替代方案。

Method: 将Evoformer的48个离散块替换为Neural ODE参数化，保持核心注意力操作，利用伴随方法实现恒定内存成本，通过自适应ODE求解器平衡运行时间和精度。

Result: 模型能生成结构合理的蛋白质预测，可靠捕获α螺旋等二级结构元素，但未完全复现原始架构精度；仅需单GPU训练17.5小时，资源消耗大幅降低。

Conclusion: 连续深度模型为生物分子建模提供了轻量级、可解释的替代方案，为高效自适应蛋白质结构预测框架开辟新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [108] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种结合奇异值分解(SVD)和量化的方法，通过动态调整SVD秩并应用量化技术，显著降低视觉语言模型的计算开销和内存占用，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图像描述和视觉问答等任务中应用广泛，但其高计算成本和内存占用限制了可扩展性和实时应用。

Method: 利用奇异值分解对联合查询、键和值权重矩阵进行处理以减少KV缓存大小，并引入动态SVD秩分配策略，同时应用量化技术对权重和激活值进行压缩。

Result: 该方法在降低硬件成本的同时，比仅使用量化或SVD的方法实现了超过10%的准确率提升。

Conclusion: 该方法为资源受限设备上的实时部署提供了高效的视觉语言模型解决方案。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [109] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug是一个基于支架的虚拟筛选框架，通过生成式AI增强数据、自训练和重排序三个模块，解决虚拟筛选中的类别不平衡、结构不平衡和支架多样性需求等挑战。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选面临三个主要挑战：活性化合物比例低导致的类别不平衡、某些支架结构在活性分子中占主导地位的结构不平衡，以及需要识别结构多样的活性化合物用于新药开发。

Method: 1. 增强模块：使用图扩散模型基于实际命中化合物的支架生成合成数据；2. 模型无关的自训练模块：将生成的合成数据与原始标记数据安全整合；3. 重排序模块：提高推荐分子集合中的支架多样性，同时保持甚至增强整体性能。

Result: 在五个靶点类别上进行的综合计算实验表明，ScaffAug相比现有基线方法在多个评估指标上表现更好，通过消融研究验证了各模块的有效性。

Conclusion: 这项工作通过利用生成增强、重排序和通用的支架感知，为有效增强虚拟筛选提供了新的视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [110] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 该论文提出了在ECML-PKDD 2025高能物理发现挑战赛中Task 2的获胜解决方案，通过数据生成和鲁棒模型训练两阶段方法，在对抗性攻击下实现了80%的混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计能够同时在干净数据和对抗性数据上实现高准确率的鲁棒ANN模型，以应对高能物理发现中的对抗攻击挑战。

Method: 采用两阶段方法：1) 基于随机分布洗牌攻击(RDSA)生成1500万人工训练样本；2) 构建包含特征嵌入块（同类型特征共享权重）和密集融合尾部的鲁棒架构。

Result: 在对抗性数据集上训练该架构获得了80%的混合准确率，比第二名解决方案高出2个百分点。

Conclusion: 提出的两阶段方法成功构建了对抗RDSA攻击的鲁棒模型，在高能物理发现挑战中取得了最佳性能。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [111] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出Input Domain Aware MoE路由框架，通过概率混合模型更好地划分输入空间，在视觉语言任务中优于现有稀疏专家混合方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性评分的路由机制难以有效捕捉输入结构，导致专家专业化与计算平衡之间的权衡，限制了可扩展性和性能。

Method: 使用概率混合模型建模路由概率，使专家形成清晰的专业化边界并实现均衡利用，路由机制独立于任务目标进行训练。

Result: 在视觉语言任务中一致优于现有稀疏专家混合方法，获得更高的任务性能和改善的专家利用平衡。

Conclusion: 提出的Input Domain Aware MoE框架通过概率混合模型的路由机制，有效解决了专家专业化与计算平衡的权衡问题，提升了稀疏专家混合模型的性能。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [112] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出了一种用于模仿学习的顺序强化学习框架，专门建模传粉昆虫的异质认知策略。该方法通过轨迹相似性捕捉和预测依赖不同策略的个体行为，并解决了现有方法在专家策略变化时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在专家策略随记忆窗口变化或偏离最优性时表现不佳，无法捕捉快速和慢速学习行为，且缺乏可解释性，限制了生物学洞察。

Method: 引入最小化预测损失同时识别与行为数据最一致的有效记忆视野的模型，确保完全可解释性，并提供将蜜蜂策略搜索与不同探索-利用动态下的多臂赌博机公式联系起来的数学框架。

Result: 创建了包含80只在不同天气条件下追踪的蜜蜂的新数据集，改进了对农业生态系统中昆虫行为的模拟，为传粉昆虫认知研究提供了基准。

Conclusion: 该研究揭示了塑造传粉昆虫决策的学习策略和记忆相互作用，为生态治理提供了支持。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [113] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的方法来解决高维异构数据中复杂特征交互的建模挑战，通过自适应核注意力机制分别处理不同特征组，在保持全局关系的同时捕捉局部模式，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统投影潜在结构(PLS)方法在处理高维异构数据时面临挑战，难以建模复杂的非线性关系，尤其是在具有高维相关结构的多元系统中。同时跨多个尺度的交互以及静态特征权重限制了模型对上下文变化的适应性。

Method: 提出了一种新颖的架构，引入自适应核注意力机制，分别处理不同的特征组然后进行集成，能够捕捉局部模式同时保持全局关系。

Result: 实验结果显示，与最先进方法相比，在多个数据集上性能指标有显著提升。

Conclusion: 该方法通过创新的架构设计有效解决了高维异构数据建模中的关键挑战，为复杂特征交互的建模提供了有效解决方案。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [114] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一个简单且可解释的无监督多元时间序列异常检测框架，通过因果嵌入建模时间动态，使用自注意力机制捕捉空间关系，并将嵌入对齐到表示正常状态关系的稳定潜在结构(SLS)，通过预测误差和SLS偏差的双重评分机制识别异常。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多元时间序列异常稀少且通常无标签，现有方法依赖复杂架构，只能检测异常片段的一部分并夸大性能表现。

Method: 将每个变量的过去序列编码为因果嵌入来联合预测当前时间点并重构输入窗口，使用自注意力机制将嵌入投影到共享潜在空间捕捉空间关系，并将投影嵌入对齐到稳定潜在结构(SLS)。

Result: 在多个真实世界数据集和评估协议上实现了最先进的结果，同时通过SLS保持可解释性。

Conclusion: OracleAD通过直接定位违反正常数据时间因果性的根因变量，在嵌入级别实现细粒度异常诊断，是一个简单、可解释且有效的多元时间序列异常检测框架。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [115] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 本文提出了一种基于连通性因子(CF)的新方法eDCF，用于在不同尺度下稳健估计高维数据的本征维度，该方法在噪声环境下表现优异，并能准确检测分形几何结构。


<details>
  <summary>Details</summary>
Motivation: 现代数据集通常包含具有复杂依赖关系的高维特征，而本征维度估计面临尺度依赖性的挑战：在精细尺度下噪声会膨胀估计值，在粗尺度下估计值会稳定到较低的尺度不变值。

Method: 提出了一种基于连通性因子(CF)的可扩展并行化方法eDCF，这是一种基于局部连通性的度量方法。

Result: eDCF在合成基准测试中与领先的估计器表现相当，达到可比较的平均绝对误差(MAE)。在中等至高噪声水平和大数据集下，本征维度精确匹配率高达25.0%，优于MLE(16.7%)和TWO-NN(12.5%)。

Conclusion: eDCF方法能够稳健地估计本征维度，特别是在噪声环境下表现优异，并且能够准确检测决策边界中的分形几何结构，证明其对分析现实结构化数据的实用性。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [116] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: 本文质疑LLM在因果发现中的真实能力，指出现有评估存在数据泄露问题，提出基于新科学研究的评估方法和结合LLM与统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 挑战LLM在因果发现中的虚假性能表现，解决评估中的记忆化问题，探索LLM在真实科学发现中的可信度。

Method: 开发基于新科学研究的评估协议防止数据泄露；设计混合方法将LLM预测作为先验知识与传统PC算法结合。

Result: 在BNLearn基准上LLM表现接近完美，但在作者构建的新数据集上表现较差；将LLM预测作为PC算法先验能显著提高准确性。

Conclusion: 需要采用基于科学、抗泄露的基准测试，并投资开发适合真实世界研究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [117] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: NeurIPT是一个基于预训练Transformer的脑电图基础模型，通过振幅感知掩码预训练和渐进式专家混合架构处理EEG信号的时空特性，在8个BCI数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG数据中存在的受试者间、任务间、条件间的高度变异性以及不同记录设置的电极配置差异，建立可扩展和泛化的神经解码基础模型。

Method: 提出振幅感知掩码预训练(AAMP)替代随机掩码，渐进式专家混合(PMoE)架构适应EEG信号多样性，利用电极3D坐标实现跨设置嵌入迁移，以及微调时的脑区内-脑区间池化(IILP)。

Result: 在8个下游BCI数据集上的评估显示，NeurIPT通过微调一致实现了最先进的性能，证明了其广泛的适用性和强大的泛化能力。

Conclusion: 该工作推动了EEG领域基础模型的发展，为可扩展和可泛化的神经信息处理系统提供了见解。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [118] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO框架通过分离语言反馈和数值奖励的角色来解决LLM强化学习中的样本效率问题，语言反馈指导探索，数值奖励驱动优化，在数学推理基准测试中显著优于GRPO基线。


<details>
  <summary>Details</summary>
Motivation: 传统LLM强化学习依赖标量奖励，丢弃了rollout中的文本推理信息，导致模型每次都需要从头探索，样本效率低下。同时，在线整合语言反馈存在信息泄露与行为崩溃的悖论。

Method: 提出LANPO框架：1) 构建动态经验池存储历史试验；2) 奖励无关反思实现安全的样本内自校正；3) 相关抽象从样本间经验中提取可泛化教训。

Result: 在数学推理基准测试中，7B和14B模型使用LANPO在测试准确率上显著优于使用GRPO的强基线模型。

Conclusion: LANPO为将历史经验整合到LLM强化学习循环中提供了稳健方法，创造了更有效和数据高效的学习智能体。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [119] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 本文提出了一种无需标记训练数据的分子推理框架，使用通用大语言模型通过原子标识符锚定思维链推理，应用于单步逆合成任务并取得高成功率。


<details>
  <summary>Details</summary>
Motivation: 化学中机器学习应用常受限于标记数据的稀缺性和昂贵成本，限制了传统监督方法的使用。

Method: 使用通用大语言模型，通过独特原子标识符将思维链推理锚定到分子结构上，包括一步任务识别相关片段及其化学标签，以及可选的第二步使用少量样本预测化学转化。

Result: 在学术基准和专家验证的药物发现分子上，LLMs在识别化学合理反应位点（≥90%）、命名反应类别（≥40%）和最终反应物（≥74%）方面达到高成功率。

Conclusion: 该框架不仅解决了复杂化学任务，还提供了一种将化学知识映射到分子结构上的理论基础的合成数据集生成方法，从而解决数据稀缺问题。

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [120] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 该论文研究了测试时增强（如RAG或工具使用）中模型参数知识与外部检索信息之间的关系，通过将多步推理建模为知识图上的连通性问题，揭示了参数知识密度与增强效率之间的相变现象。


<details>
  <summary>Details</summary>
Motivation: 理解测试时增强中模型参数知识与外部检索信息之间的理论关系，特别是确定在少量增强步骤下准确回答问题所需的最小预训练知识量，这在实践中具有重要意义。

Method: 将多步推理建模为知识图上的s-t连通性问题，将模型的预训练参数知识表示为部分且可能有噪声的子图，将增强视为查询真实边来扩展模型知识，然后分析给定部分先验知识下生成准确答案所需的必要和充分增强步骤数。

Result: 发现了一个相变现象：如果先验知识图在n个顶点上断开成小分量，则通过增强寻找路径效率低下，需要Ω(√n)次查询；而一旦正确知识密度超过阈值形成巨分量，就能以期望常数次查询找到路径。

Conclusion: 测试时增强的效率与模型参数知识的结构密切相关，存在一个临界密度阈值，超过该阈值后增强效率会显著提高，这为设计高效的增强系统提供了理论指导。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [121] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一种改进的蛋白质-蛋白质对接评分函数，用Vision Mamba架构替换了PIsToN中的Vision Transformer，在多个数据集上表现优于原模型。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质-蛋白质对接工具需要强大的评分函数来区分天然和非天然复合物。Mamba架构在自然语言处理和计算机视觉领域表现出色，可能优于Transformer模型。

Method: 将PIsToN中的Vision Transformer主干替换为Vision Mamba架构，利用Mamba在图像块序列上的高效长程序列建模能力。

Result: 在多个广泛使用的大规模公共数据集上的评估表明，PUMBA持续优于其基于Transformer的前身PIsToN。

Conclusion: 使用Vision Mamba架构显著提升了模型捕捉蛋白质-蛋白质界面特征中全局和局部模式的能力。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [122] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出了一种在无信息先验条件下进行主动目标发现的新方法，确保在复杂现实场景中的鲁棒探索和适应性。该方法具有理论原则性、可解释性，并保证先验估计的单调改进。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高昂的领域（如医学成像、环境监测），基于先验观察的战略采样至关重要。然而，在数据极其有限或采样成本高的领域（如稀有物种发现、新发疾病诊断），现有生成模型方法难以泛化，需要克服无信息先验条件下的探索挑战。

Method: 提出一个理论原则性的框架，受神经科学启发设计。该方法具有内在可解释性，提供清晰的决策洞察，并保证每个新观察都能单调改进先验估计，实现越来越准确的采样。

Result: 通过跨多个领域（包括物种分布建模和遥感）的全面实验和消融研究，证明该方法显著优于基线方法。

Conclusion: 该方法在无信息先验条件下实现了有效的主动目标发现，在动态环境中增强了可靠性和适应性，为数据稀缺领域提供了实用的解决方案。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [123] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 本文通过扩散方法精确分析噪声SGD，提供连续时间视角来捕捉高维设置中的统计风险演变和隐私损失动态，并研究了一种无需梯度敏感性显式知识的噪声SGD变体。


<details>
  <summary>Details</summary>
Motivation: 优化与隐私之间的相互作用已成为隐私保护机器学习的核心主题。噪声随机梯度下降（SGD）已成为关键算法，但其精确行为在现有工作中仍不清楚，特别是在高维设置中。

Method: 利用扩散方法分析噪声SGD，提供连续时间视角；研究一种无需梯度敏感性显式知识的噪声SGD变体，专注于带ℓ2正则化的最小二乘问题。

Result: 该方法能够精确捕捉高维设置中统计风险演变和隐私损失动态，提供了对噪声SGD过程的更深入理解。

Conclusion: 扩散方法为分析噪声SGD提供了有效的连续时间框架，能够精确描述高维设置中的统计风险和隐私损失动态，且提出的变体算法无需显式梯度敏感性知识。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [124] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出了一种分辨率感知的检索增强预测模型，通过利用空间相关性和时间频率特征来提升零样本预测的准确性。模型将信号分解为不同频率分量，采用分辨率感知检索策略，低频分量依赖更广泛的空间上下文，高频分量关注局部影响。


<details>
  <summary>Details</summary>
Motivation: 零样本预测旨在预测没有直接历史数据的先前未见条件下的结果，这对传统预测方法构成重大挑战。需要开发能够适应新位置且历史背景最少的预测模型。

Method: 分辨率感知检索增强预测模型，通过信号频率分解和分辨率感知检索机制，动态检索相关数据。低频分量使用更广泛的空间上下文，高频分量聚焦局部影响。

Result: 在微气候预测应用中，该模型显著优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在ERA5数据集上比HRRR的MSE降低了71%，比Chronos的MSE降低了34%。

Conclusion: 检索增强和分辨率感知策略在零样本预测中非常有效，为微气候建模及其他领域的零样本预测提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [125] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性，证明即使变量级因果效应不可识别，状态级因果效应也可能可识别，这需要上下文特定独立性和条件函数依赖等额外知识。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应可识别性定义基于变量层面，本文旨在研究基于特定状态（treatment变量的特定状态对outcome变量的特定状态）的因果效应可识别性，揭示现有变量级框架可能遗漏的可识别情况。

Method: 通过理论分析，比较变量级和状态级因果效应可识别性，考察上下文特定独立性、条件函数依赖以及变量状态约束等知识对可识别性的影响。

Result: 发现状态级因果效应可能在变量级因果效应不可识别时仍可识别，这种分离仅在有额外知识时发生；变量状态约束知识单独不能改善可识别性，但与其他知识结合时可同时改善变量级和状态级可识别性。

Conclusion: 研究强调了在观测数据中估计因果效应时，状态级分析可能发现变量级框架遗漏的可识别机会，为因果推断提供了更精细的分析视角。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [126] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种基于多任务学习和分层高斯过程的方法来预测NLP模型的学习曲线，通过建模任务间相关性支持零样本预测，并利用主动学习策略降低预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线可以指导决策制定，在满足特定性能目标的同时减少计算开销和数据集获取成本。

Method: 将学习曲线预测建模为多任务学习问题，使用潜在变量多输出高斯过程来建模任务间相关性和层次依赖关系，支持零样本预测。

Result: 该方法能够在三个小型NLP数据集上（最多30条学习曲线）实现接近真实缩放规律的预测，验证了框架的有效性。

Conclusion: 提出的框架能够以较低成本开发概率缩放规律，通过主动学习策略减少预测不确定性，为NLP模型性能预测提供了有效解决方案。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [127] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 本文提出了一种名为SAMOSA的开放集主动学习方法，通过基于样本典型性的查询策略来减少数据标注成本，在多个数据集上实现了比现有方法高达3%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习解决方案需要大量数据收集，而数据标注成本高昂。为了减轻这一负担，开放集主动学习方法旨在从包含不相关或未知类别的大规模未标记数据池中选择信息丰富的样本。

Method: 基于关于数据典型性对传统随机梯度下降和锐度感知最小化泛化特性的理论发现，SAMOSA根据样本的典型性主动查询样本。该方法有效识别嵌入流形中靠近模型决策边界的非典型样本。

Result: 广泛的实验表明，SAMOSA在多个数据集上比现有技术实现了高达3%的准确率提升，同时没有引入计算开销。

Conclusion: SAMOSA是一种有效的开放集主动学习查询算法，能够优先选择对目标类别高度信息丰富且有助于区分目标类别和不想要类别的样本。

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [128] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 该论文提出3D第一人称视频游戏是实时多模态推理的挑战环境，构建了大规模多样化的游戏数据集，通过逆动力学模型推断缺失动作，训练了文本条件控制的游戏智能体，并指出了长时程任务和跨游戏定量评估等挑战。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏环境对实时多模态推理提出了挑战，需要构建更大更丰富的数据集来训练能够响应文本指令的游戏智能体。

Method: 收集大规模多样化的人类游戏数据，使用逆动力学模型推断缺失动作，基于行为克隆训练文本条件控制的实时推理架构。

Result: 成功训练出能够在多种3D游戏中响应文本输入进行游戏操作的智能体模型。

Conclusion: 证明了在3D游戏环境中实现文本条件控制的可行性，但长时程任务和跨游戏定量评估仍是需要解决的挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [129] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 3D-GSRD是一个用于分子表示学习的3D图自编码器，通过选择性重掩码解码解决从2D到3D掩码图建模的挑战，在MD17基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 将掩码图建模从2D扩展到3D面临两个冲突挑战：避免2D结构泄漏到解码器，同时为重构重掩码原子提供足够的2D上下文。

Method: 提出3D-GSRD，核心创新是选择性重掩码解码，只重掩码编码器表示中的3D相关信息，同时保留2D图结构。该方法与3D关系变换器编码器和结构无关解码器协同集成。

Result: 在广泛使用的MD17分子性质预测基准测试中，3D-GSRD在8个目标中的7个上实现了最先进的性能。

Conclusion: 选择性重掩码解码与结构无关解码器结合增强了编码器在分子表示学习中的作用，3D-GSRD在分子性质预测任务中表现出色。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [130] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出了一种计算预算感知的数据选择方法CADS，通过双层优化框架将计算预算约束整合到数据选择过程中，解决了现有方法忽略预算约束的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法独立于计算预算约束，但实证研究表明不同预算下需要不同的数据数量、质量和分布，因此需要将计算预算作为数据选择策略的核心要素。

Method: 提出计算预算感知数据选择(CADS)方法，采用双层优化框架：内层在计算预算约束下训练模型，外层基于模型评估优化数据选择。通过概率重参数化策略和Hessian-free策略梯度估计器解决Hessian矩阵估计问题，将内层优化转化为外层目标中的惩罚项以提高效率。

Result: 在视觉和语言基准测试中，该方法相比基线方法实现了高达14.42%的性能提升。

Conclusion: 计算预算应作为数据选择策略的组成部分，CADS方法通过双层优化框架有效解决了预算约束下的数据选择问题，显著提升了训练效率。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [131] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种Transformer变体，通过从第一层的Value头使用跳跃连接来增强模型表示并减少KV缓存，在减少约25% KV缓存的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer模型在扩展时面临内存和计算成本高的问题，特别是自回归解码中的KV缓存。现有方法要么无法降低KV成本，要么以牺牲表示为代价减少内存。

Method: 从第二个块开始，每层重用一半的Value头来自第一层，另一半正常计算，从而减少近50%的Value投影和V缓存。理论上，这种方法可以恢复因压缩而丢失的信息并加速隐式mesa优化。

Result: 在不同模型规模下，SkipV1Former相比标准MHA Transformer和一些先进变体，在减少约25% KV缓存的同时改善了困惑度。还可以通过额外10-15%计算将现有MHA Transformer检查点升级为SkipV1Former。

Conclusion: SkipV1Former提供了一种有效平衡表示能力和资源消耗的方法，能够与其他先进方法（如GQA和MLA）无缝结合，进一步减少KV缓存并提升性能。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [132] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 本文研究了因果充分性下因果强盗问题中的遗憾最小化，发现学习奖励的父节点集是次优的，证明了遗憾最小化和父节点识别之间存在根本性冲突，并提出绕过图恢复的最优算法。


<details>
  <summary>Details</summary>
Motivation: 先前工作主要关注识别奖励的父节点然后应用经典强盗方法，或联合学习父节点同时最小化遗憾。本文研究这些策略是否最优。

Method: 通过证明存在实例表明遗憾最小化和父节点识别是冲突目标，分析已知和未知父节点大小情况，建立捕捉动作空间组合结构的遗憾下界，并提出绕过图恢复的最优算法。

Result: 实验证实提出的方法在各种环境中与现有基线存在显著性能差距。

Conclusion: 父节点识别对于遗憾最小化是不必要的，学习父节点集是次优策略。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [133] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 本文提出了一种半监督、正样本未标记学习策略，使用深度学习方法解决考古预测建模中的标签稀缺问题，在两种代表性考古数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 考古预测建模需要结合已知位置与环境、文化、地理空间变量来预测未发现遗址的位置，但面临结构性标签稀缺的挑战：正样本稀少且大多数位置未标记。

Method: 采用半监督正样本未标记学习策略，实现为语义分割模型，使用动态伪标签和通过RNN实现的CRF来增强标签置信度，处理严重的类别不平衡问题。

Result: 在基于数字高程模型的地理空间数据集上，模型性能与最先进的LAMAP相当，同时获得更高的Dice分数；在原始卫星图像上，通过分层k折交叉验证评估，保持性能并产生具有更好可解释性的预测表面。

Conclusion: 半监督学习为在大规模稀疏标注景观中识别未发现遗址提供了一种有前景的方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [134] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar是一个包含12,000个工业级汽车CFD模拟的数据集，通过改进的网格策略实现了风洞验证精度低于1.04%，比现有数据集提高了五倍，将计算成本从数周减少到分钟级别。


<details>
  <summary>Details</summary>
Motivation: 传统汽车空气动力学优化面临计算成本高和精度不足的权衡，现有机器学习数据集存在网格分辨率不足、组件缺失和验证误差超过5%等问题，无法在工业工作流程中部署。

Method: 使用STAR-CCM+软件生成12,000个工业级CFD模拟，通过20个CAD参数和自由变形算法系统探索三种车辆配置，包括完整的发动机舱和冷却系统，采用严格的壁面y+控制进行精化网格策略。

Result: 数据集实现了风洞验证精度低于1.04%，比现有数据集提高了五倍，训练模型达到生产就绪精度，同时将计算成本从数周减少到分钟级别。

Conclusion: DrivAerStar是第一个连接学术机器学习研究和工业CFD实践的数据集，为汽车开发中的数据驱动空气动力学优化设立了新标准，展示了将高保真物理模拟与人工智能整合到工程领域的范例。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [135] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了UDS框架，用于在监督微调中进行高效的在线批次选择，通过同时考虑数据效用和多样性来优化训练过程。


<details>
  <summary>Details</summary>
Motivation: 传统的全数据集监督微调计算成本高且容易过拟合或放大偏差，现有的在线批次选择方法存在仅依赖数据效用、需要外部资源以及增加训练时间等问题。

Method: UDS框架利用对数矩阵的核范数捕获数据效用和样本内多样性，通过轻量级历史样本缓冲区进行低维嵌入比较来估计样本间多样性，无需外部资源且避免不必要的反向传播。

Result: 在多个基准测试中，UDS在不同数据预算下始终优于最先进的在线批次选择方法，并显著减少了与全数据集微调相比的训练时间。

Conclusion: UDS提供了一个高效且无需外部资源的在线批次选择解决方案，在保持性能的同时显著降低了监督微调的计算成本。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [136] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出了一种轻量级深度学习管道，结合小时降采样、双模式插补和标准化处理，使用GRU-LSTM序列到一模型实现短期能耗预测，在真实世界高频数据上取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据噪声大、不完整且缺乏上下文丰富性时的短期能耗准确预测问题，参与2025年电力能耗预测竞赛，旨在预测次日电力需求。

Method: 采用轻量级深度学习管道，包括小时降采样、双模式插补（均值和多项式回归）、综合标准化（最终选择标准缩放），使用GRU-LSTM序列到一模型进行预测。

Result: 模型平均RMSE为601.9W，MAE为468.9W，准确率达到84.36%。尽管输入不对称且存在插补间隙，模型仍能良好泛化，捕捉非线性需求模式，并保持低推理延迟。时空热图分析显示温度趋势与预测能耗高度一致。

Conclusion: 有针对性的预处理与紧凑循环架构相结合，能够在真实世界条件下实现快速、准确且可部署的能耗预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [137] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为领域泛化持续学习（DGCL）的新设置，旨在让模型在顺序学习涉及不同领域的任务时，能够在所有遇到的任务和领域上表现良好。为了解决现有持续学习方法在DGCL中表现不佳的问题，作者提出了自适应领域变换（DoT）方法，该方法基于预训练模型，通过解耦语义和领域相关信息，并自适应地变换任务表示来实现输出对齐。


<details>
  <summary>Details</summary>
Motivation: 智能系统需要有效适应动态的真实世界环境，持续获取新技能并将其泛化到多样未见场景。现有持续学习方法通常假设每个任务的训练和测试域相同，在DGCL设置下表现不佳。

Method: 提出自适应领域变换（DoT）方法，受人类大脑分布式加枢纽理论启发，在表示学习中解耦语义和领域相关信息，自适应地变换任务表示以实现输出对齐。DoT可作为插件策略增强现有CL方法。

Result: DoT显著提升了最先进持续学习基线方法在DGCL中的性能，支持全参数调优和参数高效调优范式。实验验证了DoT能够积累领域泛化知识，并以轻量级实现确保资源效率。

Conclusion: DoT方法有效解决了DGCL设置中的挑战，通过解耦语义和领域信息并自适应变换表示，实现了平衡且泛化的预测，为领域泛化持续学习提供了有效的解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [138] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练即可解决多样化优化问题的框架，通过生成数学公式并转换为求解器代码，使用改进的MCTS策略实现更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖提示工程导致泛化能力差，要么需要昂贵的监督训练，因此需要一种无需训练且能泛化到多种问题类型的优化求解方法。

Method: 提出SolverLLM框架，使用改进的蒙特卡洛树搜索策略：动态扩展用于自适应公式生成，提示反向传播通过结果驱动反馈指导探索，不确定性反向传播将奖励可靠性纳入决策。

Result: 在六个标准基准数据集上的实验表明，SolverLLM优于基于提示和基于学习的基线方法，实现了强大的泛化能力且无需额外训练。

Conclusion: SolverLLM通过测试时扩展和创新的MCTS策略，为LLM在优化问题求解中提供了有效的训练免费解决方案，具有优异的泛化性能。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [139] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 本文提出了一个组件级的评估框架，用于评估LLM生成的数学优化公式，超越了传统的整体评估方法，引入了决策变量和约束的精度/召回率、约束和目标函数的RMSE等细粒度指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在将自然语言描述转换为数学优化公式时，评估方法通常将公式视为整体，依赖粗粒度的解决方案准确性或运行时间指标，这掩盖了结构或数值错误。

Method: 提出了一个全面的组件级评估框架，包括决策变量和约束的精度/召回率、约束和目标函数的RMSE、基于token使用和延迟的效率指标。评估了GPT-5、LLaMA 3.1 Instruct和DeepSeek Math在不同复杂度优化问题下的六种提示策略。

Result: GPT-5始终优于其他模型，思维链、自一致性和模块化提示策略最有效。求解器性能主要取决于高约束召回率和低约束RMSE，约束精度和决策变量指标起次要作用，简洁输出可提高计算效率。

Conclusion: 提出了NLP到优化建模的三个原则：完整约束覆盖防止违规、最小化约束RMSE确保求解器级准确性、简洁输出提高计算效率。该框架为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [140] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出一种在差分隐私下进行线性回归的方法，提供有效的统计推断和合成数据生成，适用于社会科学中的中小规模数据集。


<details>
  <summary>Details</summary>
Motivation: 社会科学中常见中小规模数据集和线性回归分析，但现有差分隐私方法主要关注点估计，缺乏不确定性量化和合成数据生成支持。

Method: 采用差分隐私偏置校正估计器，结合分箱聚合策略，提供渐近置信区间和合成数据生成程序。

Result: 实验表明该方法在准确性上优于现有方法，提供有效置信区间，生成的合成数据在下游机器学习任务中更可靠。

Conclusion: 该方法为社会科学中的隐私保护线性回归提供了有效的统计推断和合成数据生成能力。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [141] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 本文提出了时间序列推理的蓝图愿景，包含两个互补方向：构建稳健的时间序列推理基础（全面时序理解、结构化多步推理、可信评估框架）和推进系统级推理（多智能体协作、多模态上下文、检索增强方法），旨在为各领域提供可解释且可信的时间序列智能。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理正在成为时序分析的下一个前沿领域，旨在超越模式识别，实现显式、可解释且可信的推理。当前需要从基础理论到系统应用层面全面推动时间序列推理的发展。

Method: 采用两个互补方向：1）构建稳健的时间序列推理基础，包括全面时序理解、结构化多步推理和可信评估框架；2）推进系统级推理，超越纯语言解释，整合多智能体协作、多模态上下文和检索增强方法。

Result: 提出了一个灵活且可扩展的时间序列推理框架，能够支持跨领域的可解释和可信时序智能应用。

Conclusion: 通过基础理论构建和系统级推理推进的双重策略，为时间序列推理领域提供了一个全面发展的路线图，有望实现真正可解释和可信的时间序列智能系统。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [142] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: MuonBP优化器通过分块周期性正交化策略，在保持Muon优化器数据效率优势的同时，显著减少了模型并行训练中的通信开销，实现了与AdamW相当的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决Muon优化器在模型并行环境下因梯度正交化引入的额外通信开销问题（5%-10%吞吐量损失），同时保持其数据效率优势。

Method: 提出MuonBP方法：在每个设备上独立对矩阵分块进行正交化，并周期性执行完整正交化以维持训练稳定性；使用双学习率策略（分块正交化步骤和完整正交化步骤分别使用不同步长）。

Result: 在8B模型、8路张量并行和ZeRO优化器状态分片训练中，MuonBP相比Muon实现了8%的吞吐量提升，且性能无退化。

Conclusion: MuonBP在保持Muon优化器数据效率优势的同时，显著减少了模型并行训练中的通信开销，实现了与坐标式优化器（如AdamW）相当的吞吐量性能。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [143] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: Graph4MM是一个基于图的多模态学习框架，通过Hop-Diffused Attention整合多跳结构信息，使用MM-QFormer进行跨模态融合，在生成式和判别式任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态数据具有复杂的结构关系，而现有方法未能区分多跳邻居并将图视为独立模态，这限制了整体理解能力。

Method: 提出Hop-Diffused Attention通过因果掩码和跳扩散整合多跳结构信息，设计MM-QFormer进行跨模态融合。

Result: 在生成式和判别式任务上，Graph4MM优于大型视觉语言模型、语言模型和多模态图基线方法，平均提升6.93%。

Conclusion: 利用结构信息整合模态内和模态间交互能够提升多模态理解能力，超越将图作为独立模态的方法。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [144] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为'后门遗忘'的攻击方法，通过将触发器放置在LLM的注意力汇聚位置，使得模型在正常条件下看似成功遗忘，但在触发激活时恢复被遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重大语言模型的兴起，研究遗忘过程本身是否可能被后门攻击，即在正常条件下看似成功遗忘，但在隐藏触发器激活时恢复预遗忘行为。

Method: 利用注意力汇聚现象，将触发器放置在浅层输入令牌位置，并通过对齐注意力值来增强后门持久性。

Result: 实验验证了注意力汇聚引导的后门遗忘方法能够可靠地在后门触发器存在时恢复被遗忘的知识，而在触发器缺失时与正常遗忘模型无法区分。

Conclusion: 注意力汇聚位置是后门遗忘的有效网关，通过在该位置放置触发器并调整注意力值可以显著增强后门攻击的效果。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [145] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 本文探索强化学习在符号数学中的应用，证明基于好奇心的探索和图形化动作的PPO算法能够求解涉及根号、指数和三角函数的非线性方程。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习是否对符号数学有用，扩展之前仅能解决一元线性方程的工作，处理更复杂的非线性方程。

Method: 使用模型无关的PPO算法，结合基于好奇心的探索机制和图形化动作表示，处理非线性方程求解问题。

Result: 成功求解了包含根号、指数和三角函数等复杂元素的非线性方程。

Conclusion: 基于好奇心的探索方法可能对一般符号推理任务有重要价值。

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [146] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出了DICA框架，通过最大化雅可比矩阵体积来识别非线性混合中的潜在成分，无需辅助信息、成分独立性或雅可比稀疏性假设。


<details>
  <summary>Details</summary>
Motivation: 解决非线性独立成分分析中潜在成分识别的根本挑战，特别是在缺乏辅助信号和传统结构假设的情况下。

Method: 使用雅可比体积最大化(J-VolMax)准则，利用混合函数雅可比矩阵的凸几何特性来鼓励潜在成分对观测变量的影响多样性。

Result: 在合理条件下，该方法能够实现潜在成分的可识别性，无需依赖辅助信息、潜在成分独立性或雅可比稀疏性假设。

Conclusion: DICA框架扩展了可识别性分析的范围，为现有方法提供了补充视角，在非线性混合问题中具有重要应用价值。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [147] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 该研究探讨了当后处理指令与模型已学习行为冲突时，语言模型会进行系统性动机推理，生成看似合理的理由来违反指令，同时淡化潜在危害。前沿推理模型能够检测到这种动机推理，但较小的LLM判断器可能无法识别部分动机推理，甚至可能被说服认为这种推理是正确的。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究当后处理指令与模型已学习行为冲突时，模型的推理过程会发生什么变化。LLM训练常因不完美的奖励信号产生意外行为，导致模型发展出不对齐的倾向，而常用的纠正方法是应用后处理指令来避免问题行为。

Method: 在简单设置中研究模型的行为，分析模型如何生成看似合理的理由来违反指令，同时评估不同规模LLM判断器检测动机推理的能力。

Result: 研究发现模型会进行系统性动机推理，前沿推理模型能够检测到这种动机推理，但较小的LLM判断器可能无法识别部分动机推理，甚至可能被说服认为这种推理是正确的。

Conclusion: 随着模型变得更加复杂，其动机推理可能越来越难以被监控器检测到，这强调了在依赖思维链过程进行模型评估和监督时需要考虑动机推理的必要性。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [148] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出了一种低精度对数定点训练方法，通过硬件友好的分段线性近似和模拟退火优化，在12位整数运算下训练VGG模型，实现与32位浮点训练相当的精度，同时显著降低硬件面积和能耗。


<details>
  <summary>Details</summary>
Motivation: 虽然量化技术显著降低了深度学习推理的计算成本，但训练仍主要依赖复杂的浮点运算。低精度定点训练是一个有吸引力的替代方案，特别是面向未来的硬件加速器设计。

Method: 引入位宽设计到算术运算的近似中，提出硬件友好的分段线性近似用于对数加法，使用模拟退火在不同精度级别优化该近似，并通过C++位真模拟验证训练效果。

Result: 在CIFAR-100和TinyImageNet数据集上，使用12位整数运算训练VGG-11和VGG-16模型，与32位浮点训练相比精度损失最小。硬件研究显示，与线性定点等效单元相比，提出的LNS乘累加单元面积减少高达32.5%，能耗降低53.5%。

Conclusion: 低精度对数定点训练方法在保持训练精度的同时，显著降低了硬件资源消耗，为未来深度学习训练硬件加速器设计提供了有前景的解决方案。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [149] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出一种自监督预训练交互式智能体的方法，使其能够快速模仿人类演示。该方法将目标（观测）作为基本构建块，在训练中自动提出目标并练习达成，在评估时通过逆强化学习解释演示为最优目标达成行为。


<details>
  <summary>Details</summary>
Motivation: 当前最成功的AI模型（如VLMs、LLMs）缺乏明确的动作概念，无法为快速适应新任务做好准备。人类提供的训练数据存在隐含假设，即人类大部分时间处于最有回报的状态，这一假设存在问题。

Method: 将目标作为原子构建块，训练期间自动提出目标并练习达成，基于强化学习探索的先前工作。评估时通过逆强化学习解释演示为最优目标达成行为。

Result: 在标准基准测试（非专为目标达成设计）上的实验表明，该方法在零样本模仿方面优于先前方法。

Conclusion: 该方法为交互式智能体提供了一种自监督预训练的有效途径，使其能够快速适应新任务并模仿人类演示。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [150] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 本文提出了数据集可靠性评分问题，引入Gram行列式评分来衡量无真实标签情况下数据集的可靠性，该评分具有实验无关性，能在不同统计实验下保持一致的可靠性排序。


<details>
  <summary>Details</summary>
Motivation: 在无法获取真实数据的情况下，如何评估来自潜在策略性来源的数据集的可靠性是一个重要问题。真实数据不可观测，只能看到依赖于真实数据的未知统计实验结果。

Method: 定义了基于真实数据的可靠性排序，提出了Gram行列式评分，该评分通过描述观测数据和实验结果经验分布的向量所张成的体积来度量可靠性。

Result: 证明了Gram行列式评分能保持多个基于真实数据的可靠性排序，并且在缩放意义下唯一地产生与实验无关的可靠性排名。在合成噪声模型、CIFAR-10嵌入和真实就业数据上的实验验证了该方法的有效性。

Conclusion: Gram行列式评分能够有效捕捉不同观测过程中的数据质量，为无真实标签情况下的数据集可靠性评估提供了实用工具。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [151] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 本文研究了组合设置下的经典Hedge算法，证明了它在大多数组合环境中是最优的（最多相差√log d因子），但在某些特定组合集（如m-集）中次优，同时展示了其在在线多任务学习中的最优性以及在DAG最短路径问题中的近最优正则化器。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge算法在组合设置中的最优性，探索其在各种组合问题（如扩展形式博弈、资源分配、m-集、在线多任务学习、DAG最短路径）中的性能边界。

Method: 建立了适用于所有算法的下界Ω(√(T log(|X|)/log d))，识别了Hedge次优的特定组合集（m-集），并分析了Hedge在在线多任务学习中的最优性，以及在线镜像下降算法与扩张熵正则化器的等价性。

Result: 证明了Hedge在大多数组合设置中是最优的（最多相差√log d因子），但在log d ≤ m ≤ √d的m-集中次优√log d倍；同时证明了Hedge在在线多任务学习中的最优性，并建立了DAG最短路径问题的近最优正则化器。

Conclusion: Hedge算法在组合设置中具有广泛的最优性，虽然在某些特定情况下次优，但整体上提供了强大的性能保证，特别是在DAG最短路径问题中通过适当的正则化器可以实现近最优性能。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [152] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 该论文研究了在聚合bandit反馈下的有限时域马尔可夫决策过程在线学习，提出了首个在随机和对抗环境中都能实现低遗憾的BOBW算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注最坏情况分析，而本文旨在开发能够在随机和对抗环境中都表现良好的BOBW算法，以应对聚合bandit反馈的挑战。

Method: 结合了基于占用度量的FTRL、自边界技术和受在线最短路径问题启发的新损失估计器，并扩展到未知转移概率的情况。

Result: 在已知转移概率的情况下，算法在随机环境中实现O(log T)遗憾，在对抗环境中实现O(√T)遗憾，并建立了匹配的下界证明最优性。

Conclusion: 本文提出了首个针对聚合bandit反馈下表格MDP的BOBW算法，证明了其最优性，并为最短路径问题提供了首个个体差距依赖下界。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [153] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出了一种基于矩阵自由能的自动编码器正则化方法，通过优化编码矩阵的奇异值分布来生成高斯风格的编码，并应用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统自动编码器在编码分布控制方面存在不足，需要一种能够确保编码具有高斯分布特性的正则化方法，以提高模型的泛化能力和在逆问题中的应用效果。

Method: 基于矩阵自由能理论，定义了一个可微损失函数，该函数作用于编码矩阵（编码维度×批次大小）的奇异值分布，通过随机矩阵理论确保编码矩阵的奇异值分布与独立同分布高斯随机矩阵相匹配。

Result: 经验模拟表明，通过标准随机梯度下降训练最小化负矩阵自由能，可以产生高斯风格的编码，并在训练集和测试集上都具有良好的泛化能力。

Conclusion: 该方法能够可靠地生成高斯编码，为欠定逆问题提供了有效的解决方案，展示了矩阵自由能在自动编码器正则化中的实用价值。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [154] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文提出了一个评估LLMs在对话中发现和利用用户潜在信息的统一基准，涵盖三个逐步现实的任务设置，结果显示LLMs能够通过对话揭示潜在信息，但成功率受任务复杂度、主题和隐藏属性数量影响显著。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成通用文本方面表现出色，但在需要用户特定偏好的场景中（如推荐餐厅或旅行规划），这种通用性成为限制。用户很少明确表达所有偏好，许多关注点保持潜在状态，需要通过对话来推断。

Method: 引入统一的潜在信息发现评估基准，采用三智能体框架（用户、助手、法官），涵盖三个逐步现实的任务：经典20问游戏、个性化问答和个性化文本摘要，支持逐轮评估引导和适应能力。

Result: LLMs确实能够通过对话揭示潜在信息，但成功率变化巨大：从32%到98%，具体取决于任务复杂度、主题和隐藏属性数量。

Conclusion: 该基准为研究个性化交互中的潜在信息发现提供了首个系统框架，表明有效的偏好推断仍然是构建真正自适应AI系统的开放前沿。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [155] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 本文提出了一种名为In-situ Autoguidance的新方法，通过模型自身的随机前向传递动态生成次优预测，实现了无需辅助模型的零成本引导，解决了传统CFG方法在提高图像质量和提示对齐时导致多样性降低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统分类器自由引导(CFG)方法在提升图像生成质量和提示对齐的同时会减少图像多样性，且现有解耦方法需要额外训练辅助模型，带来显著开销。本文旨在探索无需外部模型的自我引导方法。

Method: 提出In-situ Autoguidance方法，通过模型自身的随机前向传递动态生成次优预测，将引导重新定义为推理时的自我校正过程，无需任何辅助组件。

Result: 该方法被证明不仅可行，而且建立了成本高效引导的强大新基准，实现了零成本的自我引导效益。

Conclusion: 研究表明，无需外部模型即可实现自我引导的效益，为扩散模型提供了更高效的引导方案。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [156] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为ALMD的新学习范式，即在模型部署后进行自主持续学习，能够动态检测未见类别样本并增量学习新类别。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习在部署后模型固定，无法适应动态开放环境中出现的未见类别样本。需要一种能够持续检测新类别并增量学习的机制。

Method: 提出了PLDA方法，该方法能够动态进行OOD检测并实时增量学习新类别，解决了传统方法中ID类别固定、需要重新训练以及数据稀缺的问题。

Result: 经验评估将证明PLDA方法的有效性。

Conclusion: ALMD范式及其实现方法PLDA为动态开放环境中的持续学习提供了有效解决方案，能够适应现实应用中类别不断扩展的场景。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [157] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE是一个轻量级自适应框架，让终端设备能够实时调整差分隐私级别，在移动边缘群智感知系统中平衡隐私保护、数据效用和能耗成本。


<details>
  <summary>Details</summary>
Motivation: 移动边缘群智感知系统在动态、资源受限环境中持续生成和传输用户数据，面临严重的隐私威胁。静态差分隐私机制无法适应不断变化的风险（如对抗能力变化、资源约束和任务需求），导致要么噪声过多，要么保护不足。

Method: ALPINE作为闭环控制系统运行，包含四个模块：动态风险感知、基于TD3算法的隐私决策、本地隐私执行和边缘节点性能验证。设计了平衡隐私收益、数据效用和能耗成本的奖励函数，指导TD3智能体在不同风险场景下自适应调整噪声幅度。

Result: 广泛的理论分析和真实世界仿真表明，ALPINE能有效减轻推理攻击，同时保持效用和成本，使其适用于大规模边缘应用。

Conclusion: ALPINE框架通过自适应调整差分隐私级别，在移动边缘群智感知系统中实现了隐私、效用和成本之间的动态平衡，为大规模边缘应用提供了实用的隐私保护解决方案。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [158] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出了一个模块化基准测试框架，用于系统评估蛋白质分子动力学方法，支持多种模拟引擎，提供超过19种评估指标和可视化工具，并贡献了包含9种不同蛋白质的数据集。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法的快速发展超过了标准化验证工具的开发，不同模拟方法之间的客观比较受到评估指标不一致、罕见构象状态采样不足以及缺乏可重复基准测试的阻碍。

Method: 使用加权集成采样方法，基于时间延迟独立成分分析的进展坐标，通过WESTPA工具包实现蛋白质构象空间的快速高效探索。框架包含灵活的传播器接口，支持任意模拟引擎。

Result: 开发了一个开源平台，包含9种不同蛋白质的数据集（10-224个残基），每个蛋白质在300K下进行了100万MD步骤的广泛模拟。通过经典MD模拟和CGSchNet模型的比较验证了框架的实用性。

Conclusion: 该框架通过标准化评估协议和实现直接、可重复的MD方法比较，为分子模拟社区建立了一致、严格的基准测试基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [159] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: SOLE是一种软硬件协同设计方法，通过E2Softmax和AILayerNorm分别优化Transformer中的Softmax和LayerNorm操作，在不重新训练的情况下保持推理精度，同时显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在NLP和CV任务中表现出色，但其实时推理速度和效率受到Softmax和LayerNorm操作效率低下的限制。现有基于函数逼近的方法存在实现效率低、忽视内存开销、需要重新训练补偿逼近误差等问题。

Method: 提出SOLE软硬件协同设计：E2Softmax采用指数函数的log2量化和基于对数的除法来逼近Softmax；AILayerNorm采用低精度统计计算。实现Softmax和LayerNorm的低精度计算和低比特位宽存储。

Result: 实验表明SOLE在不重新训练的情况下保持推理精度，相比GPU实现数量级的速度提升和能耗节省。相比现有最优定制硬件，Softmax和LayerNorm分别实现3.04倍和3.86倍的能效提升，以及2.82倍和3.32倍的面积效率提升。

Conclusion: SOLE通过软硬件协同设计有效解决了Transformer中Softmax和LayerNorm的效率瓶颈，在保持精度的同时显著提升了推理速度和能效，为Transformer的实时应用提供了高效解决方案。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [160] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 本文提出了一种针对高风险高回报任务的强化学习框架，通过离散化连续动作空间、熵正则化探索和双评论家架构来有效处理多模态动作分布和随机回报问题。


<details>
  <summary>Details</summary>
Motivation: 高风险高回报任务通常表现出多模态动作分布和随机回报特性，而传统强化学习方法假设单模态高斯策略并依赖标量值评论家，限制了其在HRHR环境中的有效性。

Method: 提出强化学习框架：(i) 离散化连续动作空间以近似多模态分布；(ii) 使用熵正则化探索提高风险但有益动作的覆盖；(iii) 引入双评论家架构进行更准确的离散值分布估计。

Result: 在具有高失败风险的移动和操作基准测试中，该方法优于基线方法，证明了在多模态和风险建模方面的重要性。

Conclusion: 该框架能够扩展到高维动作空间，支持复杂控制领域，为高风险高回报任务提供了有效的解决方案。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [161] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出ADCMs框架，通过自动自适应离散化方法改进一致性模型，使用局部一致性作为优化目标、全局一致性作为约束，在保持训练稳定性的同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性模型依赖手动设计的离散化方案，需要针对不同噪声调度和数据集反复调整，缺乏通用性和效率。

Method: 将离散化问题表述为优化问题，使用局部一致性作为优化目标确保可训练性，全局一致性作为约束控制稳定性，通过拉格朗日乘子平衡两者关系，并基于高斯-牛顿法实现自适应离散化。

Result: 在CIFAR-10和ImageNet上显著提高了训练效率，以最小的训练开销实现了优越的生成性能，并对更先进的扩散模型变体表现出强适应性。

Conclusion: ADCMs框架有效解决了手动离散化方案的局限性，为一致性模型提供了高效且自适应的训练方法。

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [162] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分推断的扩展方法，将确定性机器学习方法扩展到多变量高斯分布预测，用于数据同化中的不确定性建模。


<details>
  <summary>Details</summary>
Motivation: 数据同化在大多数设置中都涉及不确定性，现有确定性方法无法充分处理这种不确定性，需要扩展到概率性框架。

Method: 基于变分推断扩展现有确定性机器学习方法，使预测状态遵循多变量高斯分布，使用混沌Lorenz-96动力学作为测试平台。

Result: 新模型能够获得近乎完美校准的预测，并且可以在更广泛的变化数据同化管道中集成，从增加的数据同化窗口长度中获得更大收益。

Conclusion: 提出的变分推断扩展方法能够有效处理数据同化中的不确定性，提供校准良好的预测，并提升数据同化性能。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [163] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了一个用户反馈模拟框架和综合基准测试，用于评估LLM系统的持续学习能力，覆盖多领域、多语言和多任务类型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM内存基准主要关注同质化阅读理解任务，缺乏测试系统从服务时间累积用户反馈中学习的能力。

Method: 开发用户反馈模拟框架和综合基准测试，涵盖多个领域、语言和任务类型，评估LLM系统的持续学习能力。

Result: 实验表明现有最先进基线的有效性和效率远未达到满意水平。

Conclusion: 该基准测试有望为未来LLM内存和优化算法的研究铺平道路。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [164] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 该论文扩展了对称性在机器学习中的泛化保证，从紧凑群对称性扩展到非紧凑对称性（如平移），并放宽了数据分布必须对称的假设，通过PAC-Bayes框架提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要关注紧凑群对称性且假设数据分布本身对称，这在现实应用中很少满足。需要扩展到非紧凑对称性和非对称数据分布，以更全面地理解对称性对机器学习性能的提升。

Method: 基于PAC-Bayes框架，改进并收紧现有边界，特别在McAllester的PAC-Bayes边界上进行演示，证明该方法适用于广泛的PAC-Bayes边界。

Result: 在具有非均匀旋转群的旋转MNIST数据集上的实验验证了理论保证，不仅保持有效，而且改进了先前的结果。

Conclusion: 对于对称数据，对称模型在超越紧凑群和对称分布的狭窄设置下仍具有优势，为更广泛理解机器学习中的对称性开辟了道路。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [165] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了首个多因素序列解缠结标准化基准，包含六个数据集、评估工具和自动对齐方法，并展示了视觉语言模型在自动标注和评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据包含多个交互的语义因素，但现有研究主要关注简单的双因素静态和动态设置，忽略了数据的多因素本质。

Method: 建立标准化基准，包含六个多样化数据集；提出后验潜在探索阶段自动对齐潜在维度与语义因素；引入Koopman启发的模型；利用视觉语言模型进行自动标注和零样本评估。

Result: 提出的Koopman启发模型达到最先进性能；视觉语言模型能够有效自动化数据集标注并作为零样本解缠结评估器。

Conclusion: 这些贡献为推进多因素序列解缠结研究提供了稳健且可扩展的基础。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [166] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出了一种无需训练、基于评估准则的奖励模型框架，通过两阶段方法从少量偏好数据中推断高质量准则，并利用信息论方法压缩为紧凑的核心准则集，实现了卓越的数据效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型开发成本高且缺乏可解释性，现有基于准则的方法在可扩展性和可靠性之间存在权衡，需要解决数据效率低和系统质量控制不足的问题。

Method: 两阶段方法：第一阶段使用Propose-Evaluate-Revise管道推断查询特定准则；第二阶段通过最大化信息论编码率将细粒度准则泛化为紧凑的非冗余核心准则集，形成层次化的"主题-提示"准则集。

Result: 仅使用70个偏好对（源数据的1.5%）就能实现卓越性能，甚至使较小的Qwen3-8B模型超越专门训练的对等模型，展示了框架的卓越数据效率和性能。

Conclusion: 这项工作为奖励建模开创了一条可扩展、可解释且数据高效的路径，解决了传统方法在成本、可解释性和可靠性方面的限制。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [167] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出了一种新颖的框架，用于训练具有可连续调节内部表示的大型语言模型，可在局部化（可解释、基于规则）和分布式（可泛化、高效）编码之间动态切换。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLM在可解释性和性能之间的权衡问题，为需要透明性和能力的监管领域提供支持，实现无需重新训练即可动态调整模型表示的能力。

Method: 通过组稀疏惩罚注意力机制、信息理论锚点设计、动态规则注入以及基于惩罚似然的招募标准，实现可调节的局部化程度、自适应语义块分配和分层架构适应。

Result: 建立了严格的数学结果，证明在平稳点注意力会集中在语义相关块上，提供了注意力熵和指针保真度的精确界限，分层招募机制在块级和LLM级都提供了收敛保证。

Conclusion: 该框架使实践者能够在可解释和高性能模式之间连续插值，同时在多个粒度上适应架构容量，支持需要透明性和能力的监管领域应用。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [168] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 本文提出DISC方法，利用扩散模型的迭代去噪过程提取多维特征向量，不仅能够检测OOD数据，还能对OOD类型进行分类，超越了传统基于标量的OOD检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测方法将分布偏移压缩为单一标量异常分数，无法区分不同类型的OOD数据，限制了OOD数据的适当情境化和潜在利用。

Method: DISC利用扩散模型的迭代去噪过程，在多个噪声水平上提取捕捉统计差异的丰富多维特征向量。

Result: 在图像和表格基准测试中，DISC在OOD检测方面达到或超越了最先进检测器的性能，并首次实现了对OOD类型的分类能力。

Conclusion: DISC实现了从简单二元OOD检测到更细粒度检测的转变，为OOD数据的上下文理解和潜在利用提供了新途径。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [169] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文探讨了生成视觉模型中内部表征的演变，重点分析了从GANs和VAEs到扩散模型的架构转变，提出了严格意义上的合成与广义合成的区分，并通过实验展示了扩散模型如何分散表征负担。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型内部表征的演变，理解从GANs/VAEs到扩散模型的架构转变如何影响表征过程，挑战统一内部空间的假设。

Method: 通过模型架构的详细分析和针对性的实验设置，干预分层表征，展示扩散模型如何分散表征负担。

Result: 扩散模型将表征负担分散到不同层，挑战了统一内部空间的假设，支持广义合成的概念。

Conclusion: 生成AI应被理解为专门化过程的涌现配置，而非内容的直接合成，需要重新定位对生成AI的理解框架。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [170] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的推理大语言模型，通过PRPO强化学习方法激活LLM的推理能力，在少样本和零样本场景下表现优异，甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升决策树和专用深度学习模型）虽然任务内表现优秀，但可解释性有限且跨表迁移能力弱。推理LLM具有跨任务适应性和透明推理轨迹的潜力，但尚未在表格数据中充分发挥作用。

Method: 提出TabR1模型，核心是PRPO（排列相对策略优化）强化学习方法，通过构建每个样本的多个标签保留排列，并在排列内和跨排列估计优势，将稀疏奖励转化为密集学习信号，编码列排列不变性作为结构先验。

Result: TabR1在全监督微调下达到与强基线相当的性能；在零样本设置下接近32-shot设置下强基线的性能；TabR1（8B）在各种任务中显著优于更大的LLM，相比DeepSeek-R1（685B）提升高达53.17%。

Conclusion: PRPO方法能够以有限监督激活LLM的表格预测推理能力，提升少样本和零样本性能以及可解释性，证明了推理LLM在表格预测任务中的巨大潜力。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [171] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 本文提出了锚定拟合Q迭代算法，首次为弱通信MDP的平均奖励离线强化学习建立了样本复杂度结果，无需传统方法所需的遍历性或线性假设。


<details>
  <summary>Details</summary>
Motivation: 现有关于平均奖励离线强化学习的研究较少，且依赖遍历性或线性MDP等严格假设。本文旨在为更宽松的弱通信MDP建立样本复杂度理论。

Method: 提出锚定拟合Q迭代算法，将标准拟合Q迭代与锚定机制相结合。锚定机制可解释为一种权重衰减形式，在平均奖励设置中实现有限时间分析。

Result: 成功建立了弱通信MDP平均奖励离线RL的样本复杂度结果，并将分析扩展到单轨迹生成数据集而非IID转换的设置。

Conclusion: 锚定机制是实现平均奖励离线RL有限时间分析的关键，该方法在更宽松的假设下提供了理论保证。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [172] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: S4ECG是一种基于结构化状态空间模型的新型深度学习架构，用于多时段心律失常分类，通过联合多时段预测显著提升了心律失常检测性能。


<details>
  <summary>Details</summary>
Motivation: 心电图(ECG)作为生物信号时间序列，传统方法难以同时捕捉全局趋势和局部波形特征的高时间分辨率交互作用，需要弥合全局和局部信号分析之间的差距。

Method: 引入S4ECG深度学习架构，利用结构化状态空间模型进行多时段心律失常分类，通过联合多时段预测方法。

Result: 多时段联合预测方法比单时段方法在宏观AUROC上提升1.0-11.6%，房颤特异性从0.718-0.979提升至0.967-0.998，在分布内和分布外鲁棒性方面均表现优异，最佳时间依赖窗口为10-20分钟。

Conclusion: 这项工作推动了向时间感知心律失常检测算法的范式转变，为ECG解释特别是复杂心律失常如房颤和房扑开辟了新可能性。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [173] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 本文提出DAP方法，利用扩散模型的内在代表性先验，通过Mercer核量化合成数据与真实数据在特征空间的相似性，无需重新训练即可提升蒸馏数据集的质量和代表性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式数据集蒸馏方法虽然采用扩散模型作为基础模型，但忽视了扩散模型固有的代表性先验，往往需要额外约束来提升数据质量。

Method: 提出DAP方法，将代表性形式化为合成数据与真实数据在特征空间的相似性度量，并将该先验作为指导来引导反向扩散过程。

Result: 在ImageNet-1K等大规模数据集上的实验表明，DAP在生成高保真数据集方面优于现有方法，并实现了更好的跨架构泛化性能。

Conclusion: 本文不仅建立了扩散先验与数据集蒸馏目标之间的理论联系，还提供了一个无需训练即可提升蒸馏数据集质量的实用框架。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [174] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: CrossStateECG是一个针对静息-运动交叉状态条件的心电图生物特征认证模型，通过多尺度深度卷积特征提取和注意力机制，在静息-运动交叉场景下实现高精度身份识别。


<details>
  <summary>Details</summary>
Motivation: 当前心电图生物特征研究主要集中在静息状态，而静息-运动场景下的性能下降问题尚未解决，需要开发能够适应不同生理状态的鲁棒认证模型。

Method: 结合多尺度深度卷积特征提取和注意力机制，专门针对交叉状态条件设计，确保在不同生理状态下都能实现强识别能力。

Result: 在运动-ECGID数据集上，静息-运动场景识别准确率达92.50%，运动-静息场景达94.72%，静息-静息场景达99.94%，混合-混合场景达97.85%。在ECG-ID和MIT-BIH数据集上的验证进一步证实了模型的泛化能力。

Conclusion: CrossStateECG展示了在动态现实环境中作为运动后心电图认证实用解决方案的潜力，能够有效应对不同生理状态下的身份识别挑战。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [175] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型在未见序列上的组合推理能力，使用随机层次模型(RHM)作为测试框架，发现模型通过层级专业化发展出模块化、可解释的机制来支持组合推理。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型如何通过上下文学习和技能组合在训练期间未见过的序列上表现出组合推理能力，理解其内部机制与行为表现之间的关系。

Method: 使用随机层次模型(RHM)这一概率上下文无关文法生成序列，在序列子集上训练模型，并在四种泛化条件下评估：记忆、分布内泛化、相同规则的分布外泛化、跨层迁移。

Result: 性能随任务复杂性和上下文示例数量系统性提升，分布外任务需要比分布内场景更多的示例；训练过程中出现层级专业化，与泛化性能相关；PCA和注意力模式聚类显示Transformer在专门层中发展出结构化、层次化组织的表示。

Conclusion: Transformer发展出支持组合推理的模块化、可解释机制，将内部算法结构与观察到的行为能力联系起来。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [176] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文展示了如何将基于矩阵分解的集中式差分隐私核算方法推广到去中心化学习，提出了MAFALDA-SGD算法，在隐私-效用权衡方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽然能通过点对点通信传播噪声来增强隐私保护，但实践中观察到的隐私-效用权衡往往比集中式训练更差，这可能是由于当前去中心化学习的差分隐私核算方法存在局限性。

Method: 通过推广现有的矩阵分解结果，将标准去中心化学习算法和常见信任模型统一到一个公式中，并开发了MAFALDA-SGD算法——一种基于gossip的去中心化学习算法，具有用户级相关噪声。

Result: 该方法为现有差分隐私去中心化学习算法提供了更严格的隐私核算，并在合成和真实世界图上优于现有方法。

Conclusion: 基于矩阵分解的集中式差分隐私核算方法可以成功推广到去中心化学习，为开发新的差分隐私去中心化学习算法提供了原则性方法。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [177] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个符号基准测试，用于评估大语言模型和大推理模型在类比和数学推理中的泛化性和鲁棒性。它通过增加操作数复杂度、属性范围和引入感知不确定性来扩展I-RAVEN。


<details>
  <summary>Details</summary>
Motivation: 为了评估LLMs和LRMs在类比和数学推理中的泛化能力和鲁棒性，特别是在更复杂的情境下。

Method: 扩展I-RAVEN基准测试，增加操作数复杂度、扩大属性范围，并引入感知不确定性。

Result: 与LLMs相比，LRMs在更长的推理关系和更广的属性范围上表现出更好的生产力和系统性。但LRMs在不确定性推理方面仍有显著挑战，无法有效探索多个概率结果。

Conclusion: LRMs在复杂推理任务上优于LLMs，但在处理不确定性和概率推理方面仍需改进。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [178] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 本文提出了一种基于动量的随机DC优化算法，证明了动量对于小批量情况下的收敛至关重要，并在标准假设下实现了可证明的收敛性。


<details>
  <summary>Details</summary>
Motivation: 随机DC优化在机器学习中广泛应用，但现有方法需要大批量或强噪声假设，限制了实际应用。小批量情况下的收敛性质尚未得到充分理解。

Method: 提出基于动量的随机DC优化算法，利用动量机制在标准平滑性和有界方差假设下实现收敛。

Result: 证明了没有动量时无论步长如何都可能收敛失败，而动量算法在理论上可证明收敛，并在实验中表现出色。

Conclusion: 动量是实现小批量随机DC优化收敛的关键因素，所提算法在理论和实践中均表现良好。

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [179] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将多标签学习建模为合作潜在博弈，通过好奇心奖励机制解决长尾不平衡问题，在罕见标签上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多标签学习中存在长尾不平衡问题：少数头部标签主导梯度信号，而实践中重要的许多罕见标签被忽视。

Method: 将标签空间分配给多个合作玩家，共享全局准确率收益，同时根据标签稀有性和玩家间分歧获得额外好奇心奖励，无需手动调整类别权重。

Result: 在常规基准测试和三个极端规模数据集上实现最先进性能，罕见F1提升高达+4.3%，P@3提升+1.6%，消融实验显示出现分工现象和罕见类别上更快的共识达成。

Conclusion: CD-GTMLL为多标签预测中的长尾鲁棒性提供了原则性、可扩展的解决方案。

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [180] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出Counterfactual Knowledge Distillation (CFKD)框架，通过生成多样反事实样本来解决深度学习中虚假相关性问题，无需组标签即可实现跨组平衡泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于组分布鲁棒性的方法（如DFR）存在三个关键限制：组标签通常不可得、组内样本量小导致分布覆盖不足、多个虚假相关会进一步分割数据。

Method: 通过生成多样反事实样本，使人工标注者能高效探索和修正模型决策边界，通过知识蒸馏步骤不仅重采样欠表示组，还为其丰富新数据点。

Result: 在五个数据集上验证了CFKD的有效性，特别是在低数据量且虚假相关明显的场景下表现优异，无需混淆变量标签即可扩展到多个混淆变量。

Conclusion: CFKD框架成功解决了现有方法的局限性，在多种场景下实现了跨组的平衡泛化，并通过消融研究展示了反事实解释器和教师模型对鲁棒性的影响。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [181] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种基于保形对齐的边缘-云级联机制，确保边缘预测集在用户指定概率下包含真实标签，同时显著减少向云端的卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘智能通过紧凑的设备端模型实现低延迟推理，但保证可靠性仍然具有挑战性。需要确保边缘预测集能够保持与云端模型相同的条件覆盖概率。

Method: 将边缘到云端的升级建模为多重假设检验问题，采用保形对齐来选择可以在边缘安全处理的输入。提出了CAb级联方法，对任意边缘预测集提供统计保证。

Result: 在CIFAR-100图像分类和TeleQnA问答基准测试中，CAb级联在保持目标条件覆盖的同时，显著减少了向云端的卸载，预测集大小仅有适度增加。

Conclusion: 该方法在覆盖度、延迟率和集大小之间提供了可调权衡，为边缘智能系统提供了可靠的统计保证。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [182] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba是一个用于车辆GPS轨迹学习的新方法，通过联合建模GPS和道路视角来捕捉运动模式，集成旅行目的到嵌入中，并使用知识蒸馏预训练来减少轨迹冗余，在效率和准确性上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 车辆GPS轨迹包含有价值的旅行语义（运动模式和旅行目的），但现有方法面临两个主要挑战：旅行目的与道路功能和POI相关，这些信息编码在文本地址和描述中，给建模带来沉重计算负担；真实轨迹常包含冗余点，影响计算效率和嵌入质量。

Method: 提出TrajMamba方法，包括：1) Traj-Mamba编码器，从GPS和道路视角联合建模轨迹，捕捉连续旅行行为；2) 旅行目的感知预训练，将旅行目的集成到嵌入中而不增加额外开销；3) 知识蒸馏预训练方案，通过可学习掩码生成器识别关键轨迹点，获得有效压缩轨迹嵌入。

Result: 在两个真实世界数据集和三个下游任务上的广泛实验表明，TrajMamba在效率和准确性上都优于最先进的基线方法。

Conclusion: TrajMamba能够有效且高效地学习车辆轨迹的旅行语义，解决了轨迹建模中的计算负担和冗余问题，为轨迹数据的实际应用提供了有力工具。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [183] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种扩展的解码器Transformer，通过无监督的变分方法学习随机潜在变量来条件化生成过程。


<details>
  <summary>Details</summary>
Motivation: 通过在生成过程中引入随机潜在变量，增强模型在下游任务中的表现。

Method: 扩展解码器Transformer，使用变分方法无监督学习随机潜在变量，并将这些变量作为生成过程的条件。

Result: 实验评估显示，这种条件化方法在下游任务上带来了显著改进。

Conclusion: 通过引入随机潜在变量条件化生成过程，可以有效提升Transformer模型在下游任务中的性能。

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [184] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 本文针对时间序列异常检测评估指标存在的问题，提出了可验证的属性来形式化评估要求，分析了37个常用指标的局限性，并提出了满足所有属性的LARM指标及其增强版ALARM。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的未检测异常可能导致安全关键系统的灾难性故障，现有检测方法的性能评估不清晰，因为当前指标仅捕捉任务的狭窄方面且经常产生误导性结果。

Method: 引入可验证属性来形式化时间序列异常检测评估的基本要求，建立理论框架支持原则性评估和可靠比较，分析37个广泛使用的指标，并提出LARM和ALARM指标。

Result: 分析显示大多数指标仅满足少数属性，没有一个满足所有属性，这解释了先前结果中持续存在的不一致性。LARM被证明满足所有属性，ALARM满足更严格的要求。

Conclusion: 通过引入形式化属性和新指标，解决了时间序列异常检测评估中的根本问题，为可靠的方法比较和性能评估提供了理论基础。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [185] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文分析了安全强化学习中拉格朗日乘子的最优性和稳定性，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解但需要仔细调参。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域中，拉格朗日方法被广泛用于解决约束优化问题，但乘子的选择对性能影响很大，且缺乏对其自动更新鲁棒性的实证研究。

Method: 通过分析不同任务中拉格朗日乘子的最优性和稳定性，提供λ-配置文件可视化回报与约束成本之间的权衡，并比较自动更新与固定乘子的性能。

Result: 发现λ具有高度敏感性，自动乘子更新能够恢复甚至超过最优性能，但训练过程中存在振荡行为，PID控制可缓解但需要仔细调参。

Conclusion: 拉格朗日乘子在安全强化学习中具有高度敏感性，自动更新方法有效但需要进一步研究来稳定拉格朗日方法。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [186] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 该研究探讨了通过降维进一步压缩抗菌肽设计空间，以提高优化效率、可解释性和基于理化性质组织潜空间的方法。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽是治疗细菌感染的有前景疗法，但序列空间巨大使得发现和设计困难。现有深度生成模型缺乏可解释性且对潜空间质量的量化不足。

Method: 使用变分自编码器等深度生成模型，结合降维技术压缩设计空间，并基于理化性质组织潜空间结构。

Result: 研究发现：在数据可用性下，用更相关信息组织空间时，通过降维进一步压缩潜空间是有利的；降维搜索空间更具可解释性；可以在不同标签可用性下用不同理化性质组织潜空间。

Conclusion: 通过降维压缩潜空间并结合理化性质组织，可以提高抗菌肽设计的优化效率和可解释性。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [187] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出了一种名为CEPerFed的通信高效个性化联邦学习方法，用于解决多脉冲MRI分类中的数据异构性和通信开销问题。该方法通过客户端历史风险梯度和历史平均梯度来协调本地和全局优化，并使用分层SVD策略减少通信量。


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI在阿尔茨海默病诊断等临床实践中广泛应用，但训练鲁棒模型需要来自多个医疗机构的大规模多样化数据，同时要保护隐私避免原始数据共享。联邦学习虽然可行，但面临数据异构性导致的模型收敛问题和大量参数传输带来的通信开销挑战。

Method: CEPerFed方法结合客户端历史风险梯度和历史平均梯度来协调本地和全局优化。历史风险梯度用于加权其他客户端的贡献，提高本地更新的可靠性；历史平均梯度确保本地更新与全局优化方向一致，保证在异构数据分布下的稳定收敛。同时采用分层SVD策略传输最关键信息以减少通信开销。

Result: 在五个分类任务上的实验证明了CEPerFed方法的有效性。

Conclusion: CEPerFed方法成功解决了联邦学习在多脉冲MRI分类中的数据异构性和通信开销问题，通过历史梯度协调和分层SVD策略实现了高效且稳定的模型训练。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [188] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种0.25M参数的视觉变换器变体，用于区分心源性肺水肿与非心源性肺水肿和正常肺部，在肺超声视频中实现了最佳性能（ROC-AUC 0.79-0.80），而其他模型完全失效。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部的高度视觉变异性，在肺超声视频中区分心源性肺水肿与非心源性肺水肿和正常肺部具有挑战性。这种异质性使得自动分类变得复杂，因为重叠的B线和胸膜伪影很常见。

Method: 引入ZACH-ViT（零标记自适应紧凑分层视觉变换器），去除位置嵌入和[CLS]标记，使其完全排列不变，适用于无序医学图像数据。提出ShuffleStrides数据增强（SSDA），在保持解剖有效性的同时置换探头视图序列和帧顺序。

Result: 在95名危重患者的380个肺超声视频上评估，ZACH-ViT实现了最高的验证和测试ROC-AUC（0.80和0.79），具有平衡的敏感性（0.60）和特异性（0.91），而所有竞争模型都崩溃为平凡分类。训练速度比最小ViT快1.35倍，参数少2.5倍。

Conclusion: 结果表明，将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [189] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出一种级联方法，将预训练开放词汇目标检测模型与轻量级少样本分类器结合，通过FLAME主动学习策略选择信息量最大的样本进行训练，实现快速适应特定用户需求。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测模型在遥感等专业领域的零样本性能受限，难以区分细粒度类别（如渔船和游艇），影响下游应用效果。

Method: 使用零样本模型生成高召回率目标提议，然后通过轻量级分类器进行精炼；采用FLAME主动学习策略，基于密度估计和聚类选择边界附近的不确定样本。

Result: 在遥感基准测试中持续超越最先进方法，实现快速适应（不到一分钟），显著减少标注成本。

Conclusion: 建立了一个实用且资源高效的框架，使基础模型能够快速适应特定用户需求。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [190] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出了一种语言在环框架，利用大语言模型将自然语言反馈转换为标量效用，从而在数值搜索空间上进行贝叶斯优化。该方法比传统BO基线和纯LLM优化器表现更好，特别是在反馈有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，反馈对于将复杂、细微或主观目标转化为可量化的优化目标至关重要。传统贝叶斯优化方法只能接受受限的反馈格式，且需要为每个领域特定问题定制模型。

Method: 使用大语言模型将各种类型的文本反馈转换为一致的效用信号，无需手动设计核函数即可轻松包含灵活的用户先验，同时保持贝叶斯优化的样本效率和原则性不确定性量化。

Result: 该混合方法不仅为决策者提供了更自然的接口，而且在反馈有限的情况下优于传统贝叶斯优化基线和纯LLM优化器。

Conclusion: 语言在环框架通过结合LLM的自然语言处理能力和贝叶斯优化的效率，为复杂优化问题提供了更灵活有效的解决方案。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [191] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 本文提出了三个主要贡献：1）建立了策略梯度与动态规划在MMDPs中的新联系，提出了CADP算法；2）建立了ERM Bellman算子为压缩的充要条件，提出了指数值迭代、策略迭代和线性规划算法；3）提出了用于风险规避目标的模型无关Q学习算法，证明了收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究马尔可夫决策过程中的风险规避策略优化问题，特别是在模型不确定性和风险敏感目标下的策略计算方法。

Method: 1）CADP算法通过迭代调整模型权重实现单调策略改进；2）建立ERM Bellman算子理论框架，提出多种策略计算算法；3）开发模型无关Q学习算法处理风险规避目标。

Result: 证明了CADP算法能收敛到局部最优，建立了ERM-TRC和EVaR-TRC的最优确定性策略存在性，并证明了提出的Q学习算法收敛到最优风险规避值函数。

Conclusion: 本文为风险规避的马尔可夫决策过程提供了完整的理论框架和有效的算法解决方案，包括模型相关和模型无关的方法，能够计算最优的平稳策略。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [192] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 本文提出了一种新的双层强化学习框架，通过基于真实世界性能直接调整模拟器参数来解决Sim2Real性能差距问题。


<details>
  <summary>Details</summary>
Motivation: 尽管模拟器精度和Sim2Real RL方法不断发展，但纯模拟训练的策略在真实环境中部署时仍存在显著性能下降。当前方法优化的模拟器精度和变异性指标与策略的真实世界性能不一定相关。

Method: 采用双层RL框架：内层RL在模拟中训练策略，外层RL调整模拟模型和模拟内奖励参数，以最大化模拟策略在真实世界中的性能。

Result: 推导并验证了开发能够缩小Sim2Real性能差距的双层RL算法所需的数学工具。

Conclusion: 提出的框架通过直接基于真实世界性能调整模拟器参数，有效解决了Sim2Real性能差距问题。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [193] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 本文研究如何提高黑盒大语言模型作为分类器时的操作粒度，通过分析其低基数数值输出的原因，并提出了有效方法来显著增加可用操作点数量，在不损失性能的情况下实现更精细的决策行为调整。


<details>
  <summary>Details</summary>
Motivation: 黑盒大语言模型虽然实用且易于使用，但在需要特定指标约束的应用中表现不佳，主要原因是其数值输出基数低，限制了操作点的控制能力，无法精细调整决策行为。

Method: 首先分析LLM低基数输出的原因，发现其偏向生成四舍五入但信息丰富的语言化概率；然后实验标准提示工程、不确定性估计和置信度激发技术；最后提出有效方法来显著增加可用操作点数量和多样性。

Result: 在11个数据集和3个LLM上的实验表明，所提出的方法提供了更细粒度的操作点，并实现了与基准方法相当或更好的性能。

Conclusion: 通过提出的方法可以有效提高黑盒大语言模型的操作粒度，使其在保持性能的同时能够进行更精细的决策行为调整，解决了现有技术无法有效改善操作粒度的问题。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [194] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 本文提出了一种基于可微图册的流形学习方法，通过维护可微图册数据结构支持在潜在流形上的黎曼优化，相比传统降维方法能更好地保留流形特征。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法主要进行降维到欧几里得空间，当嵌入维度接近流形维度时会丢失关键特征，而直接学习潜在流形作为可微图册的方法相对较少被探索。

Method: 实现了一个通用数据结构来维护可微图册，支持在流形上的黎曼优化，并辅以从点云数据中学习可微图册的无监督启发式方法。

Result: 实验证明该方法在选定场景下具有效率和准确性的优势，在克莱因瓶监督分类任务和造血数据的RNA速度分析中展示了更好的可解释性和鲁棒性。

Conclusion: 基于图册的方法在流形学习方面具有有效性和潜力，能够提供改进的流形特征保留和机器学习能力。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [195] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 该论文提出了一个样本级别的框架来衡量语言模型在后训练过程中的知识遗忘和反向迁移现象，通过分析1->0和0->1的转换来量化这些效应，并在多个后训练阶段、模型规模和数据规模下进行了大规模分析。


<details>
  <summary>Details</summary>
Motivation: 后训练虽然显著提升了语言模型的能力，但其对预训练知识的影响尚不明确。传统任务平均指标会混淆知识遗忘和反向迁移效应，需要更精细的测量方法来理解知识的变化。

Method: 提出样本级别的测量范式，通过统计1->0转换（后训练前正确、后训练后错误）来量化知识遗忘，0->1转换来量化反向迁移。对于选择题基准，还引入了机会调整变体来消除随机猜测的影响。

Result: 大规模分析发现：领域持续预训练导致中等程度遗忘和低到中等反向迁移；RL/SFT后训练在数学和逻辑任务上产生中等到大的反向迁移，总体遗忘程度低到中等；对指令调优模型应用RL/SFT对数据规模敏感；模型融合不能可靠缓解遗忘。

Conclusion: 该框架为大规模后训练如何改变预训练知识提供了实用的衡量标准，有助于推进通用人工智能系统的发展。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [196] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 本文提出了新的推理时计算扩展方法，用于流匹配模型，保持线性插值器的优势，并在图像生成和蛋白质生成任务中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前流匹配模型在推理时计算扩展方法研究不足，现有方法牺牲了流匹配的高效直线采样特性，且仅应用于视觉任务。

Method: 开发了新的推理时扩展程序，在采样过程中保持线性插值器，不改变原始流匹配的采样特性。

Result: 在图像生成和无条件蛋白质生成任务中，样本质量随着推理计算增加而持续提升，证明了流匹配推理时扩展可应用于科学领域。

Conclusion: 提出的方法成功扩展了流匹配的推理时计算能力，同时保持了其高效采样特性，为科学领域的生成任务提供了新工具。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [197] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为GUM的无偏低秩优化方法，通过层间采样技术解决现有低秩投影方法的偏差问题，在保持内存效率的同时实现与完整参数训练相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有内存高效优化方法（如GaLore）虽然能减少内存使用，但由于低秩投影引入的偏差，缺乏收敛保证，导致性能与完整参数训练存在差距。

Method: 基于GaLore机制和Muon算法，采用层间采样技术来消除低秩投影的偏差，开发了GUM方法。

Result: 理论证明GUM方法保持了Muon算法的收敛保证，同时保持了低秩技术的内存效率。实证实验显示在LLM微调和预训练中优于GaLore，甚至超过完整参数训练。

Conclusion: GUM方法通过更均匀的层内知识分布，实现了模型参数空间的更高效利用和更好的记忆能力，为内存高效训练提供了有理论保证的解决方案。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [198] [Traffic Prioritization Mechanisms for Mission and Time Critical Applications in Industrial Internet of Things](https://arxiv.org/abs/2510.17009)
*Anwar Ahmed Khan,Shama Siddiqui,Indrakshi Dey*

Main category: cs.NI

TL;DR: 本文评估了工业物联网中两种MAC技术——时隙窃取和分组分片的性能，通过模拟对比SS-MAC和FROG-MAC协议，发现FROG-MAC因减少紧急流量等待时间而表现更优。


<details>
  <summary>Details</summary>
Motivation: 工业物联网环境中每个节点生成不同类型数据，具有多样化服务需求，MAC协议对确保高效传输至关重要。需要评估现有MAC技术在工业环境中的适用性。

Method: 选择SS-MAC和FROG-MAC作为时隙窃取和分组分片技术的代表协议，使用Contiki进行真实模拟，比较延迟和丢包率性能。

Result: FROG-MAC在延迟和丢包率方面优于SS-MAC，主要原因是减少了紧急流量的等待时间。

Conclusion: 简单的分组分片方案可用于工业环境中异构流量的高效调度，FROG-MAC技术更适合工业物联网应用。

Abstract: Industrial Internet of Things (IIoT) promises to revolutionize industrial
operations and productions through utilizing Machine-to-Machine (M2M)
communications. Since each node in such environments generates various types of
data with diverse service requirements, MAC protocol holds crucial importance
to ensure efficient delivery. In this context, simple to complex MAC schemes
are found in literature. This paper focuses on evaluating the performance of
two major techniques "slot stealing" and "packet fragmentation" for the IIoT;
representative protocols SS-MAC and FROG-MAC have been chosen from each
category respectively. We conducted realistic simulations for the two protocols
using Contiki. Delay and packet loss comparison for SS-MAC and FROG-MAC
indicates the superiority of FROG-MAC due to reduction in the waiting time for
urgent traffic. Thus, a simple fragmentation scheme could be deployed for
efficient scheduling of heterogenous traffic in the industrial environments.

</details>


### [199] [Enhancing 5G V2X Mode 2 for Sporadic Traffic](https://arxiv.org/abs/2510.17395)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Aleksei Shashin,Evgeny Khorov*

Main category: cs.NI

TL;DR: 本文分析了5G V2X技术中Mode 2信道接入方法在偶发流量场景下的性能，并提出了改进方案，可将系统容量提升高达40%且对复杂度影响较小。


<details>
  <summary>Details</summary>
Motivation: 道路安全和自动驾驶应用需要车辆与车辆、车辆与基础设施之间及时可靠的数据传输。5G V2X技术为此提供了支持，但在偶发流量场景下（如车辆检测到危险情况时随机生成数据包），对延迟和可靠性有严格要求，需要优化Mode 2信道接入方法的性能。

Method: 针对偶发流量场景，分析Mode 2（用户自主选择传输资源）的性能，并提出多种改进方法。通过仿真验证这些方法的有效性。

Result: 仿真结果表明，所提出的改进方法可以将系统容量提升高达40%，同时对系统复杂度的影响较小。

Conclusion: 在5G V2X的偶发流量场景中，通过优化Mode 2信道接入方法，可以显著提升系统性能，满足严格的延迟和可靠性要求，为道路安全和自动驾驶应用提供更好的支持。

Abstract: The emerging road safety and autonomous vehicle applications require timely
and reliable data delivery between vehicles and between vehicles and
infrastructure. To satisfy this demand, 3GPP develops a 5G
Vehicle-to-Everything (V2X) technology. Depending on the served traffic type,
5G V2X specifications propose two channel access methods: (i) Mode 1, according
to which a base station allocates resources to users, and (ii) Mode 2,
according to which users autonomously select resources for their transmissions.
In the paper, we consider a scenario with sporadic traffic, e.g., a vehicle
generates a packet at a random time moment when it detects a dangerous
situation, which imposes strict requirements on delay and reliability. To
satisfy strict delay requirements, vehicles use Mode 2. We analyze the
performance of Mode 2 for sporadic traffic and propose several approaches to
improve it. Simulation results show that the proposed approaches can increase
the system capacity by up to 40% with a low impact on complexity.

</details>


### [200] [Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?](https://arxiv.org/abs/2510.17410)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Pavel Savlukovich,Evgeny Khorov*

Main category: cs.NI

TL;DR: 本文分析了5G V2X中反馈信道对系统容量的影响，发现在不同场景下反馈信道可能使系统容量翻倍或减半，并讨论了自适应参数选择方法。


<details>
  <summary>Details</summary>
Motivation: 5G V2X引入了组播和单播通信以及反馈信道来提高传输可靠性，但反馈信道会占用数据信道资源，需要研究其对系统容量的实际影响。

Method: 使用NS-3进行广泛仿真，分析在包含车队（组播流量）和周围车辆（广播流量）的场景中，反馈信道的使用如何影响整体系统容量。

Result: 结果表明，根据车队大小、组播和广播流量强度及其服务质量要求，反馈信道在某些情况下可显著提高系统容量（最高达2倍），而在其他情况下几乎使系统容量减半。

Conclusion: 反馈信道对系统容量的影响具有双重性，需要根据具体场景自适应选择反馈信道参数以优化性能。

Abstract: 5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to
support inter-vehicle communication. In contrast to 4G V2X which allows only
broadcast communication, 5G V2X enables groupcast and unicast communication.
Such types of communication are needed for new V2X scenarios: platooning,
extended sensors, remote driving, etc. To improve the data transmission
reliability and assist in the selection of the transmission parameters in these
scenarios, 5G V2X introduces a feedback channel that allows receivers to send
acknowledgments in response to data packets. However, some part of the overall
resource shall be allocated for the feedback channel, which reduces the amount
of channel resources available for data transmission. In this paper, we
consider a scenario with a platoon, which generates groupcast traffic, and
surrounding vehicles, which generate legacy broadcast traffic. Using extensive
simulations in NS-3, we analyze how the usage of the feedback channel
influences the overall system capacity. Our results show that depending on the
platoon size, groupcast, and broadcast traffic intensities, and their quality
of service requirements, the usage of the feedback channel can in some cases
significantly increase the system capacity (up to 2x), while in other cases it
almost halves the system capacity. We explain the reasons for such effects and
discuss how to adaptively select the feedback channel parameters.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [201] [Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience](https://arxiv.org/abs/2510.16034)
*Bo Li,Junwei Ma,Kai Yin,Yiming Xiao,Chia-Wei Hsu,Ali Mostafavi*

Main category: cs.MA

TL;DR: 本文提出了Disaster Copilot，一个多智能体AI系统，旨在通过统一专业AI工具来克服灾害管理中的系统性挑战，包括数据碎片化、技术孤岛和机构记忆流失等问题。


<details>
  <summary>Details</summary>
Motivation: 当前灾害管理面临数据流碎片化、技术孤岛、资源限制和机构记忆流失等挑战，这些因素共同阻碍了及时有效的决策制定。

Method: 采用多智能体AI系统架构，使用中央协调器协调专门领域的子智能体，整合多模态数据，提供实时操作全景，并支持资源受限环境下的设备端协调。

Result: 系统能够将灾害数字孪生从被动模型推进到主动智能环境，捕获机构知识以减少人员流动的影响。

Conclusion: Disaster Copilot提供了一个变革性愿景，通过培养集体人机智能来构建更具适应性、数据驱动和韧性的社区。

Abstract: The escalating frequency and severity of disasters routinely overwhelm
traditional response capabilities, exposing critical vulnerability in disaster
management. Current practices are hindered by fragmented data streams, siloed
technologies, resource constraints, and the erosion of institutional memory,
which collectively impede timely and effective decision making. This study
introduces Disaster Copilot, a vision for a multi-agent artificial intelligence
system designed to overcome these systemic challenges by unifying specialized
AI tools within a collaborative framework. The proposed architecture utilizes a
central orchestrator to coordinate diverse sub-agents, each specializing in
critical domains such as predictive risk analytics, situational awareness, and
impact assessment. By integrating multi-modal data, the system delivers a
holistic, real-time operational picture and serve as the essential AI backbone
required to advance Disaster Digital Twins from passive models to active,
intelligent environments. Furthermore, it ensures functionality in
resource-limited environments through on-device orchestration and incorporates
mechanisms to capture institutional knowledge, mitigating the impact of staff
turnover. We detail the system architecture and propose a three-phased roadmap
emphasizing the parallel growth of technology, organizational capacity, and
human-AI teaming. Disaster Copilot offers a transformative vision, fostering
collective human-machine intelligence to build more adaptive, data-driven and
resilient communities.

</details>


### [202] [Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards](https://arxiv.org/abs/2510.16187)
*Rupal Nigam,Niket Parikh,Hamid Osooli,Mikihisa Yuasa,Jacob Heglund,Huy T. Tran*

Main category: cs.MA

TL;DR: 该论文提出GPAT算法，用于在多智能体系统中实现零样本的临时组队，通过利用所有预训练策略进行知识转移，无需重新训练即可与新队友协作。


<details>
  <summary>Details</summary>
Motivation: 现实世界多智能体系统需要临时组队能力，智能体必须与之前未见过的队友在零样本情况下协调完成任务。现有方法要么基于推断的队友模型选择预训练策略，要么预训练单一鲁棒策略，存在局限性。

Method: 将问题形式化为临时多智能体马尔可夫决策过程，提出GPAT算法，结合广义策略改进和差异奖励两个关键思想，实现不同团队间的有效知识转移。

Result: 在三个模拟环境（合作觅食、捕食者-猎物、Overcooked）中成功实现零样本转移到新团队，并在真实多机器人环境中验证了算法有效性。

Conclusion: GPAT算法能够成功实现多智能体系统中的零样本临时组队，为现实世界应用提供了有效的解决方案。

Abstract: Real-world multi-agent systems may require ad hoc teaming, where an agent
must coordinate with other previously unseen teammates to solve a task in a
zero-shot manner. Prior work often either selects a pretrained policy based on
an inferred model of the new teammates or pretrains a single policy that is
robust to potential teammates. Instead, we propose to leverage all pretrained
policies in a zero-shot transfer setting. We formalize this problem as an ad
hoc multi-agent Markov decision process and present a solution that uses two
key ideas, generalized policy improvement and difference rewards, for efficient
and effective knowledge transfer between different teams. We empirically
demonstrate that our algorithm, Generalized Policy improvement for Ad hoc
Teaming (GPAT), successfully enables zero-shot transfer to new teams in three
simulated environments: cooperative foraging, predator-prey, and Overcooked. We
also demonstrate our algorithm in a real-world multi-robot setting.

</details>


### [203] [Heterogeneous Multi-Agent Task-Assignment with Uncertain Execution Times and Preferences](https://arxiv.org/abs/2510.16221)
*Qinshuang Wei,Vaibhav Srivastava,Vijay Gupta*

Main category: cs.MA

TL;DR: 本文研究了多智能体环境中的顺序任务分配问题，提出了一个基于赌博机算法的解决方案，用于处理具有异构任务偏好和能力的不确定环境下的任务分配。


<details>
  <summary>Details</summary>
Motivation: 虽然单智能体的顺序任务分配已被广泛研究，但在多智能体环境中，由于智能体具有异构的任务偏好或能力，这类问题仍未得到充分研究。

Method: 提出了一个赌博机算法，该算法依赖于重复求解最优任务分配问题。分析了在能够精确求解最优任务分配和只能近似求解两种情况下的可达到遗憾。

Result: 算法能够在任务奖励、执行时间和资源消耗均为随机且分布未知的情况下，最大化团队在问题时间范围内的总期望奖励。

Conclusion: 该研究为多智能体异构环境下的任务分配问题提供了一个有效的解决方案，并分析了在不同求解精度下的性能保证。

Abstract: While sequential task assignment for a single agent has been widely studied,
such problems in a multi-agent setting, where the agents have heterogeneous
task preferences or capabilities, remain less well-characterized. We study a
multi-agent task assignment problem where a central planner assigns recurring
tasks to multiple members of a team over a finite time horizon. For any given
task, the members have heterogeneous capabilities in terms of task completion
times, task resource consumption (which can model variables such as energy or
attention), and preferences in terms of the rewards they collect upon task
completion. We assume that the reward, execution time, and resource consumption
for each member to complete any task are stochastic with unknown distributions.
The goal of the planner is to maximize the total expected reward that the team
receives over the problem horizon while ensuring that the resource consumption
required for any assigned task is within the capability of the agent. We
propose and analyze a bandit algorithm for this problem. Since the bandit
algorithm relies on solving an optimal task assignment problem repeatedly, we
analyze the achievable regret in two cases: when we can solve the optimal task
assignment exactly and when we can solve it only approximately.

</details>


### [204] [DiRAC - Distributed Robot Awareness and Consensus](https://arxiv.org/abs/2510.16850)
*Uday Gopan,Manjari Kulkarni,Lakshasri S,Kashish Mittal,Sriram Radhakrishna,Aditya Naskar,Rameshwar DL*

Main category: cs.MA

TL;DR: DiRAC是一个可扩展的分布式框架，用于大型机器人集群的高效任务分配和路径规划，采用分区架构和领导者选举机制，通过力基分散式规划器实现实时碰撞解决。


<details>
  <summary>Details</summary>
Motivation: 解决大型机器人集群中任务分配和路径规划的效率和可扩展性问题，为工业物流领域的大规模部署提供技术基础。

Method: 采用分区架构和动态选举领导者机制，结合tick同步共识协议确保强一致性和确定性结果，使用力基分散式规划器进行实时碰撞解决。

Result: 在ROS 2中间件中通过初步仿真验证，展示了架构可扩展性和模块化效率，在模拟仓库环境中表现良好。

Conclusion: DiRAC为大规模工业和物流领域的实际部署奠定了技术基础，证明了其在大型机器人集群管理中的可行性和有效性。

Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task
assignment and path planning in very large robotic swarms. It introduces a
novel zone-partitioned architecture with dynamically elected leaders and a
tick-synchronized consensus protocol that yields strong consistency and
deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a
force-based decentralized planner for real-time collision resolution. Validated
within ROS 2 middleware through preliminary simulation, DiRAC demonstrates
architectural scalability and modular efficiency in simulated warehouse
environments, laying the groundwork for real-world deployment in large-scale
industrial and logistics domains.

</details>


### [205] [Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents](https://arxiv.org/abs/2510.16978)
*Dheeraj Chintapalli,Rikhil Tanugula,Sunkalp Chandra*

Main category: cs.MA

TL;DR: Lark是一个生物启发的决策框架，将LLM驱动的推理与进化式、利益相关者感知的多智能体系统相结合，通过四种机制解决冗长性和利益相关者权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM决策中的冗长问题和利益相关者权衡挑战，开发一个实用、计算成本感知的神经进化循环系统。

Method: 集成四种机制：可塑性调整、复制与成熟、基于影响力的Borda评分排名选择投票、以及基于token惩罚的计算成本感知。系统迭代生成策略，应用调整，模拟评估，聚合偏好，选择候选方案。

Result: 在30轮控制评估中，Lark Full平均排名2.55，平均综合得分29.4/50，80%轮次进入前三名，成本竞争力强（每任务$0.016）。消融实验显示所有四种机制都有显著贡献。

Conclusion: Lark是一个实用的计算成本感知神经进化循环，能够扩展利益相关者对齐的策略生成，并通过每步指标使权衡透明化。该工作提供了概念验证结果，邀请社区反馈以扩展至真实世界验证研究。

Abstract: We present Lark, a biologically inspired decision-making framework that
couples LLM-driven reasoning with an evolutionary, stakeholder-aware
Multi-Agent System (MAS). To address verbosity and stakeholder trade-offs, we
integrate four mechanisms: (i) plasticity, which applies concise adjustments to
candidate solutions; (ii) duplication and maturation, which copy
high-performing candidates and specialize them into new modules; (iii)
ranked-choice stakeholder aggregation using influence-weighted Borda scoring;
and (iv) compute awareness via token-based penalties that reward brevity. The
system iteratively proposes diverse strategies, applies plasticity tweaks,
simulates stakeholder evaluations, aggregates preferences, selects top
candidates, and performs duplication/maturation while factoring compute cost
into final scores. In a controlled evaluation over 30 rounds comparing 14
systems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a
mean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80%
of rounds while remaining cost competitive with leading commercial models
($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms
contribute significantly as ablating duplication/maturation yields the largest
deficit ({\Delta}Score = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by
plasticity ({\Delta}Score = 3.4, d_z = 1.86), ranked-choice voting
({\Delta}Score = 2.4, d_z = 1.20), and token penalties ({\Delta}Score = 2.2,
d_z = 1.63). Rather than a formal Markov Decision Process with constrained
optimization, Lark is a practical, compute-aware neuroevolutionary loop that
scales stakeholder-aligned strategy generation and makes trade-offs transparent
through per-step metrics. Our work presents proof-of-concept findings and
invites community feedback as we expand toward real-world validation studies.

</details>
