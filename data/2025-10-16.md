<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 62]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.IT](#cs.IT) [Total: 7]
- [cs.CR](#cs.CR) [Total: 11]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment](https://arxiv.org/abs/2510.12927)
*Haolin Li,Hoda Bidkhori*

Main category: cs.LG

TL;DR: FedGTEA是一个联邦类增量学习框架，通过高斯任务嵌入和对齐来捕获任务特定知识并处理模型不确定性，具有可扩展性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中任务增量学习的问题，特别是在统计异构性和数据不确定性下的挑战，同时保护任务级隐私。

Method: 客户端使用Cardinality-Agnostic Task Encoder (CATE)生成高斯分布的任务嵌入，服务器端使用2-Wasserstein距离测量任务间差距并制定Wasserstein损失来增强任务分离。

Result: 在流行数据集上的广泛实证评估表明，FedGTEA实现了优越的分类性能，显著减轻了遗忘问题，并持续优于现有强基线方法。

Conclusion: FedGTEA通过概率化表述不仅增强了表示学习，还通过避免直接传输潜在嵌入来保护任务级隐私，符合联邦学习的隐私约束。

Abstract: We introduce a novel framework for Federated Class Incremental Learning,
called Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA is
designed to capture task-specific knowledge and model uncertainty in a scalable
and communication-efficient manner. At the client side, the
Cardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed task
embeddings that encode task knowledge, address statistical heterogeneity, and
quantify data uncertainty. Importantly, CATE maintains a fixed parameter size
regardless of the number of tasks, which ensures scalability across long task
sequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance to
measure inter-task gaps between Gaussian embeddings. We formulate the
Wasserstein loss to enforce inter-task separation. This probabilistic
formulation not only enhances representation learning but also preserves
task-level privacy by avoiding the direct transmission of latent embeddings,
aligning with the privacy constraints in federated learning. Extensive
empirical evaluations on popular datasets demonstrate that FedGTEA achieves
superior classification performance and significantly mitigates forgetting,
consistently outperforming strong existing baselines.

</details>


### [2] [Learning at the Speed of Physics: Equilibrium Propagation on Oscillator Ising Machines](https://arxiv.org/abs/2510.12934)
*Alex Gower*

Main category: cs.LG

TL;DR: 该论文展示了振荡器伊辛机（OIMs）作为物理系统能够通过能量下降过程加速机器学习，结合平衡传播（EP）实现局部学习规则，在MNIST和Fashion-MNIST数据集上达到竞争性准确率，并保持对硬件约束的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 利用自然执行能量下降的物理系统来加速机器学习，特别是解决传统处理器在处理能量基模型（EBMs）时的瓶颈问题。

Method: 使用振荡器伊辛机（OIMs）结合平衡传播（EP）方法，利用OIMs的GHz频率动态特性模拟能量基模型的优化和梯度下降，同时内在噪声对应朗之万动力学。

Result: 在MNIST数据集上达到97.2±0.1%的准确率，在Fashion-MNIST数据集上达到88.0±0.1%的准确率，并且在参数量化和相位噪声等实际硬件约束下保持鲁棒性。

Conclusion: OIMs为神经形态学习提供了快速、节能的物理基底，表明能量基模型可以在直接执行其优化的物理硬件上实现实际应用。

Abstract: Physical systems that naturally perform energy descent offer a direct route
to accelerating machine learning. Oscillator Ising Machines (OIMs) exemplify
this idea: their GHz-frequency dynamics mirror both the optimization of
energy-based models (EBMs) and gradient descent on loss landscapes, while
intrinsic noise corresponds to Langevin dynamics - supporting sampling as well
as optimization. Equilibrium Propagation (EP) unifies these processes into
descent on a single total energy landscape, enabling local learning rules
without global backpropagation. We show that EP on OIMs achieves competitive
accuracy ($\sim 97.2 \pm 0.1 \%$ on MNIST, $\sim 88.0 \pm 0.1 \%$ on
Fashion-MNIST), while maintaining robustness under realistic hardware
constraints such as parameter quantization and phase noise. These results
establish OIMs as a fast, energy-efficient substrate for neuromorphic learning,
and suggest that EBMs - often bottlenecked by conventional processors - may
find practical realization on physical hardware whose dynamics directly perform
their optimization.

</details>


### [3] [An Investigation of Memorization Risk in Healthcare Foundation Models](https://arxiv.org/abs/2510.12950)
*Sana Tonekaboni,Lena Stempfle,Adibvafa Fallahpour,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 本文提出了一套黑盒评估测试方法，用于评估基于结构化电子健康记录训练的基础模型中的隐私相关记忆风险，包括嵌入层和生成层的记忆探测方法。


<details>
  <summary>Details</summary>
Motivation: 基于大规模去标识化电子健康记录训练的基础模型在临床应用中具有前景，但其记忆患者信息的能力引发了重要的隐私担忧，特别是对弱势群体的潜在隐私风险。

Method: 开发了黑盒评估测试套件，包含在嵌入层和生成层探测记忆的方法，旨在区分模型泛化和有害记忆，并发布了开源工具包。

Result: 在公开可用的电子健康记录基础模型上验证了该方法，提供了可复现和协作的隐私评估工具。

Conclusion: 该框架有助于评估医疗AI中的隐私记忆风险，特别关注患者隐私保护，尤其是弱势群体的隐私安全。

Abstract: Foundation models trained on large-scale de-identified electronic health
records (EHRs) hold promise for clinical applications. However, their capacity
to memorize patient information raises important privacy concerns. In this
work, we introduce a suite of black-box evaluation tests to assess
privacy-related memorization risks in foundation models trained on structured
EHR data. Our framework includes methods for probing memorization at both the
embedding and generative levels, and aims to distinguish between model
generalization and harmful memorization in clinically relevant settings. We
contextualize memorization in terms of its potential to compromise patient
privacy, particularly for vulnerable subgroups. We validate our approach on a
publicly available EHR foundation model and release an open-source toolkit to
facilitate reproducible and collaborative privacy assessments in healthcare AI.

</details>


### [4] [Balancing Performance and Reject Inclusion: A Novel Confident Inlier Extrapolation Framework for Credit Scoring](https://arxiv.org/abs/2510.12967)
*Athyrson Machado Ribeiro,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: 提出了一个名为CI-EX的新颖拒绝推断框架，通过异常检测模型识别被拒绝客户的分布，并基于监督分类模型的概率为最接近接受人群分布的拒绝个体分配标签，在保持AUC竞争力的同时，在拒绝推断特定指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统拒绝推断方法假设被拒绝客户的行为可以从接受客户中推断，但忽略了两个群体之间可能存在的分布差异，导致盲目推断问题。

Method: 采用置信内点外推框架(CI-EX)，迭代使用异常检测模型识别被拒绝客户样本分布，基于监督分类模型的概率为最接近接受人群分布的拒绝个体分配标签。

Result: 在两个大型真实世界信用数据集上的实验表明，CI-EX在拒绝推断特定指标（如Kickout和Area under the Kickout）上持续优于现有方法，同时在大多数实验中保持AUC的竞争力。

Conclusion: 拒绝推断方法通常涉及AUC和拒绝推断特定指标之间的权衡，但提出的CI-EX框架在拒绝推断特定指标上表现优异，同时保持AUC的竞争力。

Abstract: Reject Inference (RI) methods aim to address sample bias by inferring missing
repayment data for rejected credit applicants. Traditional approaches often
assume that the behavior of rejected clients can be extrapolated from accepted
clients, despite potential distributional differences between the two
populations. To mitigate this blind extrapolation, we propose a novel Confident
Inlier Extrapolation framework (CI-EX). CI-EX iteratively identifies the
distribution of rejected client samples using an outlier detection model and
assigns labels to rejected individuals closest to the distribution of the
accepted population based on probabilities derived from a supervised
classification model. The effectiveness of our proposed framework is validated
through experiments on two large real-world credit datasets. Performance is
evaluated using the Area Under the Curve (AUC) as well as RI-specific metrics
such as Kickout and a novel metric introduced in this work, denoted as Area
under the Kickout. Our findings reveal that RI methods, including the proposed
framework, generally involve a trade-off between AUC and RI-specific metrics.
However, the proposed CI-EX framework consistently outperforms existing RI
models from the credit literature in terms of RI-specific metrics while
maintaining competitive performance in AUC across most experiments.

</details>


### [5] [A Connection Between Score Matching and Local Intrinsic Dimension](https://arxiv.org/abs/2510.12975)
*Eric Yeats,Aaron Jacobson,Darryl Hannan,Yiran Jia,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.LG

TL;DR: 本文提出使用去噪分数匹配损失作为局部内在维度（LID）的估计器，避免了传统方法需要多次前向传播或梯度计算的问题，在计算和内存受限场景下具有更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有量化高维复杂数据LID的方法需要多次前向传播或梯度计算，限制了在计算和内存受限场景下的应用，需要寻找更高效的LID估计方法。

Method: 证明LID是去噪分数匹配损失的下界，从而将去噪分数匹配损失作为LID估计器，并分析隐式分数匹配损失与FLIPD估计器的关系。

Result: 在流形基准测试和Stable Diffusion 3.5上的实验表明，去噪分数匹配损失在准确性和内存占用方面具有竞争优势，随着问题规模和量化级别的增加表现更优。

Conclusion: 去噪分数匹配损失是一种高效、可扩展的LID估计器，在计算和内存受限环境下具有实用价值。

Abstract: The local intrinsic dimension (LID) of data is a fundamental quantity in
signal processing and learning theory, but quantifying the LID of
high-dimensional, complex data has been a historically challenging task. Recent
works have discovered that diffusion models capture the LID of data through the
spectra of their score estimates and through the rate of change of their
density estimates under various noise perturbations. While these methods can
accurately quantify LID, they require either many forward passes of the
diffusion model or use of gradient computation, limiting their applicability in
compute- and memory-constrained scenarios.
  We show that the LID is a lower bound on the denoising score matching loss,
motivating use of the denoising score matching loss as a LID estimator.
Moreover, we show that the equivalent implicit score matching loss also
approximates LID via the normal dimension and is closely related to a recent
LID estimator, FLIPD. Our experiments on a manifold benchmark and with Stable
Diffusion 3.5 indicate that the denoising score matching loss is a highly
competitive and scalable LID estimator, achieving superior accuracy and memory
footprint under increasing problem size and quantization level.

</details>


### [6] [Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile Apps](https://arxiv.org/abs/2510.13405)
*Chen Gong,Yan Zhuang,Zhenzhe Zheng,Yiliu Chen,Sheng Wang,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: AdaLog是一个轻量级自适应系统，通过消除特征级冗余数据和优化异构行为存储，显著减少移动应用中用户行为日志的存储空间，同时保持模型推理精度和延迟。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在移动应用中日益普及，但记录用户行为数据带来了巨大的存储成本，导致系统响应性降低和应用卸载率增加。现有工业实践存在两个关键低效问题：跨特征和模型的冗余日志记录，以及异构行为属性描述导致的稀疏存储。

Method: AdaLog首先将特征级冗余数据消除建模为超图中的最大加权匹配问题，并提出分层算法用于设备端部署。然后采用虚拟哈希属性设计将异构行为分布到少量物理密集存储的日志文件中。最后设计增量更新机制以适应动态用户行为模式，最小化I/O操作。

Result: 在真实用户数据上的评估显示，AdaLog将行为日志大小减少了19%到44%，系统开销极小（仅2秒延迟和15MB内存使用）。

Conclusion: AdaLog为设备端机器学习的更广泛采用提供了更高效的数据基础，有效解决了移动应用中用户行为日志的存储瓶颈问题。

Abstract: Machine learning (ML) models are increasingly integrated into modern mobile
apps to enable personalized and intelligent services. These models typically
rely on rich input features derived from historical user behaviors to capture
user intents. However, as ML-driven services become more prevalent, recording
necessary user behavior data imposes substantial storage cost on mobile apps,
leading to lower system responsiveness and more app uninstalls. To address this
storage bottleneck, we present AdaLog, a lightweight and adaptive system
designed to improve the storage efficiency of user behavior log in ML-embedded
mobile apps, without compromising model inference accuracy or latency. We
identify two key inefficiencies in current industrial practices of user
behavior log: (i) redundant logging of overlapping behavior data across
different features and models, and (ii) sparse storage caused by storing
behaviors with heterogeneous attribute descriptions in a single log file. To
solve these issues, AdaLog first formulates the elimination of feature-level
redundant data as a maximum weighted matching problem in hypergraphs, and
proposes a hierarchical algorithm for efficient on-device deployment. Then,
AdaLog employs a virtually hashed attribute design to distribute heterogeneous
behaviors into a few log files with physically dense storage. Finally, to
ensure scalability to dynamic user behavior patterns, AdaLog designs an
incremental update mechanism to minimize the I/O operations needed for adapting
outdated behavior log. We implement a prototype of AdaLog and deploy it into
popular mobile apps in collaboration with our industry partner. Evaluations on
real-world user data show that AdaLog reduces behavior log size by 19% to 44%
with minimal system overhead (only 2 seconds latency and 15 MB memory usage),
providing a more efficient data foundation for broader adoption of on-device
ML.

</details>


### [7] [Reference-Specific Unlearning Metrics Can Hide the Truth: A Reality Check](https://arxiv.org/abs/2510.12981)
*Sungjun Cho,Dasol Hwang,Frederic Sala,Sangheum Hwang,Kyunghyun Cho,Sungmin Cha*

Main category: cs.LG

TL;DR: 提出FADE度量标准，通过比较未学习模型和参考模型在生成样本上的双向似然分配来评估分布相似性，解决了现有遗忘度量依赖特定参考的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型的遗忘度量基于参考响应或分类器输出，而非评估核心目标：未学习模型是否与从未见过不需要数据的模型行为无法区分。这种参考特定方法存在系统性盲点。

Method: 提出功能对齐分布等价性(FADE)度量，通过比较未学习模型和参考模型在生成样本上的双向似然分配来测量分布相似性，捕获整个输出分布的功能对齐。

Result: 在TOFU和UnlearnCanvas基准上的实验显示，传统度量接近最优的方法未能实现分布等价性，许多方法在遗忘后与黄金标准距离更远。

Conclusion: 当前评估实践存在根本性差距，FADE为开发和评估真正有效的遗忘方法提供了更稳健的基础。

Abstract: Current unlearning metrics for generative models evaluate success based on
reference responses or classifier outputs rather than assessing the core
objective: whether the unlearned model behaves indistinguishably from a model
that never saw the unwanted data. This reference-specific approach creates
systematic blind spots, allowing models to appear successful while retaining
unwanted knowledge accessible through alternative prompts or attacks. We
address these limitations by proposing Functional Alignment for Distributional
Equivalence (FADE), a novel metric that measures distributional similarity
between unlearned and reference models by comparing bidirectional likelihood
assignments over generated samples. Unlike existing approaches that rely on
predetermined references, FADE captures functional alignment across the entire
output distribution, providing a principled assessment of genuine unlearning.
Our experiments on the TOFU benchmark for LLM unlearning and the UnlearnCanvas
benchmark for text-to-image diffusion model unlearning reveal that methods
achieving near-optimal scores on traditional metrics fail to achieve
distributional equivalence, with many becoming more distant from the gold
standard than before unlearning. These findings expose fundamental gaps in
current evaluation practices and demonstrate that FADE provides a more robust
foundation for developing and assessing truly effective unlearning methods.

</details>


### [8] [Max It or Miss It: Benchmarking LLM On Solving Extremal Problems](https://arxiv.org/abs/2510.12997)
*Binxin Gao,Jingjun Han*

Main category: cs.LG

TL;DR: 该论文引入了ExtremBench基准数据集，用于评估LLMs在解决数学极值问题方面的推理能力，发现现有数学基准无法全面捕捉LLMs的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs推理能力的具体来源和机制，特别是优化推理能力，这对于规划、控制、资源分配和提示搜索等关键应用至关重要。

Method: 创建ExtremBench基准数据集，包含93个标准化极值问题，源自中国数学奥林匹克竞赛的不等式练习，并对多个先进开源模型家族进行广泛评估。

Result: 发现LLMs的极值求解推理能力与当前数学基准（如AIME25和MATH-500）并不一致，有些模型在一般数学推理方面表现良好但在极值求解方面较差，反之亦然。

Conclusion: 现有评估实践存在关键差距，现有基准可能无法全面捕捉数学推理能力的全部范围。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) with remarkable
reasoning capabilities, particularly in mathematical domains, through
intermediate chain-of-thought (CoT) reasoning before generating final answers.
However, the specific sources and mechanisms underlying these reasoning
capabilities remain insufficiently understood. Optimization reasoning, i.e.
finding extrema under constraints, represents a fundamental abstraction that
underpins critical applications in planning, control, resource allocation, and
prompt search. To systematically evaluate this capability, we introduce
ExtremBench, a benchmark dataset for solving mathematical extremal problems,
curated from inequality exercises used for Chinese Mathematical Olympiad and
transformed into $93$ standardized extrema-finding problems. We conduct
extensive evaluations across various state-of-the-art open-source model
families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that
LLMs' extremal-solving reasoning capabilities do not always align with those of
current mathematical benchmarks such as AIME25 and MATH-500, with some models
showing strong general mathematical reasoning but poor extremal-solving skills,
and vice versa. This discrepancy highlights a critical gap in current
evaluation practices and suggests that existing benchmarks may not
comprehensively capture the full spectrum of mathematical reasoning abilities.

</details>


### [9] [Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment](https://arxiv.org/abs/2510.13023)
*Joshua R. Tempelman,Adam J. Wachtor,Eric B. Flynn*

Main category: cs.LG

TL;DR: 该论文提出了一种用于自动化超声焊缝检测的端到端机器学习工作流，通过降阶建模、扩散分布对齐和U-Net分割反演来解决数据稀缺和信号噪声问题。


<details>
  <summary>Details</summary>
Motivation: 自动化超声焊缝检测面临训练数据有限和工业环境信号噪声的挑战，现有方法难以在真实工业环境中实现端到端的检测工作流。

Method: 使用基于兰姆波理论的降阶Helmholtz模型生成数据集，通过扩散模型处理实验数据的噪声分布，采用U-Net进行分割和反演，并结合迁移学习优化模型。

Result: 该集成框架能够在真实数据上实现自动化焊缝检测，有效处理分布外测量数据和不可预测的噪声分布。

Conclusion: 提出的工作流为工业环境中的自动化焊缝检测提供了完整的端到端解决方案，解决了数据稀缺和信号噪声的关键挑战。

Abstract: Automated ultrasonic weld inspection remains a significant challenge in the
nondestructive evaluation (NDE) community to factors such as limited training
data (due to the complexity of curating experimental specimens or high-fidelity
simulations) and environmental volatility of many industrial settings
(resulting in the corruption of on-the-fly measurements). Thus, an end-to-end
machine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,
industrial) settings has remained an elusive goal. This work addresses the
challenges of data curation and signal corruption by proposing workflow
consisting of a reduced-order modeling scheme, diffusion based distribution
alignment, and U-Net-based segmentation and inversion. A reduced-order
Helmholtz model based on Lamb wave theory is used to generate a comprehensive
dataset over varying weld heterogeneity and crack defects. The relatively
inexpensive low-order solutions provide a robust training dateset for inversion
models which are refined through a transfer learning stage using a limited set
of full 3D elastodynamic simulations. To handle out-of-distribution (OOD)
real-world measurements with varying and unpredictable noise distributions,
i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distribution
representations of OOD experimental LDV scans which are subsequently processed
by the inversion models. This integrated framework provides an end-to-end
solution for automated weld inspection on real data.

</details>


### [10] [Information Shapes Koopman Representation](https://arxiv.org/abs/2510.13025)
*Xiaoyuan Cheng,Wenxuan Yuan,Yiming Yang,Yuanzhao Zhang,Sibo Cheng,Yi He,Zhuo Sun*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的Koopman学习方法，通过平衡表示学习的简洁性和表达能力来解决传统方法中潜在空间崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: Koopman算子的无限维特性使得寻找合适的有限维子空间具有挑战性，特别是对于深度架构。现有方法在表示学习中存在信息瓶颈困境，难以平衡压缩表示的紧凑性和预测能力。

Method: 提出信息论拉格朗日公式，通过潜在互信息促进简洁性，通过冯·诺依曼熵维持表达能力以防止模式崩溃。基于此设计新算法来平衡这一权衡。

Result: 在多种动力系统上验证了方法的有效性，相比现有Koopman学习方法表现出改进的性能，并可视化学习到的流形结构。

Conclusion: 信息论视角下的Koopman学习能够产生稳定且可解释的表示，平衡简洁性和表达能力是提升Koopman学习效果的关键。

Abstract: The Koopman operator provides a powerful framework for modeling dynamical
systems and has attracted growing interest from the machine learning community.
However, its infinite-dimensional nature makes identifying suitable
finite-dimensional subspaces challenging, especially for deep architectures. We
argue that these difficulties come from suboptimal representation learning,
where latent variables fail to balance expressivity and simplicity. This
tension is closely related to the information bottleneck (IB) dilemma:
constructing compressed representations that are both compact and predictive.
Rethinking Koopman learning through this lens, we demonstrate that latent
mutual information promotes simplicity, yet an overemphasis on simplicity may
cause latent space to collapse onto a few dominant modes. In contrast,
expressiveness is sustained by the von Neumann entropy, which prevents such
collapse and encourages mode diversity. This insight leads us to propose an
information-theoretic Lagrangian formulation that explicitly balances this
tradeoff. Furthermore, we propose a new algorithm based on the Lagrangian
formulation that encourages both simplicity and expressiveness, leading to a
stable and interpretable Koopman representation. Beyond quantitative
evaluations, we further visualize the learned manifolds under our
representations, observing empirical results consistent with our theoretical
predictions. Finally, we validate our approach across a diverse range of
dynamical systems, demonstrating improved performance over existing Koopman
learning methods. The implementation is publicly available at
https://github.com/Wenxuan52/InformationKoopman.

</details>


### [11] [Bridging Idealized and Operational Models: An Explainable AI Framework for Earth System Emulators](https://arxiv.org/abs/2510.13030)
*Pouria Behnoudfar,Charlotte Moser,Marc Bocquet,Sibo Cheng,Nan Chen*

Main category: cs.LG

TL;DR: 开发了一个可解释的AI框架，通过结合高分辨率操作模型和理想化模型的优势，构建地球系统模拟器，显著改善了CMIP6模拟中厄尔尼诺时空模式的偏差。


<details>
  <summary>Details</summary>
Motivation: 现有高分辨率操作模型存在持续偏差，特别是在模拟极端事件和统计分布方面；而理想化模型虽然能精确校准特定特征，但不同模型之间存在学科界限。需要利用不同复杂度模型的互补优势来改进地球系统模拟。

Method: 开发可解释AI框架，通过重新配置的潜在数据同化技术连接模型层次结构，利用理想化模型的稀疏输出，构建桥接模型。该模型继承操作模型的高分辨率和全面变量，同时通过理想化模型实现全局精度提升。

Result: 该方法显著纠正了CMIP6模拟中厄尔尼诺时空模式的偏差，AI机制为这些改进提供了清晰的物理解释，超越了黑盒校正，实现了计算效率高的物理辅助数字孪生和不确定性量化。

Conclusion: 该工作证明了结合不同复杂度模型优势的有效性，强调了推动理想化模型发展和加强建模社区间沟通的重要性，为实现更准确的地球系统模拟提供了新途径。

Abstract: Computer models are indispensable tools for understanding the Earth system.
While high-resolution operational models have achieved many successes, they
exhibit persistent biases, particularly in simulating extreme events and
statistical distributions. In contrast, coarse-grained idealized models isolate
fundamental processes and can be precisely calibrated to excel in
characterizing specific dynamical and statistical features. However, different
models remain siloed by disciplinary boundaries. By leveraging the
complementary strengths of models of varying complexity, we develop an
explainable AI framework for Earth system emulators. It bridges the model
hierarchy through a reconfigured latent data assimilation technique, uniquely
suited to exploit the sparse output from the idealized models. The resulting
bridging model inherits the high resolution and comprehensive variables of
operational models while achieving global accuracy enhancements through
targeted improvements from idealized models. Crucially, the mechanism of AI
provides a clear rationale for these advancements, moving beyond black-box
correction to physically insightful understanding in a computationally
efficient framework that enables effective physics-assisted digital twins and
uncertainty quantification. We demonstrate its power by significantly
correcting biases in CMIP6 simulations of El Ni\~no spatiotemporal patterns,
leveraging statistically accurate idealized models. This work also highlights
the importance of pushing idealized model development and advancing
communication between modeling communities.

</details>


### [12] [Randomness and Interpolation Improve Gradient Descent](https://arxiv.org/abs/2510.13040)
*Jiawen Li,Pascal Lefevre,Anwar Pp Abdul Majeed*

Main category: cs.LG

TL;DR: 该论文基于SGD提出了两种优化器：IAGD和NRSGD。IAGD利用牛顿插值加速收敛，NRSGD通过噪声正则化防止过拟合。在CIFAR数据集上的实验表明这两种方法有效。


<details>
  <summary>Details</summary>
Motivation: 改进标准SGD优化器的性能，通过加速收敛和防止过拟合来提升深度学习模型的训练效果。

Method: 提出IAGD（利用二阶牛顿插值加速梯度下降）和NRSGD（噪声正则化随机梯度下降）两种优化方法。

Result: 在CIFAR-10和CIFAR-100数据集上的实验表明，这两种优化器相比Keras中的经典优化器表现更好。

Conclusion: IAGD和NRSGD是SGD的可行改进方法，证明了所提出技术进步的有效性。

Abstract: Based on Stochastic Gradient Descent (SGD), the paper introduces two
optimizers, named Interpolational Accelerating Gradient Descent (IAGD) as well
as Noise-Regularized Stochastic Gradient Descent (NRSGD). IAGD leverages
second-order Newton Interpolation to expedite the convergence process during
training, assuming relevancy in gradients between iterations. To avoid
over-fitting, NRSGD incorporates a noise regularization technique that
introduces controlled noise to the gradients during the optimization process.
Comparative experiments of this research are conducted on the CIFAR-10, and
CIFAR-100 datasets, benchmarking different CNNs(Convolutional Neural Networks)
with IAGD and NRSGD against classical optimizers in Keras Package. Results
demonstrate the potential of those two viable improvement methods in SGD,
implicating the effectiveness of the advancements.

</details>


### [13] [Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training](https://arxiv.org/abs/2510.13361)
*Yisen Wang,Yichuan Mo,Hongjun Wang,Junyi Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出了Generalist框架，通过将整体泛化目标分解为多个子任务，每个子任务由专门的基学习器负责，最终通过参数插值形成全局学习器，有效缓解了对抗训练中自然精度下降和鲁棒性跨攻击迁移差的问题。


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽然是最有效的防御方法，但在实际应用中存在两个主要限制：自然精度相比标准训练显著下降，以及在不同范数约束下攻击间的鲁棒性迁移能力差。

Method: 将整体泛化目标分解为多个子任务，每个子任务由专门的基学习器负责；在训练后期通过参数插值形成全局学习器，并定期将全局参数重新分配给基学习器以防止优化轨迹偏离共享目标。

Result: 理论分析和大量实验表明，Generalist相比基线方法实现了更低的泛化误差，并显著缓解了权衡问题。

Conclusion: Generalist为未来开发完全鲁棒的分类器提供了一个有前景的方向。

Abstract: Despite the rapid progress of neural networks, they remain highly vulnerable
to adversarial examples, for which adversarial training (AT) is currently the
most effective defense. While AT has been extensively studied, its practical
applications expose two major limitations: natural accuracy tends to degrade
significantly compared with standard training, and robustness does not transfer
well across attacks crafted under different norm constraints. Unlike prior
works that attempt to address only one issue within a single network, we
propose to partition the overall generalization goal into multiple sub-tasks,
each assigned to a dedicated base learner. By specializing in its designated
objective, each base learner quickly becomes an expert in its field. In the
later stages of training, we interpolate their parameters to form a
knowledgeable global learner, while periodically redistributing the global
parameters back to the base learners to prevent their optimization trajectories
from drifting too far from the shared target. We term this framework Generalist
and introduce three variants tailored to different application scenarios. Both
theoretical analysis and extensive experiments demonstrate that Generalist
achieves lower generalization error and significantly alleviates the trade-off
problems compared with baseline methods. Our results suggest that Generalist
provides a promising step toward developing fully robust classifiers in the
future.

</details>


### [14] [An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting](https://arxiv.org/abs/2510.13050)
*Shreya Agrawal,Mohammed Alewi Hassen,Emmanuel Asiedu Brempong,Boris Babenko,Fred Zyda,Olivia Graham,Di Li,Samier Merchant,Santiago Hincapie Potes,Tyler Russell,Danny Cheresnick,Aditya Prakash Kakkirala,Stephan Rasp,Avinatan Hassidim,Yossi Matias,Nal Kalchbrenner,Pramod Gupta,Jason Hickey,Aaron Bell*

Main category: cs.LG

TL;DR: Global MetNet是一个基于机器学习的全球降水临近预报模型，利用卫星和NWP数据为全球南方等雷达覆盖稀疏地区提供高分辨率降水预报，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决全球南方地区因雷达覆盖稀疏而无法获得准确降水临近预报的问题，减少全球预报质量的不平等。

Method: 利用全球降水任务CORRA数据集、地球静止卫星数据和全球NWP数据，构建机器学习模型进行12小时降水预报，空间分辨率约5公里，时间分辨率15分钟。

Result: 模型在数据稀疏地区表现优于美国最好的高分辨率NWP模型，在所有降水率和预报时效的关键指标上均有显著改进，预报生成时间不到1分钟。

Conclusion: 该模型已在Google搜索中为百万用户部署，是减少全球预报质量差异和将稀疏高分辨率卫星观测集成到天气预报中的重要一步。

Abstract: Precipitation nowcasting, which predicts rainfall up to a few hours ahead, is
a critical tool for vulnerable communities in the Global South frequently
exposed to intense, rapidly developing storms. Timely forecasts provide a
crucial window to protect lives and livelihoods. Traditional numerical weather
prediction (NWP) methods suffer from high latency, low spatial and temporal
resolution, and significant gaps in accuracy across the world. Recent machine
learning-based nowcasting methods, common in the Global North, cannot be
extended to the Global South due to extremely sparse radar coverage. We present
Global MetNet, an operational global machine learning nowcasting model. It
leverages the Global Precipitation Mission's CORRA dataset, geostationary
satellite data, and global NWP data to predict precipitation for the next 12
hours. The model operates at a high resolution of approximately 0.05{\deg}
(~5km) spatially and 15 minutes temporally. Global MetNet significantly
outperforms industry-standard hourly forecasts and achieves significantly
higher skill, making forecasts useful over a much larger area of the world than
previously available. Our model demonstrates better skill in data-sparse
regions than even the best high-resolution NWP models achieve in the US.
Validated using ground radar and satellite data, it shows significant
improvements across key metrics like the critical success index and fractions
skill score for all precipitation rates and lead times. Crucially, our model
generates forecasts in under a minute, making it readily deployable for
real-time applications. It is already deployed for millions of users on Google
Search. This work represents a key step in reducing global disparities in
forecast quality and integrating sparse, high-resolution satellite observations
into weather forecasting.

</details>


### [15] [Time-Varying Optimization for Streaming Data Via Temporal Weighting](https://arxiv.org/abs/2510.13052)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi,Erik G. Larsson*

Main category: cs.LG

TL;DR: 本文研究基于流数据的时变优化问题，提出了加权公式来捕捉流数据特性，分析了均匀权重和折扣权重两种策略下的跟踪误差界限。


<details>
  <summary>Details</summary>
Motivation: 经典优化理论处理固定目标函数，而时变优化在动态环境决策中日益重要。本文旨在通过时变优化视角研究流数据学习问题，特别是捕捉流数据来源的结构化特性。

Method: 引入基于权重的结构化公式，在梯度下降更新下分析两种权重策略：均匀权重（平等对待所有样本）和折扣权重（几何衰减旧数据影响）。推导跟踪误差的紧致界限。

Result: 均匀权重下跟踪误差以O(1/t)速率渐近消失，折扣权重则存在非零误差下限，由折扣因子和每时间步梯度更新次数控制。数值模拟验证了理论发现。

Conclusion: 权重策略选择对时变优化性能有显著影响，均匀权重可实现渐近精确跟踪，而折扣权重在有限误差下提供对最新数据的快速适应能力。

Abstract: Classical optimization theory deals with fixed, time-invariant objective
functions. However, time-varying optimization has emerged as an important
subject for decision-making in dynamic environments. In this work, we study the
problem of learning from streaming data through a time-varying optimization
lens. Unlike prior works that focus on generic formulations, we introduce a
structured, \emph{weight-based} formulation that explicitly captures the
streaming-data origin of the time-varying objective, where at each time step,
an agent aims to minimize a weighted average loss over all the past data
samples. We focus on two specific weighting strategies: (1) uniform weights,
which treat all samples equally, and (2) discounted weights, which
geometrically decay the influence of older data. For both schemes, we derive
tight bounds on the ``tracking error'' (TE), defined as the deviation between
the model parameter and the time-varying optimum at a given time step, under
gradient descent (GD) updates. We show that under uniform weighting, the TE
vanishes asymptotically with a $\mathcal{O}(1/t)$ decay rate, whereas
discounted weighting incurs a nonzero error floor controlled by the discount
factor and the number of gradient updates performed at each time step. Our
theoretical findings are validated through numerical simulations.

</details>


### [16] [Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games](https://arxiv.org/abs/2510.13060)
*Anupam Nayak,Tong Yang,Osman Yagan,Gauri Joshi,Yuejie Chi*

Main category: cs.LG

TL;DR: 本文研究了KL正则化在博弈论环境中的理论优势，提出了OMG和SOMG算法，在矩阵博弈和马尔可夫博弈中实现了对数级后悔，且后悔随KL正则化强度β的增大而减小。


<details>
  <summary>Details</summary>
Motivation: 尽管KL正则化在强化学习和语言模型对齐中取得了显著经验成功，但其在博弈论环境中的理论优势仍未被充分理解。本文旨在填补这一理论空白，证明KL正则化可以提升样本效率。

Method: 针对矩阵博弈提出了OMG算法，基于最佳响应采样和乐观奖励；针对马尔可夫博弈提出了SOMG算法，使用最佳响应采样和超乐观奖励概念。

Result: 两种算法都实现了对数级后悔，后悔随KL正则化强度β的增大而减小，同时保持了与未正则化设置相同的标准√T后悔界。

Conclusion: KL正则化在博弈论环境中确实能带来理论上的样本效率提升，为实际应用提供了理论依据。

Abstract: Reverse Kullback-Leibler (KL) divergence-based regularization with respect to
a fixed reference policy is widely used in modern reinforcement learning to
preserve the desired traits of the reference policy and sometimes to promote
exploration (using uniform reference policy, known as entropy regularization).
Beyond serving as a mere anchor, the reference policy can also be interpreted
as encoding prior knowledge about good actions in the environment. In the
context of alignment, recent game-theoretic approaches have leveraged KL
regularization with pretrained language models as reference policies, achieving
notable empirical success in self-play methods. Despite these advances, the
theoretical benefits of KL regularization in game-theoretic settings remain
poorly understood. In this work, we develop and analyze algorithms that
provably achieve improved sample efficiency under KL regularization. We study
both two-player zero-sum Matrix games and Markov games: for Matrix games, we
propose OMG, an algorithm based on best response sampling with optimistic
bonuses, and extend this idea to Markov games through the algorithm SOMG, which
also uses best response sampling and a novel concept of superoptimistic
bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales
inversely with the KL regularization strength $\beta$ in addition to the
standard $\widetilde{\mathcal{O}}(\sqrt{T})$ regret independent of $\beta$
which is attained in both regularized and unregularized settings

</details>


### [17] [Absolute indices for determining compactness, separability and number of clusters](https://arxiv.org/abs/2510.13065)
*Adil M. Bagirov,Ramiz M. Aliguliyev,Nargiz Sultanova,Sona Taheri*

Main category: cs.LG

TL;DR: 本文提出了新的绝对聚类指数来评估聚类的紧凑性和分离性，通过定义每个簇的紧凑度函数和簇对之间的邻域点集，用于确定真实簇的数量。


<details>
  <summary>Details</summary>
Motivation: 现有聚类有效性指数通常是相对的，依赖于底层数据结构，难以确定"真实"簇。需要开发绝对指标来直接评估簇的紧凑性和分离性。

Method: 定义每个簇的紧凑度函数和簇对之间的邻域点集，利用这些函数计算簇的紧凑性和分布紧凑性，使用邻域点集定义簇间边界和整体分布边界。

Result: 在多个合成和真实数据集上验证了所提指数的性能，并与其他广泛使用的聚类有效性指数进行了比较。

Conclusion: 提出的紧凑性和分离性指数能够有效识别真实簇数量，为聚类分析提供了新的绝对评估工具。

Abstract: Finding "true" clusters in a data set is a challenging problem. Clustering
solutions obtained using different models and algorithms do not necessarily
provide compact and well-separated clusters or the optimal number of clusters.
Cluster validity indices are commonly applied to identify such clusters.
Nevertheless, these indices are typically relative, and they are used to
compare clustering algorithms or choose the parameters of a clustering
algorithm. Moreover, the success of these indices depends on the underlying
data structure. This paper introduces novel absolute cluster indices to
determine both the compactness and separability of clusters. We define a
compactness function for each cluster and a set of neighboring points for
cluster pairs. This function is utilized to determine the compactness of each
cluster and the whole cluster distribution. The set of neighboring points is
used to define the margin between clusters and the overall distribution margin.
The proposed compactness and separability indices are applied to identify the
true number of clusters. Using a number of synthetic and real-world data sets,
we demonstrate the performance of these new indices and compare them with other
widely-used cluster validity indices.

</details>


### [18] [NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models](https://arxiv.org/abs/2510.13068)
*Konstantinos Barmpas,Na Lee,Alexandros Koliousis,Yannis Panagakis,Dimitrios A. Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 本文提出NeuroRVQ，一种基于码本的脑电信号大模型，通过多尺度特征提取、分层残差向量量化和相位-幅度感知损失函数，解决了现有脑电基础模型在信号重构保真度方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有脑电基础模型的信号标记化模块无法保留高频动态信息，限制了信号重构的保真度，需要开发能够捕捉全频段神经频谱的高效标记化方法。

Method: 设计基于码本的标记器，包含多尺度特征提取模块、分层残差向量量化码本和相位-幅度感知损失函数，实现高效脑电压缩和精确重构。

Result: NeuroRVQ实现了更低的信号重构误差，在多种下游任务中优于现有脑电大模型，建立了基于码本的通用脑电模型的强先验。

Conclusion: NeuroRVQ为基于码本的通用脑电模型提供了强大的基础，推动了神经解码、生成建模和多模态生物信号集成的发展。

Abstract: Electroencephalography (EEG) captures neural activity across multiple
temporal and spectral scales, yielding signals that are rich but complex for
representation learning. Recently, EEG foundation models trained to predict
masked signal-tokens have shown promise for learning generalizable
representations. However, their performance is hindered by their signal
tokenization modules. Existing neural tokenizers fail to preserve
high-frequency dynamics, limiting their ability to reconstruct EEG signals with
high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)
centered on a codebook-based tokenizer. Our tokenizer integrates: (i)
multi-scale feature extraction modules that capture the full frequency neural
spectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks for
high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware
loss function for efficient training. This design enables efficient EEG
compression while supporting accurate reconstruction across all frequency
bands, leading to robust generative masked modeling. Our empirical results
demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms
existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ
tokenizer establishes a strong prior for codebook-based general-purpose
brainwave models, enabling advances in neural decoding, generative modeling and
multimodal biosignal integration.

</details>


### [19] [Transformer-based Scalable Beamforming Optimization via Deep Residual Learning](https://arxiv.org/abs/2510.13077)
*Yubo Zhang,Xiao-Yang Liu,Xiaodong Wang*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的无监督深度学习框架，用于大规模MU-MISO系统的下行波束成形。该模型离线训练，在线轻量推理，通过课程学习、半摊销学习和滑动窗口训练等策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多用户MISO系统中下行波束成形的计算复杂性问题，实现实时高效的波束成形设计。

Method: 采用学习优化范式，使用多层Transformer通过残差连接迭代优化信道和波束成形特征，并引入课程学习、半摊销学习和滑动窗口训练三种策略来增强训练效果。

Result: 在低到中等信噪比下优于现有基线方法，在高信噪比下接近WMMSE性能，同时推理速度显著快于迭代和在线学习方法。

Conclusion: 所提出的无监督深度学习框架能够有效解决大规模MU-MISO波束成形问题，在性能和效率之间取得了良好平衡。

Abstract: We develop an unsupervised deep learning framework for downlink beamforming
in large-scale MU-MISO channels. The model is trained offline, allowing
real-time inference through lightweight feedforward computations in dynamic
communication environments. Following the learning-to-optimize (L2O) paradigm,
a multi-layer Transformer iteratively refines both channel and beamformer
features via residual connections. To enhance training, three strategies are
introduced: (i) curriculum learning (CL) to improve early-stage convergence and
avoid local optima, (ii) semi-amortized learning to refine each Transformer
block with a few gradient ascent steps, and (iii) sliding-window training to
stabilize optimization by training only a subset of Transformer blocks at a
time. Extensive simulations show that the proposed scheme outperforms existing
baselines at low-to-medium SNRs and closely approaches WMMSE performance at
high SNRs, while achieving substantially faster inference than iterative and
online learning approaches.

</details>


### [20] [DeepCausalMMM: A Deep Learning Framework for Marketing Mix Modeling with Causal Inference](https://arxiv.org/abs/2510.13087)
*Aditya Puttaparthi Tirumala*

Main category: cs.LG

TL;DR: DeepCausalMMM是一个Python包，结合深度学习、因果推理和营销科学，通过GRU自动学习时间模式，使用DAG学习营销渠道间的因果关系，并实现基于Hill方程的饱和曲线来优化预算分配。


<details>
  <summary>Details</summary>
Motivation: 传统营销组合建模方法依赖线性回归或贝叶斯层次模型，假设营销渠道间独立，难以捕捉复杂的时间动态和非线性饱和效应。

Method: 使用门控循环单元(GRU)自动学习时间模式（如广告存量效应和滞后），通过有向无环图(DAG)学习营销渠道间的统计依赖和潜在因果结构，实现基于Hill方程的饱和曲线建模。

Result: 实现了数据驱动设计（超参数和转换从数据中学习）、多区域建模（共享和区域特定参数）、鲁棒统计方法（Huber损失和高级正则化）、全面的响应曲线分析和14+交互式仪表板的可视化套件。

Conclusion: DeepCausalMMM通过结合深度学习、因果推理和先进营销科学，解决了传统MMM方法的局限性，提供了更准确和实用的营销效果评估工具。

Abstract: Marketing Mix Modeling (MMM) is a statistical technique used to estimate the
impact of marketing activities on business outcomes such as sales, revenue, or
customer visits. Traditional MMM approaches often rely on linear regression or
Bayesian hierarchical models that assume independence between marketing
channels and struggle to capture complex temporal dynamics and non-linear
saturation effects [@Hanssens2005; @Ng2021Bayesian].
  DeepCausalMMM is a Python package that addresses these limitations by
combining deep learning, causal inference, and advanced marketing science. The
package uses Gated Recurrent Units (GRUs) to automatically learn temporal
patterns such as adstock (carryover effects) and lag, while simultaneously
learning statistical dependencies and potential causal structures between
marketing channels through Directed Acyclic Graph (DAG) learning
[@Zheng2018NOTEARS; @Gong2024CausalMMM]. Additionally, it implements Hill
equation-based saturation curves to model diminishing returns and optimize
budget allocation.
  Key innovations include: (1) a data-driven design where hyperparameters and
transformations (e.g., adstock decay, saturation curves) are learned or
estimated from data with sensible defaults, rather than requiring fixed
heuristics or manual specification, (2) multi-region modeling with both shared
and region-specific parameters, (3) robust statistical methods including Huber
loss and advanced regularization, (4) comprehensive response curve analysis for
understanding channel saturation, and (5) an extensive visualization suite with
14+ interactive dashboards for business insights.

</details>


### [21] [On the Reasoning Abilities of Masked Diffusion Language Models](https://arxiv.org/abs/2510.13117)
*Anej Svete,Ashish Sabharwal*

Main category: cs.LG

TL;DR: 该论文证明了掩码扩散模型（MDMs）在文本生成中与传统自回归模型相比具有并行生成优势，并建立了MDMs与链式思维（CoT）和填充循环变换器（PLTs）在有限精度对数宽度设置下的等价关系，展示了MDMs在某些问题类别上比CoT变换器更高效。


<details>
  <summary>Details</summary>
Motivation: 探索掩码扩散模型在文本生成中的计算能力和并行性限制，填补对这类模型推理能力理解的研究空白。

Method: 通过将MDMs与链式思维（CoT）和填充循环变换器（PLTs）在有限精度对数宽度设置下建立理论等价关系，分析MDMs的计算能力。

Result: 证明了MDMs与多项式填充PLTs在该设置下等价，MDMs能够解决所有CoT增强变换器能解决的问题，并且在某些问题类别（如正则语言）上比CoT变换器更高效。

Conclusion: 掩码扩散模型在并行生成方面具有显著优势，能够在某些推理问题上实现比传统自回归模型更快的推理速度，为文本生成模型提供了新的理论支持。

Abstract: Masked diffusion models (MDMs) for text offer a compelling alternative to
traditional autoregressive language models. Parallel generation makes them
efficient, but their computational capabilities and the limitations inherent to
their parallelism remain largely unexplored. To this end, we characterize what
types of reasoning problems MDMs can provably solve and how efficiently. We do
this by connecting MDMs to the well-understood reasoning frameworks of chain of
thought (CoT) and padded looped transformers (PLTs) in the finite-precision
log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact,
equivalent in this setting, and that MDMs can solve all problems that
CoT-augmented transformers can. Moreover, we showcase classes of problems
(including regular languages) for which MDMs are inherently more efficient than
CoT transformers, where parallel generation allows for substantially faster
reasoning.

</details>


### [22] [Cluster-Based Client Selection for Dependent Multi-Task Federated Learning in Edge Computing](https://arxiv.org/abs/2510.13132)
*Jieping Luo,Qiyue Li,Zhizhang Liu,Hang Qi,Jiaying Yin,Jingjin Wu*

Main category: cs.LG

TL;DR: CoDa-FL是一个面向集群和依赖感知的联邦学习框架，通过基于聚类的客户端选择和依赖任务分配来减少多任务学习的总完成时间。


<details>
  <summary>Details</summary>
Motivation: 在移动边缘计算环境下的联邦学习中，特别是在依赖多任务设置下，需要减少完成各种学习任务所需的总时间。

Method: 使用Earth Mover's Distance进行基于本地数据分布的客户端聚类，降低计算成本并提高通信效率；结合有向无环图的任务调度机制来管理任务依赖关系。

Result: 数值实验验证CoDa-FL在异构MEC设置下相比现有基准方法，实现了更快的收敛速度、更低的通信和计算成本以及更高的学习精度。

Conclusion: CoDa-FL框架通过集群导向和依赖感知的方法，有效解决了联邦学习中的客户端选择问题，显著提升了多任务学习的效率。

Abstract: We study the client selection problem in Federated Learning (FL) within
mobile edge computing (MEC) environments, particularly under the dependent
multi-task settings, to reduce the total time required to complete various
learning tasks. We propose CoDa-FL, a Cluster-oriented and Dependency-aware
framework designed to reduce the total required time via cluster-based client
selection and dependent task assignment. Our approach considers Earth Mover's
Distance (EMD) for client clustering based on their local data distributions to
lower computational cost and improve communication efficiency. We derive a
direct and explicit relationship between intra-cluster EMD and the number of
training rounds required for convergence, thereby simplifying the otherwise
complex process of obtaining the optimal solution. Additionally, we incorporate
a directed acyclic graph-based task scheduling mechanism to effectively manage
task dependencies. Through numerical experiments, we validate that our proposed
CoDa-FL outperforms existing benchmarks by achieving faster convergence, lower
communication and computational costs, and higher learning accuracy under
heterogeneous MEC settings.

</details>


### [23] [Convergence, design and training of continuous-time dropout as a random batch method](https://arxiv.org/abs/2510.13134)
*Antonio Álvarez-López,Martín Hernández*

Main category: cs.LG

TL;DR: 本文通过随机批处理方法研究连续时间模型中的dropout正则化，构建了无偏估计器，建立了轨迹和分布层面的收敛性分析，并在训练中分析了最优控制和梯度下降的偏差，最终在单层神经ODE上验证了理论。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间模型中dropout正则化的理论基础，通过随机批处理方法降低计算成本，同时保持正则化效果。

Method: 使用随机批处理方法构建无偏估计器，通过采样时间间隔为h的神经元批次来模拟dropout，建立了轨迹收敛性和分布稳定性分析，并在训练中采用Pontryagin伴随分析。

Result: 轨迹层面建立了线性收敛率，分布层面获得了总变差误差为h^{1/2}的稳定性，训练中控制了最优成本和梯度的偏差，并在实验中观察到预测的收敛率和正则化效果。

Conclusion: 提出的随机批处理方法有效实现了连续时间模型中的dropout正则化，具有理论保证和实际优势，包括良好的运行时和内存性能。

Abstract: We study dropout regularization in continuous-time models through the lens of
random-batch methods -- a family of stochastic sampling schemes originally
devised to reduce the computational cost of interacting particle systems. We
construct an unbiased, well-posed estimator that mimics dropout by sampling
neuron batches over time intervals of length $h$. Trajectory-wise convergence
is established with linear rate in $h$ for the expected uniform error. At the
distribution level, we establish stability for the associated continuity
equation, with total-variation error of order $h^{1/2}$ under mild moment
assumptions. During training with fixed batch sampling across epochs, a
Pontryagin-based adjoint analysis bounds deviations in the optimal cost and
control, as well as in gradient-descent iterates. On the design side, we
compare convergence rates for canonical batch sampling schemes, recover
standard Bernoulli dropout as a special case, and derive a cost--accuracy
trade-off yielding a closed-form optimal $h$. We then specialize to a
single-layer neural ODE and validate the theory on classification and flow
matching, observing the predicted rates, regularization effects, and favorable
runtime and memory profiles.

</details>


### [24] [Behavioral Embeddings of Programs: A Quasi-Dynamic Approach for Optimization Prediction](https://arxiv.org/abs/2510.13158)
*Haolin Pan,Jinyuan Dong,Hongbin Zhang,Hongyu Lin,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种准动态程序表示框架，通过建模程序的优化敏感性来生成程序行为谱，使用产品量化和多任务Transformer模型学习行为代码的上下文语法，在编译器优化任务中优于现有静态方法。


<details>
  <summary>Details</summary>
Motivation: 现有程序表示方法存在困境：静态表示高效但无法洞察程序在复杂代码变换下的行为，动态表示能提供性能洞察但开销大且非确定性。需要超越这种权衡的新方法。

Method: 提出准动态框架，通过用多样化优化序列探测程序的中间表示，量化静态特征变化来生成程序行为谱。使用产品量化将连续反应向量离散化为结构化子词，然后用多任务Transformer模型PQ-BERT预训练学习行为代码的上下文语法。

Result: 在两个代表性编译器优化任务（最佳传递预测和-Oz收益预测）上的综合实验表明，该方法优于最先进的静态基线方法。

Conclusion: 准动态程序表示框架成功超越了静态和动态表示之间的权衡，通过建模优化敏感性提供了更有效的程序表示，在编译器优化任务中表现出优越性能。

Abstract: Learning effective numerical representations, or embeddings, of programs is a
fundamental prerequisite for applying machine learning to automate and enhance
compiler optimization. Prevailing paradigms, however, present a dilemma. Static
representations, derived from source code or intermediate representation (IR),
are efficient and deterministic but offer limited insight into how a program
will behave or evolve under complex code transformations. Conversely, dynamic
representations, which rely on runtime profiling, provide profound insights
into performance bottlenecks but are often impractical for large-scale tasks
due to prohibitive overhead and inherent non-determinism. This paper transcends
this trade-off by proposing a novel quasi-dynamic framework for program
representation. The core insight is to model a program's optimization
sensitivity. We introduce the Program Behavior Spectrum, a new representation
generated by probing a program's IR with a diverse set of optimization
sequences and quantifying the resulting changes in its static features. To
effectively encode this high-dimensional, continuous spectrum, we pioneer a
compositional learning approach. Product Quantization is employed to discretize
the continuous reaction vectors into structured, compositional sub-words.
Subsequently, a multi-task Transformer model, termed PQ-BERT, is pre-trained to
learn the deep contextual grammar of these behavioral codes. Comprehensive
experiments on two representative compiler optimization tasks -- Best Pass
Prediction and -Oz Benefit Prediction -- demonstrate that our method
outperforms state-of-the-art static baselines. Our code is publicly available
at https://github.com/Panhaolin2001/PREP/.

</details>


### [25] [Information-Theoretic Criteria for Knowledge Distillation in Multimodal Learning](https://arxiv.org/abs/2510.13182)
*Rongrong Xie,Yizhou Xu,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 本文提出了跨模态互补性假设（CCH），认为当教师模态与学生模态表征之间的互信息超过学生模态表征与标签之间的互信息时，跨模态知识蒸馏（KD）才有效。该假设在联合高斯模型中得到了理论验证，并在多个多模态数据集上进行了实证检验。


<details>
  <summary>Details</summary>
Motivation: 尽管跨模态知识蒸馏技术在多模态数据应用中取得了成功，但由于缺乏理论指导，该方法并不总能改善性能。本文旨在填补这一理论空白，为跨模态KD提供理论基础和实践指导。

Method: 提出了跨模态互补性假设（CCH），在联合高斯模型中进行理论验证，并在图像、文本、视频、音频和癌症相关组学数据等多种多模态数据集上进行实证检验。

Result: 理论分析和实证结果均支持CCH假设，表明当教师模态与学生模态表征之间的互信息超过学生模态表征与标签之间的互信息时，跨模态KD能够有效提升性能。

Conclusion: 本研究建立了一个理解跨模态知识蒸馏的新理论框架，并基于CCH准则提供了选择最优教师模态的实用指南，以改善较弱模态的性能。

Abstract: The rapid increase in multimodal data availability has sparked significant
interest in cross-modal knowledge distillation (KD) techniques, where richer
"teacher" modalities transfer information to weaker "student" modalities during
model training to improve performance. However, despite successes across
various applications, cross-modal KD does not always result in improved
outcomes, primarily due to a limited theoretical understanding that could
inform practice. To address this gap, we introduce the Cross-modal
Complementarity Hypothesis (CCH): we propose that cross-modal KD is effective
when the mutual information between teacher and student representations exceeds
the mutual information between the student representation and the labels. We
theoretically validate the CCH in a joint Gaussian model and further confirm it
empirically across diverse multimodal datasets, including image, text, video,
audio, and cancer-related omics data. Our study establishes a novel theoretical
framework for understanding cross-modal KD and offers practical guidelines
based on the CCH criterion to select optimal teacher modalities for improving
the performance of weaker modalities.

</details>


### [26] [Performance Evaluation of Ising and QUBO Variable Encodings in Boltzmann Machine Learning](https://arxiv.org/abs/2510.13210)
*Yasushi Hasegawa,Masayuki Ohzeki*

Main category: cs.LG

TL;DR: 比较Ising({-1,+1})和QUBO({0,1})编码在玻尔兹曼机学习中的表现，发现QUBO编码导致Fisher信息矩阵条件数更差，使得随机梯度下降收敛更慢，而自然梯度下降对编码选择不敏感。


<details>
  <summary>Details</summary>
Motivation: 研究不同变量编码（Ising vs QUBO）如何影响玻尔兹曼机学习的信息几何特性和收敛性能，为实际应用提供编码选择和预处理指导。

Method: 在固定模型、采样器和步长的受控协议下，利用Fisher信息矩阵等于充分统计量协方差的特性，可视化模型样本的经验矩，分析两种编码的差异。

Result: QUBO编码在一阶和二阶统计量之间产生更大的交叉项，导致Fisher信息矩阵出现更多小特征值方向，降低谱熵，使随机梯度下降收敛更慢。自然梯度下降由于重参数化不变性，在不同编码下收敛性能相似。

Conclusion: 对于基于随机梯度下降的训练，Ising编码提供更各向同性的曲率和更快收敛；对于QUBO编码，中心化/缩放或自然梯度下降风格的预处理可以缓解曲率问题。这些结果为玻尔兹曼机中的变量编码和预处理提供了实用指导。

Abstract: We compare Ising ({-1,+1}) and QUBO ({0,1}) encodings for Boltzmann machine
learning under a controlled protocol that fixes the model, sampler, and step
size. Exploiting the identity that the Fisher information matrix (FIM) equals
the covariance of sufficient statistics, we visualize empirical moments from
model samples and reveal systematic, representation-dependent differences. QUBO
induces larger cross terms between first- and second-order statistics, creating
more small-eigenvalue directions in the FIM and lowering spectral entropy. This
ill-conditioning explains slower convergence under stochastic gradient descent
(SGD). In contrast, natural gradient descent (NGD)-which rescales updates by
the FIM metric-achieves similar convergence across encodings due to
reparameterization invariance. Practically, for SGD-based training, the Ising
encoding provides more isotropic curvature and faster convergence; for QUBO,
centering/scaling or NGD-style preconditioning mitigates curvature pathologies.
These results clarify how representation shapes information geometry and
finite-time learning dynamics in Boltzmann machines and yield actionable
guidelines for variable encoding and preprocessing.

</details>


### [27] [Towards Understanding Valuable Preference Data for Large Language Model Alignment](https://arxiv.org/abs/2510.13212)
*Zizhuo Zhang,Qizhou Wang,Shanshan Ye,Jianing Zhu,Jiangchao Yao,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了一种基于截断影响函数(TIF)的偏好数据质量评估方法，发现数据质量是模型相关的，并开发了两种简化的评分函数来改进偏好数据选择，通过组合这些函数实现了更精确的数据选择，在更少数据下获得了更好的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常使用外部奖励模型或现成LLM预处理原始训练数据集来识别有价值的偏好对，但很少检查单个数据点是否真正有益。本文旨在通过个体影响评估数据质量，并改进偏好数据选择方法以适应特定模型。

Method: 提出截断影响函数(TIF)来评估个体数据对验证数据的影响，缓解传统度量中的过度评分问题；引入两种计算更简单的评分函数(SF)，与TIF正相关且模型相关；组合这些SF以抵消不同的误差源，形成简单有效的数据选择规则。

Result: 实验表明，使用更少的数据可以实现更好的对齐性能，证明了所提方法的有效性和通用性。

Conclusion: 偏好数据质量是模型相关的，一个数据对可能有益于一个模型但损害另一个模型；通过模型相关的数据选择方法可以更精确地选择有价值的偏好数据，在减少数据使用的同时提升对齐性能。

Abstract: Large language model (LLM) alignment is typically achieved through learning
from human preference comparisons, making the quality of preference data
critical to its success. Existing studies often pre-process raw training
datasets to identify valuable preference pairs using external reward models or
off-the-shelf LLMs, achieving improved overall performance but rarely examining
whether individual, selected data point is genuinely beneficial. We assess data
quality through individual influence on validation data using our newly
proposed truncated influence function (TIF), which mitigates the over-scoring
present in traditional measures and reveals that preference data quality is
inherently a property of the model. In other words, a data pair that benefits
one model may harm another. This leaves the need to improve the preference data
selection approaches to be adapting to specific models. To this end, we
introduce two candidate scoring functions (SFs) that are computationally
simpler than TIF and positively correlated with it. They are also model
dependent and can serve as potential indicators of individual data quality for
preference data selection. Furthermore, we observe that these SFs inherently
exhibit errors when compared to TIF. To this end, we combine them to offset
their diverse error sources, resulting in a simple yet effective data selection
rule that enables the models to achieve a more precise selection of valuable
preference data. We conduct experiments across diverse alignment benchmarks and
various LLM families, with results demonstrating that better alignment
performance can be achieved using less data, showing the generality of our
findings and new methods.

</details>


### [28] [BlendFL: Blended Federated Learning for Handling Multimodal Data Heterogeneity](https://arxiv.org/abs/2510.13266)
*Alejandro Guerra-Manzanares,Omar El-Herraoui,Michail Maniatakos,Farah E. Shamout*

Main category: cs.LG

TL;DR: BlendFL是一个新颖的联邦学习框架，通过融合水平和垂直联邦学习的原理，有效处理多模态数据异构性问题，支持非对称客户端参与，并提供去中心化推理机制。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界环境中多模态数据异构性挑战，现有联邦学习框架在非理想设置下效果有限，无法处理客户端间模态和样本分布不均的情况。

Method: 提出BlendFL框架，同步融合水平和垂直联邦学习原则，支持客户端根据可用数据集灵活选择参与方式，引入去中心化推理机制和自适应全局模型聚合策略BlendAvg。

Result: 在三个分类任务上使用大规模真实世界多模态医疗数据集和流行多模态基准进行评估，BlendFL在多模态和单模态分类中均表现出优越性能，消融研究显示其收敛速度更快。

Conclusion: BlendFL在数据隐私至关重要的现实世界环境（如医疗和金融）中，具有处理多模态数据异构性的巨大潜力，能加速协作学习过程。

Abstract: One of the key challenges of collaborative machine learning, without data
sharing, is multimodal data heterogeneity in real-world settings. While
Federated Learning (FL) enables model training across multiple clients,
existing frameworks, such as horizontal and vertical FL, are only effective in
`ideal' settings that meet specific assumptions. Hence, they struggle to
address scenarios where neither all modalities nor all samples are represented
across the participating clients. To address this gap, we propose BlendFL, a
novel FL framework that seamlessly blends the principles of horizontal and
vertical FL in a synchronized and non-restrictive fashion despite the asymmetry
across clients. Specifically, any client within BlendFL can benefit from either
of the approaches, or both simultaneously, according to its available dataset.
In addition, BlendFL features a decentralized inference mechanism, empowering
clients to run collaboratively trained local models using available local data,
thereby reducing latency and reliance on central servers for inference. We also
introduce BlendAvg, an adaptive global model aggregation strategy that
prioritizes collaborative model updates based on each client's performance. We
trained and evaluated BlendFL and other state-of-the-art baselines on three
classification tasks using a large-scale real-world multimodal medical dataset
and a popular multimodal benchmark. Our results highlight BlendFL's superior
performance for both multimodal and unimodal classification. Ablation studies
demonstrate BlendFL's faster convergence compared to traditional approaches,
accelerating collaborative learning. Overall, in our study we highlight the
potential of BlendFL for handling multimodal data heterogeneity for
collaborative learning in real-world settings where data privacy is crucial,
such as in healthcare and finance.

</details>


### [29] [To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models](https://arxiv.org/abs/2510.13290)
*Anna Hedström,Salim I. Amoukou,Tom Bewley,Saumitra Mishra,Manuela Veloso*

Main category: cs.LG

TL;DR: MERA是一个通过选择性、自适应干预来引导语言模型减少错误的框架，它优化干预方向并校准何时以及如何引导，从而在无法自信修正时放弃干预，提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定、手动调整的引导强度，往往导致引导不足或过度引导，MERA旨在解决这些限制。

Method: MERA通过(i)优化干预方向，(ii)校准何时以及如何引导，来改进性能或在无法自信修正时放弃干预。

Result: 在多个数据集和语言模型家族上的实验表明，MERA实现了安全、有效、不降低性能的错误修正，并优于现有基线方法。

Conclusion: MERA可以作为现有引导技术的补充，进一步提升其性能，是一种通用且高效的机制激活引导方法。

Abstract: We introduce Mechanistic Error Reduction with Abstention (MERA), a principled
framework for steering language models (LMs) to mitigate errors through
selective, adaptive interventions. Unlike existing methods that rely on fixed,
manually tuned steering strengths, often resulting in under or oversteering,
MERA addresses these limitations by (i) optimising the intervention direction,
and (ii) calibrating when, and how much to steer, thereby provably improving
performance or abstaining when no confident correction is possible. Experiments
across diverse datasets, and LM families demonstrate safe, effective,
non-degrading error correction, and that MERA outperforms existing baselines.
Moreover, MERA can be applied on top of existing steering techniques to further
enhance their performance, establishing it as a general-purpose, and efficient
approach to mechanistic activation steering.

</details>


### [30] [Federated Conditional Conformal Prediction via Generative Models](https://arxiv.org/abs/2510.13297)
*Rui Xu,Sihong Xie*

Main category: cs.LG

TL;DR: 提出Fed-CCP方法，通过生成模型实现联邦学习中的条件共形预测，解决传统方法在非独立同分布数据下无法提供输入条件不确定性量化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法假设数据独立同分布，但在联邦学习场景中客户端数据分布差异显著，现有联邦CP方法只能保证边际覆盖，无法反映输入条件的不确定性。

Method: 使用生成模型（如归一化流或扩散模型）近似条件数据分布，无需共享原始数据，让每个客户端本地校准反映其独特不确定性的共形分数，通过联邦聚合保持全局一致性。

Result: 在真实数据集上的实验表明，Fed-CCP能够实现更自适应的预测集。

Conclusion: Fed-CCP通过生成模型成功解决了联邦学习中数据异构性的挑战，提供了更准确的条件不确定性量化。

Abstract: Conformal Prediction (CP) provides distribution-free uncertainty
quantification by constructing prediction sets that guarantee coverage of the
true labels. This reliability makes CP valuable for high-stakes federated
learning scenarios such as multi-center healthcare. However, standard CP
assumes i.i.d. data, which is violated in federated settings where client
distributions differ substantially. Existing federated CP methods address this
by maintaining marginal coverage on each client, but such guarantees often fail
to reflect input-conditional uncertainty. In this work, we propose Federated
Conditional Conformal Prediction (Fed-CCP) via generative models, which aims
for conditional coverage that adapts to local data heterogeneity. Fed-CCP
leverages generative models, such as normalizing flows or diffusion models, to
approximate conditional data distributions without requiring the sharing of raw
data. This enables each client to locally calibrate conformal scores that
reflect its unique uncertainty, while preserving global consistency through
federated aggregation. Experiments on real datasets demonstrate that Fed-CCP
achieves more adaptive prediction sets.

</details>


### [31] [Km-scale dynamical downscaling through conformalized latent diffusion models](https://arxiv.org/abs/2510.13301)
*Alessandro Brusaferri,Andrea Ballarino*

Main category: cs.LG

TL;DR: 该论文提出了一种结合生成扩散模型和保形预测框架的方法，用于改进气象数据降尺度中的不确定性量化，确保有限样本下的边际有效性。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型在气象数据降尺度中缺乏有限样本保证，导致网格点级别的不确定性估计不准确，影响其在业务应用中的可靠性。

Method: 在降尺度流程中增加保形预测框架，对扩散模型的样本进行后处理得到条件分位数估计，并纳入保形化分位数回归过程，获得具有局部自适应性的预测区间。

Result: 在意大利ERA5再分析数据上的实验表明，该方法显著改善了网格点级别的不确定性估计覆盖率和概率评分稳定性。

Conclusion: 保形化生成模型在高分辨率气象场概率降尺度中具有提高可信度的潜力。

Abstract: Dynamical downscaling is crucial for deriving high-resolution meteorological
fields from coarse-scale simulations, enabling detailed analysis for critical
applications such as weather forecasting and renewable energy modeling.
Generative Diffusion models (DMs) have recently emerged as powerful data-driven
tools for this task, offering reconstruction fidelity and more scalable
sampling supporting uncertainty quantification. However, DMs lack finite-sample
guarantees against overconfident predictions, resulting in miscalibrated
grid-point-level uncertainty estimates hindering their reliability in
operational contexts. In this work, we tackle this issue by augmenting the
downscaling pipeline with a conformal prediction framework. Specifically, the
DM's samples are post-processed to derive conditional quantile estimates,
incorporated into a conformalized quantile regression procedure targeting
locally adaptive prediction intervals with finite-sample marginal validity. The
proposed approach is evaluated on ERA5 reanalysis data over Italy, downscaled
to a 2-km grid. Results demonstrate grid-point-level uncertainty estimates with
markedly improved coverage and stable probabilistic scores relative to the DM
baseline, highlighting the potential of conformalized generative models for
more trustworthy probabilistic downscaling to high-resolution meteorological
fields.

</details>


### [32] [Thompson Sampling via Fine-Tuning of LLMs](https://arxiv.org/abs/2510.13328)
*Nicolas Menet,Aleksandar Terzić,Andreas Krause,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出ToSFiT方法，利用大语言模型进行Thompson采样，避免传统贝叶斯优化中昂贵的采集函数最大化过程，在离散空间中实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在大型离散空间中因缺乏梯度而计算成本高昂，需要一种无需采集函数最大化的可扩展替代方案。

Method: 基于Thompson采样，通过微调提示条件大语言模型直接参数化候选解获得最大奖励的概率，逐步适应后验分布。

Result: 在FAQ响应优化、热稳定蛋白质搜索和量子电路设计三个任务中验证，在线微调显著提高样本效率，计算效率影响可忽略。

Conclusion: ToSFiT方法通过仔细适应最大概率后验的原则，在离散优化任务中实现了高效的Thompson采样，理论分析和实证结果均验证了其有效性。

Abstract: Bayesian optimization in large unstructured discrete spaces is often hindered
by the computational cost of maximizing acquisition functions due to the
absence of gradients. We propose a scalable alternative based on Thompson
sampling that eliminates the need for acquisition function maximization by
directly parameterizing the probability that a candidate yields the maximum
reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the
prior knowledge embedded in prompt-conditioned large language models, and
incrementally adapts them toward the posterior. Theoretically, we derive a
novel regret bound for a variational formulation of Thompson Sampling that
matches the strong guarantees of its standard counterpart. Our analysis reveals
the critical role of careful adaptation to the posterior probability of
maximality--a principle that underpins our ToSFiT algorithm. Empirically, we
validate our method on three diverse tasks: FAQ response refinement, thermally
stable protein search, and quantum circuit design. We demonstrate that online
fine-tuning significantly improves sample efficiency, with negligible impact on
computational efficiency.

</details>


### [33] [A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control](https://arxiv.org/abs/2510.13367)
*Nikita Kachaev,Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovelev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文研究表明transformer可以作为在线无模型强化学习中连续控制任务的强基线，探讨了输入条件、组件共享和序列数据处理等关键设计问题，并提出了稳定的架构和训练策略。


<details>
  <summary>Details</summary>
Motivation: 尽管transformer在离线或基于模型的强化学习中表现出色，但在在线无模型强化学习中仍未充分探索，主要因为其对训练设置和模型设计决策的敏感性。

Method: 研究了关键设计问题：如何条件化输入、在actor和critic之间共享组件、以及如何切片序列数据进行训练。

Result: 实验揭示了稳定的架构和训练策略，在完全和部分可观测任务中，以及向量和图像设置下都实现了有竞争力的性能。

Conclusion: 这些发现为在在线强化学习中应用transformer提供了实用指导。

Abstract: Despite their effectiveness and popularity in offline or model-based
reinforcement learning (RL), transformers remain underexplored in online
model-free RL due to their sensitivity to training setups and model design
decisions such as how to structure the policy and value networks, share
components, or handle temporal information. In this paper, we show that
transformers can be strong baselines for continuous control in online
model-free RL. We investigate key design questions: how to condition inputs,
share components between actor and critic, and slice sequential data for
training. Our experiments reveal stable architectural and training strategies
enabling competitive performance across fully and partially observable tasks,
and in both vector- and image-based settings. These findings offer practical
guidance for applying transformers in online RL.

</details>


### [34] [Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2510.13368)
*Yue Xing,Yingnan Deng,Heyao Liu,Ming Wang,Yun Zi,Xiaoxuan Sun*

Main category: cs.LG

TL;DR: 提出一种结合对比学习的依赖建模和异常检测方法，通过构建依赖图、提取时空特征、使用图卷积聚合邻域信息，并引入对比学习框架增强正常与异常模式的可分性，在云服务环境中实现稳定可靠的异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决云服务环境中复杂依赖关系和多样化异常模式的挑战，传统方法难以有效处理这些复杂场景。

Method: 将服务交互抽象为依赖图，通过嵌入函数提取时空特征，使用图卷积机制聚合邻域信息，引入对比学习框架构建正负样本对，并设计时间一致性约束保持表示稳定性。

Result: 在公开数据集上的实验表明，该方法在精确率、召回率、F1分数和AUC等关键指标上显著优于现有方法，在稀疏标注、监控噪声和流量波动等条件下保持鲁棒性。

Conclusion: 验证了依赖建模与对比学习结合的有效性，为云服务异常检测提供了完整技术方案，在复杂环境中表现出强大的适应性和稳定性。

Abstract: This paper addresses the challenges of complex dependencies and diverse
anomaly patterns in cloud service environments by proposing a dependency
modeling and anomaly detection method that integrates contrastive learning. The
method abstracts service interactions into a dependency graph, extracts
temporal and structural features through embedding functions, and employs a
graph convolution mechanism to aggregate neighborhood information for
context-aware service representations. A contrastive learning framework is then
introduced, constructing positive and negative sample pairs to enhance the
separability of normal and abnormal patterns in the representation space.
Furthermore, a temporal consistency constraint is designed to maintain
representation stability across time steps and reduce the impact of short-term
fluctuations and noise. The overall optimization combines contrastive loss and
temporal consistency loss to ensure stable and reliable detection across
multi-dimensional features. Experiments on public datasets systematically
evaluate the method from hyperparameter, environmental, and data sensitivity
perspectives. Results show that the proposed approach significantly outperforms
existing methods on key metrics such as Precision, Recall, F1-Score, and AUC,
while maintaining robustness under conditions of sparse labeling, monitoring
noise, and traffic fluctuations. This study verifies the effectiveness of
integrating dependency modeling with contrastive learning, provides a complete
technical solution for cloud service anomaly detection, and demonstrates strong
adaptability and stability in complex environments.

</details>


### [35] [Assessing the robustness of heterogeneous treatment effects in survival analysis under informative censoring](https://arxiv.org/abs/2510.13397)
*Yuxin Wang,Dennis Frauen,Jonas Schweisthal,Maresa Schröder,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出一个假设宽松的框架，用于评估生存分析中条件平均治疗效应在存在删失偏差时的稳健性，通过部分识别方法推导CATE的边界，并开发新的元学习器来估计这些边界。


<details>
  <summary>Details</summary>
Motivation: 临床研究中高达一半的患者因副作用等原因提前退出，当退出与生存时间相关时会产生信息性删失，导致治疗效应估计存在偏差。现有方法依赖强假设，需要更稳健的评估框架。

Method: 使用部分识别方法推导CATE的边界，开发新的元学习器来估计边界，该学习器具有双重稳健性和准oracle效率等理论性质。

Result: 数值实验和癌症药物试验应用表明，该框架能识别在信息性删失下治疗仍然有效的患者亚组，提供实用的治疗效应稳健性评估工具。

Conclusion: 该框架为存在删失时评估治疗效应稳健性提供了实用工具，促进了医学和流行病学中生存数据的可靠使用。

Abstract: Dropout is common in clinical studies, with up to half of patients leaving
early due to side effects or other reasons. When dropout is informative (i.e.,
dependent on survival time), it introduces censoring bias, because of which
treatment effect estimates are also biased. In this paper, we propose an
assumption-lean framework to assess the robustness of conditional average
treatment effect (CATE) estimates in survival analysis when facing censoring
bias. Unlike existing works that rely on strong assumptions, such as
non-informative censoring, to obtain point estimation, we use partial
identification to derive informative bounds on the CATE. Thereby, our framework
helps to identify patient subgroups where treatment is effective despite
informative censoring. We further develop a novel meta-learner that estimates
the bounds using arbitrary machine learning models and with favorable
theoretical properties, including double robustness and quasi-oracle
efficiency. We demonstrate the practical value of our meta-learner through
numerical experiments and in an application to a cancer drug trial. Together,
our framework offers a practical tool for assessing the robustness of estimated
treatment effects in the presence of censoring and thus promotes the reliable
use of survival data for evidence generation in medicine and epidemiology.

</details>


### [36] [When Embedding Models Meet: Procrustes Bounds and Applications](https://arxiv.org/abs/2510.13406)
*Lucas Maystre,Alvaro Ortega Gonzalez,Charles Park,Rares Dolga,Tudor Berariu,Yu Zhao,Kamil Ciosek*

Main category: cs.LG

TL;DR: 本文研究嵌入模型的对齐问题，证明当两个嵌入集合的点积近似保持时，存在正交变换能够对齐它们，并提出Procrustes后处理方法来实现嵌入模型的互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决相似数据上分别训练的嵌入模型因缺乏互操作性而带来的实际应用挑战，如模型重训练、部分模型升级和多模态搜索等问题。

Method: 通过理论分析证明点积近似保持时存在对齐等距变换，提出基于Procrustes分析的后处理方法来实现嵌入对齐。

Result: 理论分析给出了对齐误差的紧界，实证研究表明该方法在模型重训练兼容性、文本检索模型组合和多模态搜索中表现优异，达到最先进性能。

Conclusion: Procrustes后处理是一种简单有效的嵌入对齐方法，能够使不同嵌入模型互操作，同时保持各自嵌入空间的几何结构。

Abstract: Embedding models trained separately on similar data often produce
representations that encode stable information but are not directly
interchangeable. This lack of interoperability raises challenges in several
practical applications, such as model retraining, partial model upgrades, and
multimodal search. Driven by these challenges, we study when two sets of
embeddings can be aligned by an orthogonal transformation. We show that if
pairwise dot products are approximately preserved, then there exists an
isometry that closely aligns the two sets, and we provide a tight bound on the
alignment error. This insight yields a simple alignment recipe, Procrustes
post-processing, that makes two embedding models interoperable while preserving
the geometry of each embedding space. Empirically, we demonstrate its
effectiveness in three applications: maintaining compatibility across
retrainings, combining different models for text retrieval, and improving
mixed-modality search, where it achieves state-of-the-art performance.

</details>


### [37] [Hybrid Interval Type-2 Mamdani-TSK Fuzzy System for Regression Analysis](https://arxiv.org/abs/2510.13437)
*Ashish Bhatia,Renato Cordeiro de Amorim,Vito De Feo*

Main category: cs.LG

TL;DR: 提出了一种新颖的模糊回归方法，结合了Mamdani系统的可解释性和TSK模型的精确性，通过混合规则结构和双重支配类型来平衡准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法在处理现实世界数据的复杂性和不确定性方面存在困难，深度学习虽然能捕捉复杂非线性关系但缺乏可解释性且容易在小数据集上过拟合，模糊系统提供了处理不确定性的替代框架。

Method: 提出混合模糊回归方法，采用包含模糊和清晰组件的混合规则结构以及双重支配类型，在保持Mamdani系统可解释性的同时提高TSK模型的精度。

Result: 在6个基准数据集上的评估显示，该方法在4个数据集中获得了最佳模糊方法得分，在2个数据集中优于不透明模型，在1个数据集中获得最佳总体得分，RMSE改进范围从0.4%到19%。

Conclusion: 这种混合方法为预测建模提供了一个平衡且多功能的工具，解决了模糊系统中可解释性和准确性之间的权衡问题。

Abstract: Regression analysis is employed to examine and quantify the relationships
between input variables and a dependent and continuous output variable. It is
widely used for predictive modelling in fields such as finance, healthcare, and
engineering. However, traditional methods often struggle with real-world data
complexities, including uncertainty and ambiguity. While deep learning
approaches excel at capturing complex non-linear relationships, they lack
interpretability and risk over-fitting on small datasets. Fuzzy systems provide
an alternative framework for handling uncertainty and imprecision, with Mamdani
and Takagi-Sugeno-Kang (TSK) systems offering complementary strengths:
interpretability versus accuracy. This paper presents a novel fuzzy regression
method that combines the interpretability of Mamdani systems with the precision
of TSK models. The proposed approach introduces a hybrid rule structure with
fuzzy and crisp components and dual dominance types, enhancing both accuracy
and explainability. Evaluations on benchmark datasets demonstrate
state-of-the-art performance in several cases, with rules maintaining a
component similar to traditional Mamdani systems while improving precision
through improved rule outputs. This hybrid methodology offers a balanced and
versatile tool for predictive modelling, addressing the trade-off between
interpretability and accuracy inherent in fuzzy systems. In the 6 datasets
tested, the proposed approach gave the best fuzzy methodology score in 4
datasets, out-performed the opaque models in 2 datasets and produced the best
overall score in 1 dataset with the improvements in RMSE ranging from 0.4% to
19%.

</details>


### [38] [Rectify and Align GPS Points to Parking Spots via Rank-1 Constraint](https://arxiv.org/abs/2510.13439)
*Jiaxing Deng,Junbiao Pang,Zhicheng Wang,Haitao Yu*

Main category: cs.LG

TL;DR: 提出了一种无监督的低秩方法来校正停车位GPS点的误差，并基于停车位与道路平行的物理约束将其对齐到实际位置。


<details>
  <summary>Details</summary>
Motivation: 高层建筑会导致GPS点偏离停车位实际位置，且低成本GPS设备本身存在定位误差，需要一种无监督方法来校正大量停车位中的错误GPS点。

Method: 利用停车位与道路平行的物理约束，提出无监督低秩方法，在统一框架中同时进行GPS点误差校正和对齐。

Result: 该方法简单有效，适用于各种类型的GPS点误差，实验证明了其解决实际问题的优越性。

Conclusion: 提出的非常规校正和对齐方法能够有效解决停车位GPS点定位问题，数据集和代码已公开。

Abstract: Parking spots are essential components, providing vital mobile resources for
residents in a city. Accurate Global Positioning System (GPS) points of parking
spots are the core data for subsequent applications,e.g., parking management,
parking policy, and urban development. However, high-rise buildings tend to
cause GPS points to drift from the actual locations of parking spots; besides,
the standard lower-cost GPS equipment itself has a certain location error.
Therefore, it is a non-trivial task to correct a few wrong GPS points from a
large number of parking spots in an unsupervised approach. In this paper,
motivated by the physical constraints of parking spots (i.e., parking spots are
parallel to the sides of roads), we propose an unsupervised low-rank method to
effectively rectify errors in GPS points and further align them to the parking
spots in a unified framework. The proposed unconventional rectification and
alignment method is simple and yet effective for any type of GPS point errors.
Extensive experiments demonstrate the superiority of the proposed method to
solve a practical problem. The data set and the code are publicly accessible
at:https://github.com/pangjunbiao/ITS-Parking-spots-Dataset.

</details>


### [39] [Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers](https://arxiv.org/abs/2510.13444)
*Nico Pelleriti,Christoph Spiegel,Shiwei Liu,David Martínez-Rubio,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本文提出首个学习增强算法来验证多项式的平方和(SOS)性质，通过训练Transformer模型预测几乎最小的单项式基，大幅减小相应的半定规划问题规模，实现超过100倍的加速。


<details>
  <summary>Details</summary>
Motivation: 验证多项式的非负性是一个NP难问题，而平方和(SOS)性质是充分条件。但实际中验证SOS标准计算成本高，需要求解维度随单项式基大小二次增长的半定规划。现有方法主要关注减小单项式基规模。

Method: 训练Transformer模型预测给定多项式的几乎最小单项式基，从而大幅减小相应半定规划的规模。方法包括：高效生成超过1亿个SOS多项式的训练数据集、设计和训练相应的Transformer架构、以及理论分析的系统性回退机制。

Result: 在200多个基准数据集上验证，相比最先进求解器实现超过100倍的加速，并能解决竞争方法失败的实例。

Conclusion: 该研究为提升SOS编程的实际可扩展性提供了新的见解，通过机器学习方法显著改善了SOS验证的计算效率。

Abstract: Certifying nonnegativity of polynomials is a well-known NP-hard problem with
direct applications spanning non-convex optimization, control, robotics, and
beyond. A sufficient condition for nonnegativity is the Sum of Squares (SOS)
property, i.e., it can be written as a sum of squares of other polynomials. In
practice, however, certifying the SOS criterion remains computationally
expensive and often involves solving a Semidefinite Program (SDP), whose
dimensionality grows quadratically in the size of the monomial basis of the SOS
expression; hence, various methods to reduce the size of the monomial basis
have been proposed. In this work, we introduce the first learning-augmented
algorithm to certify the SOS criterion. To this end, we train a Transformer
model that predicts an almost-minimal monomial basis for a given polynomial,
thereby drastically reducing the size of the corresponding SDP. Our overall
methodology comprises three key components: efficient training dataset
generation of over 100 million SOS polynomials, design and training of the
corresponding Transformer architecture, and a systematic fallback mechanism to
ensure correct termination, which we analyze theoretically. We validate our
approach on over 200 benchmark datasets, achieving speedups of over $100\times$
compared to state-of-the-art solvers and enabling the solution of instances
where competing approaches fail. Our findings provide novel insights towards
transforming the practical scalability of SOS programming.

</details>


### [40] [$L_2$-Regularized Empirical Risk Minimization Guarantees Small Smooth Calibration Error](https://arxiv.org/abs/2510.13450)
*Masahiro Fujisawa,Futoshi Futami*

Main category: cs.LG

TL;DR: 该论文首次从理论上证明，标准的L2正则化经验风险最小化可以直接控制平滑校准误差，无需后处理校正或专门的校准正则化器。


<details>
  <summary>Details</summary>
Motivation: 预测概率的校准对于可靠的机器学习至关重要，但标准训练程序如何产生良好校准模型的理解仍然不足。

Method: 基于优化误差、正则化强度和Rademacher复杂度，建立了平滑校准误差的有限样本泛化界，并在再生核希尔伯特空间中实例化该理论，为核岭回归和逻辑回归提供具体保证。

Result: 实验证实了这些具体保证，表明L2正则化的ERM可以提供良好校准的模型，无需增强或后处理重新校准。

Conclusion: 标准L2正则化经验风险最小化本身就能有效控制模型校准误差，为可靠的概率预测提供了理论基础。

Abstract: Calibration of predicted probabilities is critical for reliable machine
learning, yet it is poorly understood how standard training procedures yield
well-calibrated models. This work provides the first theoretical proof that
canonical $L_{2}$-regularized empirical risk minimization directly controls the
smooth calibration error (smCE) without post-hoc correction or specialized
calibration-promoting regularizer. We establish finite-sample generalization
bounds for smCE based on optimization error, regularization strength, and the
Rademacher complexity. We then instantiate this theory for models in
reproducing kernel Hilbert spaces, deriving concrete guarantees for kernel
ridge and logistic regression. Our experiments confirm these specific
guarantees, demonstrating that $L_{2}$-regularized ERM can provide a
well-calibrated model without boosting or post-hoc recalibration. The source
code to reproduce all experiments is available at
https://github.com/msfuji0211/erm_calibration.

</details>


### [41] [Towards Blackwell Optimality: Bellman Optimality Is All You Can Get](https://arxiv.org/abs/2510.13476)
*Victor Boone,Adrienne Tuynman*

Main category: cs.LG

TL;DR: 本文研究了马尔可夫决策过程中偏差最优性层次结构的策略识别问题，构建了具有渐近误差消失的学习算法，并确定了能够在有限时间内停止识别的MDP类别。


<details>
  <summary>Details</summary>
Motivation: 平均收益最优性在MDP中过于渐近，需要结合即时损失度量来建立偏差最优性层次结构，直至Blackwell最优性。本文旨在识别具有此类最优性阶次的策略。

Method: 为每个最优性阶次构建学习算法，并开发可追踪的停止规则，当与学习算法结合时，在可能的情况下在有限时间内触发停止。

Result: 确定了MDP类别，其中识别算法可以在有限时间内停止，该类对应于具有唯一Bellman最优策略的MDP，且不依赖于所考虑的最优性阶次。

Conclusion: 本文提供了在偏差最优性层次结构中识别策略的完整框架，包括学习算法和停止条件，为MDP中的最优策略识别提供了理论保证。

Abstract: Although average gain optimality is a commonly adopted performance measure in
Markov Decision Processes (MDPs), it is often too asymptotic. Further
incorporating measures of immediate losses leads to the hierarchy of bias
optimalities, all the way up to Blackwell optimality. In this paper, we
investigate the problem of identifying policies of such optimality orders. To
that end, for each order, we construct a learning algorithm with vanishing
probability of error. Furthermore, we characterize the class of MDPs for which
identification algorithms can stop in finite time. That class corresponds to
the MDPs with a unique Bellman optimal policy, and does not depend on the
optimality order considered. Lastly, we provide a tractable stopping rule that
when coupled to our learning algorithm triggers in finite time whenever it is
possible to do so.

</details>


### [42] [Tahakom LLM guidelines and receipts: from pre-training data to an Arabic LLM](https://arxiv.org/abs/2510.13481)
*Areej AlOtaibi,Lina Alyahya,Raghad Alshabanah,Shahad Alfawzan,Shuruq Alarefei,Reem Alsabti,Nouf Alsubaie,Abdulaziz Alhuzaymi,Lujain Alkhelb,Majd Alsayari,Waad Alahmed,Omar Talabay,Jalal Alowibdi,Salem Alelyani,Adel Bibi*

Main category: cs.LG

TL;DR: 本文探讨了开发阿拉伯语大语言模型（LLMs）面临的独特挑战，重点关注数据整理、分词器设计和评估方法，并分享了相关数据和方法的透明度。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语大语言模型开发面临特殊挑战，需要解决数据整理、分词器设计和评估框架等关键问题，以推动阿拉伯语语言建模的发展。

Method: 采用系统方法收集和过滤阿拉伯语预训练数据集，评估不同分词器设计对模型性能的影响，并提出针对现有阿拉伯语评估框架局限性的纠正方法。

Result: 开发了改进的阿拉伯语数据整理流程和分词器设计，提出了系统化的评估纠正方法，为阿拉伯语LLMs开发提供了实用解决方案。

Conclusion: 通过分享数据和方法的透明度，促进了阿拉伯语语言建模的协作发展，为阿拉伯语LLMs的进步做出了贡献。

Abstract: Large Language Models (LLMs) have significantly advanced the field of natural
language processing, enhancing capabilities in both language understanding and
generation across diverse domains. However, developing LLMs for Arabic presents
unique challenges. This paper explores these challenges by focusing on critical
aspects such as data curation, tokenizer design, and evaluation. We detail our
approach to the collection and filtration of Arabic pre-training datasets,
assess the impact of various tokenizer designs on model performance, and
examine the limitations of existing Arabic evaluation frameworks, for which we
propose a systematic corrective methodology. To promote transparency and
facilitate collaborative development, we share our data and methodologies,
contributing to the advancement of language modeling, particularly for the
Arabic language.

</details>


### [43] [DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation](https://arxiv.org/abs/2510.13497)
*Zexin Wang,Lin Shi,Haoyu Wu,Junru Luo,Xiangzeng Kong,Jun Qi*

Main category: cs.LG

TL;DR: 提出了一种基于CLIP框架的多模态癫痫检测模型DistilCLIP-EEG，整合EEG信号和文本描述，通过知识蒸馏方法训练轻量级学生模型，在多个数据集上达到97%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习癫痫检测方法仅依赖单模态EEG信号，忽视了多模态信息的潜在优势，需要开发能整合多模态信息的模型来提高检测性能。

Method: 基于CLIP框架构建多模态模型，使用Conformer架构的EEG编码器和可学习BERT作为文本编码器，在共享潜在空间进行跨模态表示学习，并引入知识蒸馏方法训练轻量级学生模型。

Result: 在TUSZ、AUBMC和CHB-MIT数据集上，教师模型和学生模型准确率均超过97%，F1分数均高于0.94，学生模型参数量和模型大小约为教师模型的58.1%。

Conclusion: 该模型在EEG癫痫检测方面具有巨大潜力，为在资源受限环境中部署轻量级模型奠定了坚实基础，同时保持了高性能。

Abstract: Epilepsy is a prevalent neurological disorder marked by sudden, brief
episodes of excessive neuronal activity caused by abnormal electrical
discharges, which may lead to some mental disorders. Most existing deep
learning methods for epilepsy detection rely solely on unimodal EEG signals,
neglecting the potential benefits of multimodal information. To address this,
we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIP
framework, which integrates both EEG signals and text descriptions to capture
comprehensive features of epileptic seizures. The model involves an EEG encoder
based on the Conformer architecture as a text encoder, the proposed Learnable
BERT (BERT-LP) as prompt learning within the encoders. Both operate in a shared
latent space for effective cross-modal representation learning. To enhance
efficiency and adaptability, we introduce a knowledge distillation method where
the trained DistilCLIP-EEG serves as a teacher to guide a more compact student
model to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MIT
datasets, both the teacher and student models achieved accuracy rates exceeding
97%. Across all datasets, the F1-scores were consistently above 0.94,
demonstrating the robustness and reliability of the proposed framework.
Moreover, the student model's parameter count and model size are approximately
58.1% of those of the teacher model, significantly reducing model complexity
and storage requirements while maintaining high performance. These results
highlight the potential of our proposed model for EEG-based epilepsy detection
and establish a solid foundation for deploying lightweight models in
resource-constrained settings.

</details>


### [44] [Offline and Online KL-Regularized RLHF under Differential Privacy](https://arxiv.org/abs/2510.13512)
*Yulian Wu,Rushil Thareja,Praneeth Vepakomma,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文研究了在局部差分隐私模型下基于人类反馈的强化学习（RLHF）的离线和在线设置，提出了新的算法并分析了其理论性能。


<details>
  <summary>Details</summary>
Motivation: 研究在保护隐私的前提下进行RLHF训练，特别是在大语言模型对齐中广泛使用的KL正则化目标函数，解决人类偏好标签的隐私保护问题。

Method: 在离线设置中设计了基于悲观主义原则的算法，在在线设置中设计了基于乐观主义的算法，并分析了在局部差分隐私约束下的性能。

Result: 离线设置中获得了$\tilde{O}(1/[(e^\epsilon-1)^2 n])$的次优性差距，并证明了其最优性；在线设置中获得了$O(d_{\mathcal{F}}\log(N_{\mathcal{F}}\cdot T)/(e^\epsilon-1)^2)$的对数遗憾界。

Conclusion: 本文首次在理论上研究了具有LDP的KL正则化RLHF问题，提出的算法在隐私保护下具有良好的理论性能，并为非隐私情况下的在线KL正则化RLHF提供了首个分析。

Abstract: In this paper, we study the offline and online settings of reinforcement
learning from human feedback (RLHF) with KL-regularization -- a widely used
objective function in large language model alignment -- under the $\epsilon$
local differential privacy ($\epsilon$-LDP) model on the label of the human
preference. In the offline setting, we design an algorithm based on the
principle of pessimism and derive a new suboptimality gap of
$\tilde{O}(1/[(e^\epsilon-1)^2 n])$ on the KL-regularized objective under
single-policy concentrability. We also prove its optimality by providing a
matching lower bound where $n$ is the sample size.
  In the online setting, we are the first one to theoretically investigate the
problem of KL-regularized RLHF with LDP. We design an optimism-based algorithm
and derive a logarithmic regret bound of $O(d_{\mathcal{F}}\log
(N_{\mathcal{F}}\cdot T) /(e^\epsilon-1)^2 )$, where $T$ is the total time
step, $N_{\mathcal{F}}$ is cardinality of the reward function space
$\mathcal{F}$ and $d_{\mathcal{F}}$ is a variant of eluder dimension for RLHF.
As a by-product of our analysis, our results also imply the first analysis for
online KL-regularized RLHF without privacy. We implement our algorithm in the
offline setting to verify our theoretical results and release our open source
code at: https://github.com/rushil-thareja/PPKL-RLHF-Official.

</details>


### [45] [K-Merge: Online Continual Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2510.13537)
*Donald Shenaj,Ondrej Bohdal,Taha Ceritli,Mete Ozay,Pietro Zanuttigh,Umberto Michieli*

Main category: cs.LG

TL;DR: 提出了一种在设备存储限制下在线持续合并LoRA适配器的数据无关且计算高效的方法，以支持增量任务需求。


<details>
  <summary>Details</summary>
Motivation: 移动设备上部署大语言模型时，LoRA适配器通常增量交付，需要在有限存储空间下持续合并新适配器同时保持已有任务性能。

Method: 开发了数据无关且计算高效的策略，在新LoRA可用时选择和合并适配器，仅存储有限数量的适配器。

Result: 在真实世界任务上的广泛实验表明，该方法在遵守设备存储预算和计算限制的同时，优于其他替代策略。

Conclusion: 该方法有效解决了移动设备上LoRA适配器的在线持续合并问题，在资源受限环境下实现了良好的性能保持。

Abstract: On-device deployment of Large Language Models (LLMs) frequently leverages
Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight
resource constraints. To address the limited storage capacity of mobile
devices, recent works have explored model merging techniques to fuse multiple
LoRAs into a single one. In practice, however, LoRAs are often delivered
incrementally, as users request support for new tasks (e.g., novel problem
types or languages). This scenario introduces a new challenge: on-device online
continual merging, where the objective is to incorporate new LoRAs while
preserving the performance on previously supported tasks. In this paper, we
propose a data-free and computationally efficient strategy for selecting and
merging LoRAs when a new one becomes available, assuming the device can store
only a limited number of adapters. Extensive experiments across real-world
tasks demonstrate the superiority of our approach compared to alternative
strategies while adhering to the storage budget and compute limitations of
on-device settings.

</details>


### [46] [Multi-Objective $\textit{min-max}$ Online Convex Optimization](https://arxiv.org/abs/2510.13560)
*Rahul Vaze,Sumiran Mishra*

Main category: cs.LG

TL;DR: 本文提出了多目标在线凸优化（multi-objective OCO）问题，考虑K个不同的损失函数序列，采用min-max遗憾作为性能度量。在i.i.d.输入设置下，提出了结合Hedge和在线梯度下降（OGD）的简单算法，证明了其期望min-max遗憾为O(√(T log K))。


<details>
  <summary>Details</summary>
Motivation: 扩展传统在线凸优化（OCO）的视野，考虑多目标场景，其中存在K个不同的损失函数序列。为了捕捉跟踪这K个不同序列之间的权衡，采用min-max遗憾作为性能度量，这是一个严格的标准，要求算法在所有时间都能紧密跟踪所有损失函数序列。

Method: 在i.i.d.输入设置下（所有损失函数从未知分布中独立同分布生成），提出了一个简单算法，结合了著名的Hedge算法和在线梯度下降（OGD）。通过一个非常简单的证明来分析该算法的性能。

Result: 证明了所提出算法的期望min-max遗憾为O(√(T log K))，其中T是时间范围，K是损失函数序列的数量。

Conclusion: 本文成功地将传统OCO扩展到多目标设置，提出了有效的算法来解决min-max遗憾问题，并在i.i.d.模型下获得了最优的遗憾界限。

Abstract: In online convex optimization (OCO), a single loss function sequence is
revealed over a time horizon of $T$, and an online algorithm has to choose its
action at time $t$, before the loss function at time $t$ is revealed. The goal
of the online algorithm is to incur minimal penalty (called $\textit{regret}$
compared to a static optimal action made by an optimal offline algorithm
knowing all functions of the sequence in advance.
  In this paper, we broaden the horizon of OCO, and consider multi-objective
OCO, where there are $K$ distinct loss function sequences, and an algorithm has
to choose its action at time $t$, before the $K$ loss functions at time $t$ are
revealed. To capture the tradeoff between tracking the $K$ different sequences,
we consider the $\textit{min-max}$ regret, where the benchmark (optimal offline
algorithm) takes a static action across all time slots that minimizes the
maximum of the total loss (summed across time slots) incurred by each of the
$K$ sequences. An online algorithm is allowed to change its action across time
slots, and its {\it min-max} regret is defined as the difference between its
$\textit{min-max}$ cost and that of the benchmark. The $\textit{min-max}$
regret is a stringent performance measure and an algorithm with small regret
needs to `track' all loss function sequences closely at all times.
  We consider this $\textit{min-max}$ regret in the i.i.d. input setting where
all loss functions are i.i.d. generated from an unknown distribution. For the
i.i.d. model we propose a simple algorithm that combines the well-known
$\textit{Hedge}$ and online gradient descent (OGD) and show via a remarkably
simple proof that its expected $\textit{min-max}$ regret is $O(\sqrt{T \log
K})$.

</details>


### [47] [DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning](https://arxiv.org/abs/2510.13567)
*Omayma Moussadek,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 提出DOLFIN方法，结合Vision Transformers和低秩适配器，在联邦持续学习中实现高效稳定的新任务学习，同时最小化通信开销和防止遗忘。


<details>
  <summary>Details</summary>
Motivation: 当前联邦持续学习方法在平衡性能、隐私保护和通信效率方面面临挑战，需要一种更有效的解决方案。

Method: 使用低秩适配器(LoRA)减少通信开销，结合DualGradient Projection Memory(DualGPM)防止遗忘，在Vision Transformers架构上实现分布式在线学习。

Result: 在CIFAR-100、ImageNet-R、ImageNet-A和CUB-200数据集上，DOLFIN在两种Dirichlet异构设置下持续超越六个强基线方法，在最终平均准确率上表现优异，同时保持相同的内存占用。

Conclusion: 正交低秩适配器为联邦设置中的隐私保护持续学习提供了有效且可扩展的解决方案。

Abstract: Federated continual learning (FCL) enables models to learn new tasks across
multiple distributed clients, protecting privacy and without forgetting
previously acquired knowledge. However, current methods face challenges
balancing performance, privacy preservation, and communication efficiency. We
introduce a Distributed Online LoRA for Federated INcremental learning method
DOLFIN, a novel approach combining Vision Transformers with low-rank adapters
designed to efficiently and stably learn new tasks in federated environments.
Our method leverages LoRA for minimal communication overhead and incorporates
DualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on
CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet
heterogeneity settings, DOLFIN consistently surpasses six strong baselines in
final average accuracy while matching their memory footprint. Orthogonal
low-rank adapters offer an effective and scalable solution for
privacy-preserving continual learning in federated settings.

</details>


### [48] [ArtNet: Hierarchical Clustering-Based Artificial Netlist Generator for ML and DTCO Application](https://arxiv.org/abs/2510.13582)
*Andrew B. Kahng. Seokhyeong Kang,Seonghyeon Park,Dooseok Yoon*

Main category: cs.LG

TL;DR: ArtNet是一个人工网表生成器，通过生成多样化的人工数据集来增强机器学习模型的泛化能力，支持设计技术协同优化（DTCO）中的设计空间探索，在PPA优化方面取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 在先进节点中，功率、性能和面积（PPA）优化变得高度复杂和具有挑战性。机器学习和设计技术协同优化面临训练数据不足和设计流程周转时间长的限制。

Method: 提出ArtNet人工网表生成器，复制关键拓扑特征，生成与现实设计参数匹配的人工数据集，用于增强ML模型训练和DTCO设计空间探索。

Result: 在基于CNN的DRV预测中，ArtNet数据增强使F1分数比仅使用原始数据集提高了0.16；在DTCO环境中，ArtNet生成的mini-brains与目标全规模块设计的PPA匹配度达到97.94%。

Conclusion: ArtNet通过生成现实的人工数据集，有效解决了ML和DTCO中的数据多样性不足问题，显著提升了PPA优化效率和设计探索能力。

Abstract: In advanced nodes, optimization of power, performance and area (PPA) has
become highly complex and challenging. Machine learning (ML) and
design-technology co-optimization (DTCO) provide promising mitigations, but
face limitations due to a lack of diverse training data as well as long design
flow turnaround times (TAT). We propose ArtNet, a novel artificial netlist
generator designed to tackle these issues. Unlike previous methods, ArtNet
replicates key topological characteristics, enhancing ML model generalization
and supporting broader design space exploration for DTCO. By producing
realistic artificial datasets that moreclosely match given target parameters,
ArtNet enables more efficient PPAoptimization and exploration of flows and
design enablements. In the context of CNN-based DRV prediction, ArtNet's data
augmentationimproves F1 score by 0.16 compared to using only the original
(real) dataset. In the DTCO context, ArtNet-generated mini-brains achieve a PPA
match up to 97.94%, demonstrating close alignment with design metrics of
targeted full-scale block designs.

</details>


### [49] [EEGChaT: A Transformer-Based Modular Channel Selector for SEEG Analysis](https://arxiv.org/abs/2510.13592)
*Chen Wang,Yansen Wang,Dongqi Han,Zilong Wang,Dongsheng Li*

Main category: cs.LG

TL;DR: 提出了EEGChaT，一种基于Transformer的通道选择模块，用于自动识别SEEG记录中最相关的通道，通过通道聚合令牌和改进的注意力展开技术计算可解释的通道重要性分数。


<details>
  <summary>Details</summary>
Motivation: SEEG信号分析对脑机接口和神经科学研究至关重要，但由于输入通道数量大且相关性异质，传统通道选择方法难以扩展或提供有意义的可解释性。

Method: EEGChaT引入通道聚合令牌来聚合跨通道信息，并利用改进的注意力展开技术计算可解释的定量通道重要性分数。

Result: 在DuIN数据集上的评估显示，EEGChaT与现有分类模型集成能持续提高解码准确率，实现高达17%的绝对增益，且产生的通道权重与手动选择的通道有显著重叠。

Conclusion: EEGChaT是高维SEEG分析中通道选择的有效且可推广解决方案，既提升了性能又提供了对神经信号相关性的洞察。

Abstract: Analyzing stereoelectroencephalography (SEEG) signals is critical for
brain-computer interface (BCI) applications and neuroscience research, yet
poses significant challenges due to the large number of input channels and
their heterogeneous relevance. Traditional channel selection methods struggle
to scale or provide meaningful interpretability for SEEG data. In this work, we
propose EEGChaT, a novel Transformer-based channel selection module designed to
automatically identify the most task-relevant channels in SEEG recordings.
EEGChaT introduces Channel Aggregation Tokens (CATs) to aggregate information
across channels, and leverages an improved Attention Rollout technique to
compute interpretable, quantitative channel importance scores. We evaluate
EEGChaT on the DuIN dataset, demonstrating that integrating EEGChaT with
existing classification models consistently improves decoding accuracy,
achieving up to 17\% absolute gains. Furthermore, the channel weights produced
by EEGChaT show substantial overlap with manually selected channels, supporting
the interpretability of the approach. Our results suggest that EEGChaT is an
effective and generalizable solution for channel selection in high-dimensional
SEEG analysis, offering both enhanced performance and insights into neural
signal relevance.

</details>


### [50] [Physics-augmented Multi-task Gaussian Process for Modeling Spatiotemporal Dynamics](https://arxiv.org/abs/2510.13601)
*Xizhuo Zhang,Bing Yao*

Main category: cs.LG

TL;DR: 该论文提出了一个物理增强的多任务高斯过程框架，用于处理复杂几何域上的高维时空数据建模，特别针对心脏电动力学建模任务，通过结合物理规律和几何先验显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 随着传感和成像技术的进步，复杂几何域上的高维时空数据收集变得可能，但由于不规则空间结构、快速时间动态以及需要联合预测多个相关物理变量，有效建模此类数据仍然具有挑战性。

Method: 开发了一个几何感知的多任务高斯过程模型来捕捉内在时空结构和任务间依赖关系，并通过基于物理的规则化方案融入控制物理定律，使预测与动态原理保持一致。

Result: 在3D心脏电动力学建模任务上的数值实验表明，该方法通过有效整合领域特定的物理约束和几何先验，显著提高了预测精度，优于现有方法。

Conclusion: 提出的物理增强多任务高斯过程框架能够有效建模复杂几何域上的时空动态系统，通过结合物理知识和数据驱动方法实现了更准确和鲁棒的预测。

Abstract: Recent advances in sensing and imaging technologies have enabled the
collection of high-dimensional spatiotemporal data across complex geometric
domains. However, effective modeling of such data remains challenging due to
irregular spatial structures, rapid temporal dynamics, and the need to jointly
predict multiple interrelated physical variables. This paper presents a
physics-augmented multi-task Gaussian Process (P-M-GP) framework tailored for
spatiotemporal dynamic systems. Specifically, we develop a geometry-aware,
multi-task Gaussian Process (M-GP) model to effectively capture intrinsic
spatiotemporal structure and inter-task dependencies. To further enhance the
model fidelity and robustness, we incorporate governing physical laws through a
physics-based regularization scheme, thereby constraining predictions to be
consistent with governing dynamical principles. We validate the proposed P-M-GP
framework on a 3D cardiac electrodynamics modeling task. Numerical experiments
demonstrate that our method significantly improves prediction accuracy over
existing methods by effectively incorporating domain-specific physical
constraints and geometric prior.

</details>


### [51] [Towards Robust Knowledge Removal in Federated Learning with High Data Heterogeneity](https://arxiv.org/abs/2510.13606)
*Riccardo Santi,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 提出一种基于任务算术和神经正切核的创新方法，用于快速移除客户端对联邦学习模型的影响，解决现有知识移除方法需要多轮通信导致模型不可用的问题。


<details>
  <summary>Details</summary>
Motivation: 随着便携设备收集大量数据并具备计算能力，联邦学习得以发展，但隐私法规要求必须能够移除特定客户端对模型的贡献。现有方法需要多轮通信，导致模型在移除过程中不可用，影响系统服务。

Method: 基于任务算术和神经正切核的创新解决方案，通过数学运算快速消除客户端对模型的影响，避免多轮通信需求。

Result: 该方法能够快速移除客户端贡献，满足时效性要求，同时保持模型的有效性。

Conclusion: 提出的方法解决了联邦学习中客户端贡献移除的时效性问题，为隐私保护下的模型管理提供了高效解决方案。

Abstract: Nowdays, there are an abundance of portable devices capable of collecting
large amounts of data and with decent computational power. This opened the
possibility to train AI models in a distributed manner, preserving the
participating clients' privacy. However, because of privacy regulations and
safety requirements, elimination upon necessity of a client contribution to the
model has become mandatory. The cleansing process must satisfy specific
efficacy and time requirements. In recent years, research efforts have produced
several knowledge removal methods, but these require multiple communication
rounds between the data holders and the process coordinator. This can cause the
unavailability of an effective model up to the end of the removal process,
which can result in a disservice to the system users. In this paper, we
introduce an innovative solution based on Task Arithmetic and the Neural
Tangent Kernel, to rapidly remove a client's influence from a model.

</details>


### [52] [Multivariate Time Series Forecasting with Gate-Based Quantum Reservoir Computing on NISQ Hardware](https://arxiv.org/abs/2510.13634)
*Wissal Hamhoum,Soumaya Cherkaoui,Jean-Frederic Laprade,Ola Ahmed,Shengrui Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于门电路的量子储层计算方法（MTS-QRC），专门针对多元时间序列预测，在NISQ硬件上实现了与经典方法相媲美的性能，并发现设备噪声在某些情况下可以起到正则化作用。


<details>
  <summary>Details</summary>
Motivation: 现有量子储层计算研究主要针对单变量信号，且忽视了近期硬件约束。本文旨在开发适合当前量子设备连接性和深度的多元时间序列量子储层计算方法。

Method: 采用基于门电路的量子储层计算，配对注入和记忆量子比特，使用特罗特化的最近邻横向场伊辛演化，优化以适应当前设备的连接性和深度限制。

Result: 在Lorenz-63和ENSO数据集上分别获得0.0087和0.0036的均方误差，在Lorenz上与经典储层计算性能相当，在两个数据集上都优于学习的RNN。在IBM Heron R2上，MTS-QRC在现实深度下保持准确性，在ENSO上甚至优于无噪声模拟器。

Conclusion: 研究结果支持基于门电路的量子储层计算在NISQ硬件上进行多元时间序列预测的实用性，并激励系统研究硬件噪声何时以及如何有益于量子储层计算的读出。

Abstract: Quantum reservoir computing (QRC) offers a hardware-friendly approach to
temporal learning, yet most studies target univariate signals and overlook
near-term hardware constraints. This work introduces a gate-based QRC for
multivariate time series (MTS-QRC) that pairs injection and memory qubits and
uses a Trotterized nearest-neighbor transverse-field Ising evolution optimized
for current device connectivity and depth. On Lorenz-63 and ENSO, the method
achieves a mean square error (MSE) of 0.0087 and 0.0036, respectively,
performing on par with classical reservoir computing on Lorenz and above
learned RNNs on both, while NVAR and clustered ESN remain stronger on some
settings. On IBM Heron R2, MTS-QRC sustains accuracy with realistic depths and,
interestingly, outperforms a noiseless simulator on ENSO; singular value
analysis indicates that device noise can concentrate variance in feature
directions, acting as an implicit regularizer for linear readout in this
regime. These findings support the practicality of gate-based QRC for MTS
forecasting on NISQ hardware and motivate systematic studies on when and how
hardware noise benefits QRC readouts.

</details>


### [53] [What is the objective of reasoning with reinforcement learning?](https://arxiv.org/abs/2510.13651)
*Damek Davis,Benjamin Recht*

Main category: cs.LG

TL;DR: 该论文表明几种流行的强化学习算法可以视为在正确概率的单调变换上的随机梯度上升，其中拒绝采样对应对数变换，GRPO算法对应反正弦平方根变换。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型中基于二元奖励的强化学习算法的数学本质，建立不同算法之间的统一理论框架。

Method: 通过数学分析将拒绝采样和GRPO等算法重新解释为在正确概率的单调变换上的随机梯度上升过程。

Result: 发现拒绝采样算法对应对数变换，GRPO算法对应反正弦平方根变换，建立了算法与数学变换之间的对应关系。

Conclusion: 多种强化学习算法本质上是相同优化框架的不同实现，为理解和改进这些算法提供了统一的数学视角。

Abstract: We show that several popular algorithms for reinforcement learning in large
language models with binary rewards can be viewed as stochastic gradient ascent
on a monotone transform of the probability of a correct answer given a prompt.
In particular, the transformation associated with rejection sampling algorithms
is the logarithm and that associated with the GRPO algorithm is the arcsine of
the square root.

</details>


### [54] [Time Series Foundation Models: Benchmarking Challenges and Requirements](https://arxiv.org/abs/2510.13654)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Oliver Müller*

Main category: cs.LG

TL;DR: 本文分析了时间序列基础模型（TSFMs）评估中存在的多个挑战，包括基准数据集代表性不足、缺乏时空评估、信息泄露风险以及外部冲击导致的全局模式记忆问题，呼吁开发更稳健的评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列基础模型的兴起，需要解决其评估过程中存在的挑战，特别是避免重蹈大型语言模型和传统时间序列基准测试中已观察到的陷阱，确保评估的完整性。

Method: 通过调查现有TSFM评估方法，识别出数据划分混淆、基准数据集代表性不足、缺乏时空评估、信息泄露风险以及外部冲击导致的全局模式记忆等问题。

Result: 研究发现现有评估方法存在广泛的数据划分混淆问题，可能导致性能估计膨胀和全局知识错误转移到局部时间序列。

Conclusion: 需要开发稳健的评估方法，如基于真正样本外未来数据的评估，以保障TSFM评估的完整性，并呼吁研究社区设计新的原则性方法。

Abstract: Time Series Foundation Models (TSFMs) represent a new paradigm for time
series forecasting, offering zero-shot forecasting capabilities without the
need for domain-specific pre-training or fine-tuning. However, as with Large
Language Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive
training sets, it becomes more and more challenging to ensure the integrity of
benchmarking data. Our investigation of existing TSFM evaluation highlights
multiple challenges, ranging from the representativeness of the benchmark
datasets, over the lack of spatiotemporal evaluation, to risks of information
leakage due to overlapping and obscure datasets, and the memorization of global
patterns caused by external shocks like economic crises or pandemics. Our
findings reveal widespread confusion regarding data partitions, risking
inflated performance estimates and incorrect transfer of global knowledge to
local time series. We argue for the development of robust evaluation
methodologies to prevent pitfalls already observed in LLM and classical time
series benchmarking, and call upon the research community to design new,
principled approaches, such as evaluations on truly out-of-sample future data,
to safeguard the integrity of TSFM assessment.

</details>


### [55] [Adam or Gauss-Newton? A Comparative Study In Terms of Basis Alignment and SGD Noise](https://arxiv.org/abs/2510.13680)
*Bingbin Liu,Rachit Bansal,Depen Morwani,Nikhil Vyas,David Alvarez-Melis,Sham M. Kakade*

Main category: cs.LG

TL;DR: 本文比较了基于Adam和高斯牛顿(GN)的两种对角预条件方法，分析了预条件基选择和梯度噪声的影响。理论分析表明，在全批次设置下，Adam在某些情况下优于GN方法；在随机设置下，Adam与GN^{-1/2}在特定条件下表现相似。


<details>
  <summary>Details</summary>
Motivation: 对角预条件器作为二阶优化器的计算可行近似，在深度学习模型训练中显示出显著加速潜力。Adam和GN是两种主要方法，但它们在预条件基选择和梯度噪声影响方面的比较研究不足。

Method: 通过理论分析在二次目标和逻辑回归上比较Adam和GN方法，考虑四种不同象限情况。使用全批次和随机设置进行对比，并在凸和非凸目标上进行实证研究。

Result: 理论分析表明，无论预条件基如何选择，在全批次设置下都存在Adam优于GN^{-1}和GN^{-1/2}的情况。在随机设置下，在特定假设下Adam与GN^{-1/2}表现相似。实证研究支持了这些理论发现。

Conclusion: Adam和GN对角预条件方法各有优势，选择取决于具体设置。在全批次情况下Adam可能更优，而在随机设置下Adam与GN^{-1/2}有相似行为。这些发现为深度学习优化器的选择提供了理论指导。

Abstract: Diagonal preconditioners are computationally feasible approximate to
second-order optimizers, which have shown significant promise in accelerating
training of deep learning models. Two predominant approaches are based on Adam
and Gauss-Newton (GN) methods: the former leverages statistics of current
gradients and is the de-factor optimizers for neural networks, and the latter
uses the diagonal elements of the Gauss-Newton matrix and underpins some of the
recent diagonal optimizers such as Sophia.
  In this work, we compare these two diagonal preconditioning methods through
the lens of two key factors: the choice of basis in the preconditioner, and the
impact of gradient noise from mini-batching. To gain insights, we analyze these
optimizers on quadratic objectives and logistic regression under all four
quadrants. We show that regardless of the basis, there exist instances where
Adam outperforms both GN$^{-1}$ and GN$^{-1/2}$ in full-batch settings.
Conversely, in the stochastic regime, Adam behaves similarly to GN$^{-1/2}$ for
linear regression under a Gaussian data assumption. These theoretical results
are supported by empirical studies on both convex and non-convex objectives.

</details>


### [56] [Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking](https://arxiv.org/abs/2510.13694)
*Yuchun Miao,Liang Ding,Sen Zhang,Rong Bao,Lefei Zhang,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出了InfoRM和IBL框架来解决RLHF中的奖励过优化问题，通过信息瓶颈原理过滤偏好无关信息，并使用分布级正则化防止策略偏离。


<details>
  <summary>Details</summary>
Motivation: 尽管RLHF在使语言模型与人类价值观对齐方面取得成功，但奖励过优化（奖励黑客攻击）仍然是主要挑战，存在奖励模型过拟合和缺乏合适正则化两个关键障碍。

Method: 提出基于信息瓶颈原理的InfoRM奖励建模框架来过滤偏好无关信息，并引入IBL分布级正则化惩罚在IB潜在空间中的异常偏离，同时提出MOP统计指标来量化奖励过优化严重程度。

Result: 在多种LLM和数据集上的广泛实验证实了发现的一般性，InfoRM和IBL的有效性，以及MOP作为诊断工具的可靠性。

Conclusion: 这些方法共同推进了RLHF的技术水平，有效缓解了奖励过优化问题，为超参数调优和在线缓解提供了原则性方法。

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
aligning language models with human values, reward hacking-or reward
over-optimization-remains a major challenge. We identify two key obstacles to
its mitigation: (1) reward misgeneralization in reward modeling, where reward
models overfit to spurious, preference-irrelevant features; and (2) the lack of
suitable regularization during RL optimization, as existing token-level
constraints often over-restrict the policy space. To address these issues, we
propose InfoRM, an information-theoretic reward modeling framework based on the
Information Bottleneck (IB) principle, which filters out preference-irrelevant
information to alleviate reward misgeneralization. We further observe that
reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent
space, measured by Mahalanobis distance from the SFT-induced distribution.
Motivated by this, we introduce IBL, a distribution-level regularization that
penalizes such deviations, effectively expanding the optimization landscape
while maintaining alignment. We prove that IBL is theoretically equivalent to
the pessimistic RL objective within the IB latent space. Finally, we present
Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying
reward hacking severity, enabling principled hyperparameter tuning and online
mitigation such as early stopping. Extensive experiments across diverse LLMs
and datasets confirm the generality of our findings, the effectiveness of
InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively
advancing the state of RLHF.

</details>


### [57] [Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents](https://arxiv.org/abs/2510.13704)
*Johan Obando-Ceron,Walter Mayor,Samuel Lavoie,Scott Fujimoto,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文提出使用单纯形嵌入来加速强化学习训练，通过在表示层中引入几何归纳偏置，生成稀疏离散特征，从而稳定critic引导和增强策略梯度，在不损失运行速度的情况下提高样本效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模环境并行化方法虽然能加速actor-critic方法的训练时间，但有时仍需要大量环境交互才能达到期望性能水平。注意到良好结构的表示可以改善深度强化学习的泛化能力和样本效率。

Method: 提出使用单纯形嵌入：轻量级表示层，将嵌入约束到单纯形结构中。这种几何归纳偏置产生稀疏和离散特征，稳定critic引导并增强策略梯度。

Result: 当应用于FastTD3、FastSAC和PPO算法时，单纯形嵌入在各种连续和离散控制环境中一致提高了样本效率和最终性能，且没有损失运行速度。

Conclusion: 单纯形嵌入是一种有效的表示学习方法，能够在不牺牲运行效率的前提下显著提升强化学习算法的性能表现。

Abstract: Recent works have proposed accelerating the wall-clock training time of
actor-critic methods via the use of large-scale environment parallelization;
unfortunately, these can sometimes still require large number of environment
interactions to achieve a desired level of performance. Noting that
well-structured representations can improve the generalization and sample
efficiency of deep reinforcement learning (RL) agents, we propose the use of
simplicial embeddings: lightweight representation layers that constrain
embeddings to simplicial structures. This geometric inductive bias results in
sparse and discrete features that stabilize critic bootstrapping and strengthen
policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial
embeddings consistently improve sample efficiency and final performance across
a variety of continuous- and discrete-control environments, without any loss in
runtime speed.

</details>


### [58] [Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe](https://arxiv.org/abs/2510.13713)
*Christophe Roux,Max Zimmer,Alexandre d'Aspremont,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本文提出了一种基于Frank-Wolfe算法的LLM剪枝方法，通过凸松弛技术解决剪枝掩码的硬组合优化问题，显著减少了层间剪枝误差，在GPT架构上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法需要重新训练模型来恢复性能损失，而现有LLM剪枝方法使用贪婪启发式算法，忽略了权重交互，导致剪枝效果不理想。本文旨在解决剪枝掩码这一硬组合优化问题。

Method: 采用凸松弛技术将组合约束转化为连续优化问题，使用Frank-Wolfe算法求解，并通过舍入获得原始组合问题的近似解。

Result: 该方法大幅减少了层间剪枝误差，在GPT架构上优于强基线方法，且保持内存效率。

Conclusion: Frank-Wolfe算法结合凸松弛技术为LLM剪枝提供了有效的解决方案，理论分析证明了该方法能够获得原始组合问题的近似最优解。

Abstract: Pruning is a common technique to reduce the compute and storage requirements
of Neural Networks. While conventional approaches typically retrain the model
to recover pruning-induced performance degradation, state-of-the-art Large
Language Model (LLM) pruning methods operate layer-wise, minimizing the
per-layer pruning error on a small calibration dataset to avoid full
retraining, which is considered computationally prohibitive for LLMs. However,
finding the optimal pruning mask is a hard combinatorial problem and solving it
to optimality is intractable. Existing methods hence rely on greedy heuristics
that ignore the weight interactions in the pruning objective. In this work, we
instead consider the convex relaxation of these combinatorial constraints and
solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method
drastically reduces the per-layer pruning error, outperforms strong baselines
on state-of-the-art GPT architectures, and remains memory-efficient. We provide
theoretical justification by showing that, combined with the convergence
guarantees of the FW algorithm, we obtain an approximate solution to the
original combinatorial problem upon rounding the relaxed solution to
integrality.

</details>


### [59] [Assessing the Geographic Generalization and Physical Consistency of Generative Models for Climate Downscaling](https://arxiv.org/abs/2510.13722)
*Carlo Saccardi,Maximilian Pierzyna,Haitz Sáez de Ocáriz Borde,Simone Monaco,Cristian Meo,Pietro Liò,Rudolf Saathof,Geethu Joseph,Justin Dauwels*

Main category: cs.LG

TL;DR: 论文评估了深度学习模型在千米尺度天气数据降尺度中的表现，发现现有模型在地理泛化和物理一致性方面存在不足，并提出使用功率谱密度损失函数来改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 千米尺度天气数据对实际应用至关重要，但传统天气模拟计算成本高。深度学习模型提供了更快的替代方案，但其可靠性仍受质疑，因为评估通常基于标准机器学习指标而非大气物理洞察。

Method: 论文对最先进的深度学习模型进行基准测试，引入基于物理的诊断方法来评估性能，特别关注地理泛化和物理一致性。实验发现模型在训练区域外泛化能力差，无法准确捕捉二阶变量。

Result: 实验显示，即使在训练区域内，模型也难以产生物理一致的预测。提出的功率谱密度损失函数经验性地改善了地理泛化能力，促进了小尺度物理结构的重建。

Conclusion: 现有深度学习模型在天气降尺度任务中仍存在地理泛化和物理一致性问题，功率谱密度损失函数是一个有效的初步解决方案，但需要进一步研究来确保模型的物理可靠性。

Abstract: Kilometer-scale weather data is crucial for real-world applications but
remains computationally intensive to produce using traditional weather
simulations. An emerging solution is to use deep learning models, which offer a
faster alternative for climate downscaling. However, their reliability is still
in question, as they are often evaluated using standard machine learning
metrics rather than insights from atmospheric and weather physics. This paper
benchmarks recent state-of-the-art deep learning models and introduces
physics-inspired diagnostics to evaluate their performance and reliability,
with a particular focus on geographic generalization and physical consistency.
Our experiments show that, despite the seemingly strong performance of models
such as CorrDiff, when trained on a limited set of European geographies (e.g.,
central Europe), they struggle to generalize to other regions such as Iberia,
Morocco in the south, or Scandinavia in the north. They also fail to accurately
capture second-order variables such as divergence and vorticity derived from
predicted velocity fields. These deficiencies appear even in in-distribution
geographies, indicating challenges in producing physically consistent
predictions. We propose a simple initial solution: introducing a power spectral
density loss function that empirically improves geographic generalization by
encouraging the reconstruction of small-scale physical structures. The code for
reproducing the experimental results can be found at
https://github.com/CarloSaccardi/PSD-Downscaling

</details>


### [60] [Asymptotically optimal reinforcement learning in Block Markov Decision Processes](https://arxiv.org/abs/2510.13748)
*Thomas van Vuren,Fiona Sloothaak,Maarten G. Wolf,Jaron Sanders*

Main category: cs.LG

TL;DR: 本文提出了一种针对块马尔可夫决策过程（BMDPs）的两阶段强化学习算法，通过聚类学习潜在状态结构，显著提高了学习效率，将遗憾从O(√T+n²)改进到O(√T+n)，并在该类BMDPs上达到渐近最优。


<details>
  <summary>Details</summary>
Motivation: 高维状态和动作空间使得强化学习在许多实际场景中不实用，但许多环境具有可利用的结构。BMDPs建模了具有大观测空间但转移动态完全由潜在状态决定的问题，通过恢复潜在结构可以加速学习。

Method: 采用两阶段强化学习算法：第一阶段通过随机探索学习潜在结构，第二阶段切换到适应已发现结构的乐观引导策略。

Result: 算法在可聚类的BMDPs类上实现了O(√T+n)的遗憾，其中T是时间步数，n是观测空间基数，这比之前最好的O(√T+n²)界限有显著改进。

Conclusion: 该算法在可聚类的BMDPs类上实现了渐近最优性，证明了准确估计潜在状态确实能有效加速学习，且没有算法能在该类BMDPs上实现更低的遗憾。

Abstract: The curse of dimensionality renders Reinforcement Learning (RL) impractical
in many real-world settings with exponentially large state and action spaces.
Yet, many environments exhibit exploitable structure that can accelerate
learning. To formalize this idea, we study RL in Block Markov Decision
Processes (BMDPs). BMDPs model problems with large observation spaces, but
where transition dynamics are fully determined by latent states. Recent
advances in clustering methods have enabled the efficient recovery of this
latent structure. However, a regret analysis that exploits these techniques to
determine their impact on learning performance remained open. We are now
addressing this gap by providing a regret analysis that explicitly leverages
clustering, demonstrating that accurate latent state estimation can indeed
effectively speed up learning.
  Concretely, this paper analyzes a two-phase RL algorithm for BMDPs that first
learns the latent structure through random exploration and then switches to an
optimism-guided strategy adapted to the uncovered structure. This algorithm
achieves a regret that is $O(\sqrt{T}+n)$ on a large class of BMDPs susceptible
to clustering. Here, $T$ denotes the number of time steps, $n$ is the
cardinality of the observation space, and the Landau notation $O(\cdot)$ holds
up to constants and polylogarithmic factors. This improves the best prior
bound, $O(\sqrt{T}+n^2)$, especially when $n$ is large. Moreover, we prove that
no algorithm can achieve lower regret uniformly on this same class of BMDPs.
This establishes that, on this class, the algorithm achieves asymptotic
optimality.

</details>


### [61] [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/abs/2510.13786)
*Devvrit Khatri,Lovish Madaan,Rishabh Tiwari,Rachit Bansal,Sai Surya Duvvuri,Manzil Zaheer,Inderjit S. Dhillon,David Brandfonbrener,Rishabh Agarwal*

Main category: cs.LG

TL;DR: 本文提出了首个大规模系统研究，建立了强化学习在大型语言模型中的可预测扩展框架，通过超过40万GPU小时的实验定义了计算-性能曲线，并提出了最佳实践配方ScaleRL。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为训练大型语言模型的核心技术，但该领域缺乏与预训练相当的预测性扩展方法。尽管计算预算迅速增加，但如何评估算法改进以扩展强化学习计算仍缺乏原则性理解。

Method: 进行大规模系统研究（超过40万GPU小时），拟合强化学习训练的S型计算-性能曲线，消融分析各种常见设计选择对渐进性能和计算效率的影响。

Result: 研究发现：不同配方产生不同的渐进性能；损失聚合、归一化、课程学习和离策略算法等细节主要调节计算效率而不显著改变渐进值；稳定可扩展的配方遵循可预测的扩展轨迹。

Conclusion: 提出了最佳实践配方ScaleRL，并在单次强化学习运行中成功扩展到10万GPU小时并预测验证性能。该工作为分析强化学习扩展提供了科学框架，使强化学习训练更接近预训练长期实现的预测性。

Abstract: Reinforcement learning (RL) has become central to training large language
models (LLMs), yet the field lacks predictive scaling methodologies comparable
to those established for pre-training. Despite rapidly rising compute budgets,
there is no principled understanding of how to evaluate algorithmic
improvements for scaling RL compute. We present the first large-scale
systematic study, amounting to more than 400,000 GPU-hours, that defines a
principled framework for analyzing and predicting RL scaling in LLMs. We fit
sigmoidal compute-performance curves for RL training and ablate a wide range of
common design choices to analyze their effects on asymptotic performance and
compute efficiency. We observe: (1) Not all recipes yield similar asymptotic
performance, (2) Details such as loss aggregation, normalization, curriculum,
and off-policy algorithm primarily modulate compute efficiency without
materially shifting the asymptote, and (3) Stable, scalable recipes follow
predictable scaling trajectories, enabling extrapolation from smaller-scale
runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and
demonstrate its effectiveness by successfully scaling and predicting validation
performance on a single RL run scaled up to 100,000 GPU-hours. Our work
provides both a scientific framework for analyzing scaling in RL and a
practical recipe that brings RL training closer to the predictability long
achieved in pre-training.

</details>


### [62] [Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach](https://arxiv.org/abs/2510.13792)
*Ziqing Lu,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的"不可战胜"对抗攻击方法，通过率失真理论随机改变智能体对转移核的观察，使其在训练中获得零或有限的地面真实信息。


<details>
  <summary>Details</summary>
Motivation: 提高RL系统对抗攻击的鲁棒性需要研究各种对抗攻击策略，现有确定性攻击可被受害智能体逆转，因此需要开发不可逆的攻击方法。

Method: 采用率失真信息论方法，随机改变智能体对转移核或其他属性的观察，限制其获取地面真实信息。

Result: 推导了受害智能体奖励遗憾的信息论下界，展示了率失真攻击对最先进模型基和无模型算法的影响。

Conclusion: 信息论方法可扩展到其他类型的对抗攻击，如状态观察攻击，为RL安全防御提供了新的研究方向。

Abstract: Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged
in many security-related applications, such as autonomous driving, financial
decisions, and drone/robot algorithms. In order to improve the
robustness/defense of RL systems against adversaries, studying various
adversarial attacks on RL systems is very important. Most previous work
considered deterministic adversarial attack strategies in MDP, which the
recipient (victim) agent can defeat by reversing the deterministic attacks. In
this paper, we propose a provably ``invincible'' or ``uncounterable'' type of
adversarial attack on RL. The attackers apply a rate-distortion
information-theoretic approach to randomly change agents' observations of the
transition kernel (or other properties) so that the agent gains zero or very
limited information about the ground-truth kernel (or other properties) during
the training. We derive an information-theoretic lower bound on the recipient
agent's reward regret and show the impact of rate-distortion attacks on
state-of-the-art model-based and model-free algorithms. We also extend this
notion of an information-theoretic approach to other types of adversarial
attack, such as state observation attacks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [63] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Dodoor是一种高效的随机化去中心化调度器，通过批量更新缓存服务器信息来减少通信开销，使用新颖的负载评分机制处理异构集群中的多维资源需求任务调度。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心需要高效的任务调度系统，传统去中心化调度器依赖实时探测远程服务器，通信开销大。Dodoor旨在通过缓存信息和批量更新来减少通信成本，同时处理异构集群中的动态多维资源需求。

Method: 基于加权球-箱模型的b批处理设置，使用缓存服务器信息而非实时探测，引入新颖的负载评分机制来捕捉服务器与任务之间的反亲和性，替代传统的待处理任务计数启发式方法。

Result: 在101节点异构集群上的评估显示：调度消息减少55-66%，吞吐量提升最高33.2%和21.5%，平均完工时间延迟降低12.1%和7.2%，尾部延迟改善21.9%和24.6%。

Conclusion: Dodoor通过减少通信开销和优化负载平衡，在异构数据中心环境中显著提升了调度性能和效率。

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [64] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 本文提出了一个新的集成分布式系统框架FDIRS，通过使用异构分布式数据库技术来提高系统性能、响应速度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有集成系统框架的性能和可靠性问题，提高用户满意度和系统效率。

Method: 使用三部分方法：分析现有集成系统及其演进过程，简要介绍ERPSD和ERPDRT框架，然后详细说明新的FDIRS框架，最后通过仿真比较各框架结果。

Result: FDIRS框架通过异构分布式数据库技术显著提高了系统性能、响应速度和效率，同时增强了系统可靠性并解决了先前框架的一些问题。

Conclusion: FDIRS框架成功提高了集成系统的效率、性能和可靠性，有效解决了先前框架存在的问题。

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [65] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: BanaServe是一个动态编排框架，通过层级权重迁移、注意力级KV缓存迁移和全局KV缓存共享，解决了分解式LLM服务中的资源分配不平衡、负载不均和缓存感知路由导致的负载倾斜问题。


<details>
  <summary>Details</summary>
Motivation: 当前分解式LLM服务系统面临三个关键限制：静态资源分配无法适应动态工作负载、预填充和解码阶段固有的负载不平衡、以及前缀缓存感知路由导致的负载分布倾斜，这些问题导致资源浪费或违反SLO。

Method: 引入层级权重迁移、注意力级KV缓存迁移和全局KV缓存共享与层级重叠传输，实现粗粒度（层级）和细粒度（注意力级）负载重分布，路由器可进行纯负载感知调度而不受缓存位置限制。

Result: 与vLLM相比，BanaServe实现了1.2x-3.9x更高的吞吐量和3.9%-78.4%更低的总处理时间；与DistServe相比，吞吐量提高1.1x-2.8x，延迟降低1.4%-70.1%。

Conclusion: BanaServe通过动态资源重新平衡和缓存热点消除，显著提高了分解式LLM服务的效率和性能。

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [66] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Großmann,Mattthias Schimek*

Main category: cs.DC

TL;DR: 本文提出了首个分布式内存并行缩减算法，用于解决最大权重独立集问题，实现了在大规模图上的高效计算，在1024个处理器上展示了良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 最大权重独立集是一个重要的NP难优化问题，现有算法主要使用数据缩减规则来寻找最优解。然而，这些方法在处理大规模图时面临挑战，需要开发分布式并行算法来扩展计算规模。

Method: 提出了分布式内存并行缩减算法，包括分布式reduce-and-greedy和reduce-and-peel启发式算法，支持异步计算模式，能够处理超过10亿顶点和170亿边的大规模图。

Result: 在1024个处理器上实验显示良好可扩展性，异步reduce-and-peel方法在36个真实世界图上平均加速33倍，解质量接近顺序算法；reduce-and-greedy算法平均加速达50倍，但解质量较低。

Conclusion: 分布式并行算法成功扩展了最大权重独立集问题的计算规模，在保持解质量的同时实现了显著加速，为处理超大规模图提供了有效解决方案。

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [67] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: ARES是一个基于长度预测的自适应解码重调度系统，通过LLM原生预测方法准确预测剩余生成长度，在解码阶段动态平衡工作负载，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中输出长度的变化导致解码阶段工作负载严重不平衡，特别是在长输出推理任务中。现有系统采用静态预填充到解码调度，在变化的解码工作负载下经常导致SLO违规和OOM故障。

Method: 提出ARES系统，包含：(1)轻量级连续LLM原生预测方法，利用LLM隐藏状态高精度建模剩余生成长度；(2)解码阶段重调度解决方案，集成当前和预测工作负载的动态平衡机制。

Result: 预测方法将MAE降低49.42%，预测器参数减少93.28%；重调度机制将P99 TPOT降低74.77%，实现高达2.24倍的吞吐量提升。

Conclusion: ARES通过自适应解码重调度有效解决了LLM推理中的工作负载不平衡问题，显著提升了系统性能和资源利用率。

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [68] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit Côté,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: FIRST是一个联邦推理资源调度工具包，可在分布式高性能计算集群上提供推理即服务，支持多种AI模型，通过OpenAI兼容API实现并行推理工作负载。


<details>
  <summary>Details</summary>
Motivation: 解决科学工作流中对私有、安全和可扩展AI推理日益增长的需求，让研究人员能够在本地基础设施上生成数十亿个token，而无需依赖商业云基础设施。

Method: 利用Globus Auth和Globus Compute，通过集群无关的API在联邦集群间分发请求，支持多种推理后端（如vLLM），自动扩展资源，维护"热"节点以实现低延迟执行。

Result: 提供了一个云式访问平台，可在现有HPC基础设施上运行多样化的AI模型（如大语言模型），支持高吞吐量批处理和交互模式。

Conclusion: FIRST框架成功实现了在私有安全环境中进行大规模AI推理的能力，为科学研究提供了可靠的本地方案。

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [69] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timothé Albouy,Antonio Fernández Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: 本文研究了具有二进制输出的分布式任务的必要和充分系统条件，重点关注任务可以产生的不同输出值集合，提供了同步和异步系统中二进制输出任务可解性的完整特征化。


<details>
  <summary>Details</summary>
Motivation: 探索分布式系统中二进制输出任务的系统条件，统一多个分布式计算问题如二进制共识和对称性破坏，并为更强的任务表述提供不可能性证明。

Method: 采用输出集合方法，关注任务产生的不同输出值集合，忽略有效性和值多重性，考虑某些进程可能不输出值的情况。

Result: 在具有n个进程、最多t个进程可能崩溃的分布式系统中，为同步和异步系统提供了二进制输出任务类可解性的紧致条件完整特征化。

Conclusion: 输出集合方法产生了高度一般化的结果，统一了多个分布式计算问题，并为考虑有效性、值多重性或超越二进制输出的更强任务表述提供了不可能性证明。

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [70] [KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems](https://arxiv.org/abs/2510.12872)
*Hancheng Ye,Zhengqi Gao,Mingyuan Ma,Qinsi Wang,Yuzhe Fu,Ming-Yu Chung,Yueqian Lin,Zhijian Liu,Jianyi Zhang,Danyang Zhuo,Yiran Chen*

Main category: cs.MA

TL;DR: KVCOMM是一个无需训练的框架，通过重用KV缓存并在不同前缀上下文中对齐缓存偏移，解决多智能体LLM系统中重复处理重叠上下文导致的效率问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在处理复杂语言任务时，由于智能体间需要通信协调，经常重复处理重叠的上下文，导致显著的计算开销。传统的KV缓存在单智能体设置中有效，但在多智能体场景中由于前缀分歧而无法直接重用。

Method: 提出KVCOMM框架，通过引用存储观察到的缓存偏差的锚点池来估计和调整共享内容的KV缓存，在线维护和更新锚点池以动态适应不同的用户请求和上下文结构。

Result: KVCOMM在多样化多智能体工作负载中实现了超过70%的重用率，包括检索增强生成、数学推理和协作编码任务，且无质量下降。在五智能体设置下，当每个全连接智能体接收1K输入token时，实现了7.8倍加速，将TTFT从约430ms降低到约55ms。

Conclusion: KVCOMM通过有效重用和调整KV缓存，显著提升了多智能体LLM系统的推理效率，为复杂多智能体任务提供了高效的解决方案。

Abstract: Multi-agent large language model (LLM) systems are increasingly adopted for
complex language processing tasks that require communication and coordination
among agents. However, these systems often suffer substantial overhead from
repeated reprocessing of overlapping contexts across agents. In typical
pipelines, once an agent receives a message from its predecessor, the full
context-including prior turns-must be reprocessed from scratch, leading to
inefficient processing. While key-value (KV) caching is an effective solution
for avoiding redundant computation in single-agent settings where prefixes
remain unchanged, it cannot be directly reused in multi-agent scenarios due to
diverging prefixes introduced by agent-specific context extensions. We identify
that the core challenge lies in the offset variance of KV-caches across agents.
To address this, we propose KVCOMM, a training-free framework that enables
efficient prefilling in multi-agent inference by reusing KV-caches and aligning
cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM
estimates and adjusts KV-caches for shared content by referencing a pool of
cached examples-termed anchors-that store observed cache deviations under
varying prefixes. The anchor pool is maintained and updated online, allowing
dynamic adaptation to distinct user requests and context structures. KVCOMM
achieves over 70% reuse rate across diverse multi-agent workloads, including
retrieval-augmented generation, math reasoning, and collaborative coding tasks,
all without quality degradation. Particularly, when each fully-connected agent
receives 1K input tokens with 512 prefix tokens and 512 output tokens under a
five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard
prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.

</details>


### [71] [Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility](https://arxiv.org/abs/2510.13227)
*Divyanshu Singh,Ashman Mehra,Snehanshu Saha,Santonu Sarkar*

Main category: cs.MA

TL;DR: 本文提出了一种基于利他主义的去中心化拼车系统ARS，使用多智能体强化学习进行动态匹配，通过游戏理论保证公平性，相比传统拼车系统能减少出行距离和排放，提高车辆利用率。


<details>
  <summary>Details</summary>
Motivation: 解决城市交通拥堵和燃料消耗问题，传统拼车平台过于追求利润而忽视公平性和可持续性，需要一种更公平、可持续的出行方案。

Method: 采用去中心化点对点框架，参与者基于利他主义积分轮流担任司机和乘客角色，整合多智能体强化学习进行动态拼车匹配，使用游戏理论保证公平性，并建立人口模型维持长期平衡。

Result: 使用纽约市真实出租车数据验证，ARS相比无拼车和基于优化的基准方法，减少了出行距离和排放，提高了车辆利用率，促进了公平参与。

Conclusion: ARS作为一种可扩展的社区驱动替代方案，能够将个人行为与集体城市可持续性目标相结合，是传统拼车系统的可行替代方案。

Abstract: Urban mobility faces persistent challenges of congestion and fuel
consumption, specifically when people choose a private, point-to-point commute
option. Profit-driven ride-sharing platforms prioritize revenue over fairness
and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a
decentralized, peer-to-peer mobility framework where participants alternate
between driver and rider roles based on altruism points rather than monetary
incentives. The system integrates multi-agent reinforcement learning (MADDPG)
for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness,
and a population model to sustain long-term balance. Using real-world New York
City taxi data, we demonstrate that ARS reduces travel distance and emissions,
increases vehicle utilization, and promotes equitable participation compared to
both no-sharing and optimization-based baselines. These results establish ARS
as a scalable, community-driven alternative to conventional ride-sharing,
aligning individual behavior with collective urban sustainability goals.

</details>


### [72] [AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions](https://arxiv.org/abs/2510.13343)
*Shota Takayama,Katsuhide Fujita*

Main category: cs.MA

TL;DR: 本文提出AOAD-MAT模型，通过显式考虑智能体决策顺序来改进多智能体强化学习性能，在星际争霸和MuJoCo基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MARL模型虽然利用序列决策过程提升性能，但未明确考虑智能体决策顺序的重要性，因此需要开发能够学习最优智能体行动顺序的新方法。

Method: 提出AOAD-MAT模型，采用基于Transformer的actor-critic架构，动态调整智能体行动序列，引入预测下一个行动智能体的子任务，并与PPO损失函数协同优化序列决策优势。

Result: 在StarCraft Multi-Agent Challenge和Multi-Agent MuJoCo基准测试中的实验结果表明，AOAD-MAT模型优于现有的MAT和其他基线模型。

Conclusion: 调整智能体行动决策顺序在MARL中是有效的，AOAD-MAT模型通过显式整合决策序列到学习过程中，能够学习并预测最优的智能体行动顺序。

Abstract: Multi-agent reinforcement learning focuses on training the behaviors of
multiple learning agents that coexist in a shared environment. Recently, MARL
models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep
Q-learning (ACE), have significantly improved performance by leveraging
sequential decision-making processes. Although these models can enhance
performance, they do not explicitly consider the importance of the order in
which agents make decisions. In this paper, we propose an Agent Order of Action
Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which
agents make decisions. The proposed model explicitly incorporates the sequence
of action decisions into the learning process, allowing the model to learn and
predict the optimal order of agent actions. The AOAD-MAT model leverages a
Transformer-based actor-critic architecture that dynamically adjusts the
sequence of agent actions. To achieve this, we introduce a novel MARL
architecture that cooperates with a subtask focused on predicting the next
agent to act, integrated into a Proximal Policy Optimization based loss
function to synergistically maximize the advantage of the sequential
decision-making. The proposed method was validated through extensive
experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo
benchmarks. The experimental results show that the proposed AOAD-MAT model
outperforms existing MAT and other baseline models, demonstrating the
effectiveness of adjusting the AOAD order in MARL.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [73] [Fair Ordering](https://arxiv.org/abs/2510.13664)
*Muhammad Haseeb,Jinkun Geng,Radhika Mittal,Aurojit Panda,Srinivas Narayana,Anirudh Sivaraman*

Main category: cs.NI

TL;DR: Tommy系统提出了一种概率性的公平排序方法，通过统计模型比较噪声时间戳，建立"可能发生在前"关系来处理时钟同步限制下的公平排序问题。


<details>
  <summary>Details</summary>
Motivation: 解决分布式系统中由于时钟同步限制导致的公平排序挑战，传统方法试图消除时钟误差，而Tommy系统则接受并利用时钟变异性。

Method: 使用统计模型学习每个时钟的偏移分布，概率性地比较两个噪声时间戳，计算一个事件在另一个事件之前发生的概率，建立"可能发生在前"关系。

Result: 提出了基于概率的"可能发生在前"关系，为传统"发生在前"关系认为是并发的事件提供排序基础。

Conclusion: Tommy系统为处理时钟同步限制下的公平排序问题提供了新的概率性方法，并指出了多个研究方向，包括在线公平排序、随机公平全序等。

Abstract: A growing class of applications demands \emph{fair ordering/sequencing} of
events which ensures that events generated earlier by one client are processed
before later events from other clients. However, achieving such sequencing is
fundamentally challenging due to the inherent limitations of clock
synchronization. We advocate for an approach that embraces, rather than
eliminates, clock variability. Instead of attempting to remove error from a
timestamp, Tommy, our proposed system, leverages a statistical model to compare
two noisy timestamps probabilistically by learning per-clock offset
distributions. Our preliminary statistical model computes the probability that
one event precedes another w.r.t. the wall-clock time without access to the
wall-clock. This serves as a foundation for a new relation:
\emph{likely-happened-before} denoted by $\xrightarrow{p}$ where $p$ represents
the probability of an event to have happened before another. The
$\xrightarrow{p}$ relation provides a basis for ordering multiple events which
are otherwise considered \emph{concurrent} by the typical
\emph{happened-before} ($\rightarrow$) relation. We highlight various related
challenges including intransitivity of $\xrightarrow{p}$ relation as opposed to
the transitive $\rightarrow$ relation. We also outline several research
directions: online fair sequencing, stochastically fair total ordering,
host-level support for fairness and more.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [74] [On the performance of Active STAR-RIS-Assisted Cell-Free Massive MIMO Systems with Phase Errors and Channel Aging](https://arxiv.org/abs/2510.13171)
*Jun Qian,Ross Murch,Khaled B. Letaief*

Main category: cs.IT

TL;DR: 本文分析了主动同时传输和反射可重构智能表面在无蜂窝大规模MIMO系统中相位误差和信道老化的影响，提出了基于最小均方误差估计的信道估计方法和下行频谱效率的闭式表达式，并给出了缓解性能下降的实用设计指南。


<details>
  <summary>Details</summary>
Motivation: 主动可重构智能表面采用放大来克服由RIS级联链路引起的衰减，但相位误差和信道老化会影响系统性能，需要分析这些因素对主动STAR-RIS辅助无蜂窝大规模MIMO系统的影响。

Method: 利用空间相关瑞利衰落模型，推导基于最小均方误差估计的信道估计，并制定下行频谱效率的闭式表达式，全面评估信道老化和均匀分布相位误差对系统性能的影响。

Result: 结果表明主动STAR-RIS能有效补偿相位误差和信道老化的不利影响，增加接入点和STAR-RIS元素数量以及更大的放大因子可以缓解性能下降。

Conclusion: 为抵消信道老化的影响，提出了资源块长度设计的实用指南，主动STAR-RIS能够有效改善系统在相位误差和信道老化条件下的性能。

Abstract: Active reconfigurable intelligent surfaces (RISs) employ amplification to
overcome attenuation caused by the RIS cascaded link. In this paper, we analyze
the effects of phase errors and channel aging in active simultaneously
transmitting and reflecting (STAR) RIS-assisted cell-free massive
multiple-input multiple-output (MIMO) systems. By leveraging a spatially
correlated Rayleigh fading model, this paper derives minimum mean square error
estimate-based channel estimates and formulates closed-form expressions for
downlink spectral efficiency. This analytical framework enables a comprehensive
evaluation of the effects of channel aging and uniformly distributed phase
errors on system performance. The results demonstrate that active STAR-RISs can
effectively compensate for the adverse effects of phase errors and channel
aging. To counteract the impact of channel aging, we propose practical
guidelines for resource-block-length design. Also, an increase in APs and
STAR-RIS elements, along with a larger amplification factor, can alleviate
performance degradation.

</details>


### [75] [A Dimension-Keeping Semi-Tensor Product Framework for Compressed Sensing](https://arxiv.org/abs/2510.13180)
*Qi Qi,Abdelhamid Tayebi,Daizhan Cheng,Jun-e Feng*

Main category: cs.IT

TL;DR: 提出了一种新的维度保持半张量积压缩感知方法，利用组内相关性同时保持组间不相干性来改进测量矩阵设计，在图像压缩重建任务中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知框架依赖于测量矩阵列的不相干性来保证重建性能，但未能充分利用信号在变换域中的稀疏表示特性。

Method: 将DK-STP算法集成到感知矩阵设计中，实现降维同时保持信号恢复能力，利用组内相关性并保持组间不相干性。

Result: 在图像压缩重建任务中实现了显著的噪声抑制和视觉保真度提升，PSNR值显著高于传统CS和STP-CS方法，在噪声条件和不同采样率下表现出鲁棒性。

Conclusion: DK-STP-CS方法在资源受限环境中具有实际应用潜力，通过改进测量矩阵设计有效提升了压缩感知性能。

Abstract: In compressed sensing (CS), sparse signals can be reconstructed from
significantly fewer samples than required by the Nyquist-Shannon sampling
theorem. While non-sparse signals can be sparsely represented in appropriate
transformation domains, conventional CS frameworks rely on the incoherence of
the measurement matrix columns to guarantee reconstruction performance. This
paper proposes a novel method termed Dimension-Keeping Semi-Tensor Product
Compressed Sensing (DK-STP-CS), which leverages intra-group correlations while
maintaining inter-group incoherence to enhance the measurement matrix design.
Specifically, the DK-STP algorithm is integrated into the design of the sensing
matrix, enabling dimensionality reduction while preserving signal recovery
capability. For image compression and reconstruction tasks, the proposed method
achieves notable noise suppression and improves visual fidelity. Experimental
results demonstrate that DK-STP-CS significantly outperforms traditional CS and
STP-CS approaches, as evidenced by higher Peak Signal-to-Noise Ratio (PSNR)
values between the reconstructed and original images. The robustness of
DK-STP-CS is further validated under noisy conditions and varying sampling
rates, highlighting its potential for practical applications in
resource-constrained environments.

</details>


### [76] [Non-Linear Precoding via Dirty Paper Coding for Near-Field Downlink MISO Communications](https://arxiv.org/abs/2510.13485)
*Akash Kulkarni,Rajshekhar V Bhat*

Main category: cs.IT

TL;DR: 本文提出了一种基于脏纸编码（DPC）的非线性预编码框架，用于6G系统中的近场通信，相比传统线性预编码（如ZF）能显著提升和速率性能。


<details>
  <summary>Details</summary>
Motivation: 现有近场通信系统主要采用线性预编码技术，如迫零预编码，这些方法需要高发射功率来抑制干扰，导致性能下降。

Method: 提出基于脏纸编码的非线性预编码框架，通过预消除已知干扰来最大化和速率性能，并制定和解决了相应的和速率最大化问题。

Result: 大量仿真表明，DPC在各种近场配置下相比ZF实现了显著的和速率增益，在用户间距较小时改善最为明显。

Conclusion: DPC非线性预编码框架在6G近场通信系统中具有优越性能，特别是在用户密集场景下能显著提升系统容量。

Abstract: In 6G systems, extremely large-scale antenna arrays operating at terahertz
frequencies extend the near-field region to typical user distances from the
base station, enabling near-field communication (NFC) with fine spatial
resolution through beamfocusing. Existing multiuser NFC systems predominantly
employ linear precoding techniques such as zero-forcing (ZF), which suffer from
performance degradation due to the high transmit power required to suppress
interference. This paper proposes a nonlinear precoding framework based on
Dirty Paper Coding (DPC), which pre-cancels known interference to maximize the
sum-rate performance. We formulate and solve the corresponding sum-rate
maximization problems, deriving optimal power allocation strategies for both
DPC and ZF schemes. Extensive simulations demonstrate that DPC achieves
substantial sum-rate gains over ZF across various near-field configurations,
with the most pronounced improvements observed for closely spaced users.

</details>


### [77] [Simulating Mediumband Wireless Communication Systems: A Concise Description](https://arxiv.org/abs/2510.13532)
*Dushyantha A Basnayaka*

Main category: cs.IT

TL;DR: 本文详细描述了在MATLAB中准确模拟中频数字无线通信系统从单发射器到单接收器的必要步骤，特别关注物理层操作的详细模拟。


<details>
  <summary>Details</summary>
Motivation: 现有文献中数字无线通信系统通常在离散时间复基带域进行模拟，忽略了脉冲整形、上变频、混频、载波同步和符号定时同步等关键操作。虽然这些假设在大多数情况下足够，但要准确捕捉中频通信的本质，需要详细模拟物理层操作。

Method: 使用MATLAB详细模拟从中频单发射器到单接收器的无线通信场景，详细阐述关键物理层子系统的操作，包括脉冲整形、上变频、混频、载波同步和符号定时同步等。

Result: 所描述的方法确保模拟系统能够捕捉中频无线通信的微妙动态，包括深度衰落避免的效果。

Conclusion: 本文提供了一种在MATLAB中准确模拟中频无线通信系统的详细方法，特别强调了物理层操作的精确模拟对于捕捉通信系统动态特性的重要性。

Abstract: In this paper, we describe the necessary procedures for accurately simulating
digital wireless communication systems operating in the mediumband, aimed at
both beginners and experts. In the research literature, digital wireless
communication systems are typically simulated in the discrete-time complex
baseband domain, where pulse shaping, upconversion, mixing, carrier
synchronization, and symbol timing synchronization are often ignored. These
assumptions are indeed sufficient in most cases, but to capture the essence of
communication in the mediumband, certain physical layer (PHY) operations should
be simulated in detail. In this paper, we concisely describe how to simulate a
mediumband wireless communication scenario from a single transmitter (TX) to a
single receiver (RX) in MATLAB, elaborating the operation of key PHY
subsystems. The approach described here ensures that the simulated system
captures the delicate dynamics of mediumband wireless communication, including
the effect of deep fading avoidance.

</details>


### [78] [Local Information-Theoretic Security via Euclidean Geometry](https://arxiv.org/abs/2510.13661)
*Emmanouil M. Athanasakos,Nicholas Kalouptsidis,Hariprasad Manjunath*

Main category: cs.IT

TL;DR: 本文提出了一种基于欧几里得信息论的方法，用于研究离散无记忆窃听信道上安全通信的局部特性。通过将非凸优化问题转化为可处理的二次规划结构，推导了近似局部保密容量的解析公式，并定义了新的秘密局部收缩系数来量化信道的固有局部泄漏效率。


<details>
  <summary>Details</summary>
Motivation: 研究离散无记忆窃听信道上安全通信的局部特性，旨在在限制信息泄露给窃听者和编码秘密消息的信息成本的同时，最大化合法用户的信息速率。

Method: 基于欧几里得信息论，将非凸优化问题通过局部几何近似转化为可处理的二次规划结构，利用KKT条件和信道矩阵的广义特征值求解最优拉格朗日乘子，并通过线性规划找到最优解。

Result: 推导出了近似局部保密容量的解析公式，定义了新的秘密局部收缩系数来量化信道的固有局部泄漏效率，并建立了这些局部系数与其全局对应物之间的界限关系。

Conclusion: 所提出的框架通过详细分析和数值示例证明了其有效性，适用于通用多模信道和典型二进制对称窃听信道，为安全通信的局部特性分析提供了有力工具。

Abstract: This paper introduces a methodology based on Euclidean information theory to
investigate local properties of secure communication over discrete memoryless
wiretap channels. We formulate a constrained optimization problem that
maximizes a legitimate user's information rate while imposing explicit upper
bounds on both the information leakage to an eavesdropper and the informational
cost of encoding the secret message. By leveraging local geometric
approximations, this inherently non-convex problem is transformed into a
tractable quadratic programming structure. It is demonstrated that the optimal
Lagrange multipliers governing this approximated problem can be found by
solving a linear program. The constraints of this linear program are derived
from Karush-Kuhn-Tucker conditions and are expressed in terms of the
generalized eigenvalues of channel-derived matrices. This framework facilitates
the derivation of an analytical formula for an approximate local secrecy
capacity. Furthermore, we define and analyze a new class of secret local
contraction coefficients. These coefficients, characterized as the largest
generalized eigenvalues of a matrix pencil, quantify the maximum achievable
ratio of approximate utility to approximate leakage, thus measuring the
intrinsic local leakage efficiency of the channel. We establish bounds
connecting these local coefficients to their global counterparts defined over
true mutual information measures. The efficacy of the proposed framework is
demonstrated through detailed analysis and numerical illustrations for both
general multi-mode channels and the canonical binary symmetric wiretap channel.

</details>


### [79] [Combinatorial Bounds for List Recovery via Discrete Brascamp--Lieb Inequalities](https://arxiv.org/abs/2510.13775)
*Joshua Brakensiek,Yeyuan Chen,Manik Dhar,Zihan Zhang*

Main category: cs.IT

TL;DR: 本文提出了编码理论中列表恢复问题的新组合边界，证明了当ρ接近容量时，列表大小L最多为(ℓ/(R+ε))^{O(R/ε)}，解决了L是否能有ℓ的多项式上界的长期开放问题。


<details>
  <summary>Details</summary>
Motivation: 解决编码理论中列表恢复问题的关键问题：当ρ接近容量时，列表大小L是否能有ℓ的多项式上界，这是一个长期未解决的开放问题。

Method: 主要技术是离散熵Brascamp-Lieb不等式在列表恢复问题中的新颖应用，通过该技术将每个坐标的局部结构与恢复列表的全局结构联系起来。

Result: 对于各种线性和折叠线性码族，包括随机线性码、随机Reed-Solomon码、显式折叠Reed-Solomon码和显式单变量多重码，当ρ=1-R-ε接近容量时，列表大小L最多为(ℓ/(R+ε))^{O(R/ε)}。

Conclusion: 该结果证明了列表大小L确实有ℓ的多项式上界，在零错误机制下与已知下界完美匹配，并推广了Chen和Zhang关于折叠Reed-Solomon码列表可解码性的结果。

Abstract: In coding theory, the problem of list recovery asks one to find all codewords
$c$ of a given code $C$ which such that at least $1-\rho$ fraction of the
symbols of $c$ lie in some predetermined set of $\ell$ symbols for each
coordinate of the code. A key question is bounding the maximum possible list
size $L$ of such codewords for the given code $C$.
  In this paper, we give novel combinatorial bounds on the list recoverability
of various families of linear and folded linear codes, including random linear
codes, random Reed--Solomon codes, explicit folded Reed--Solomon codes, and
explicit univariate multiplicity codes. Our main result is that in all of these
settings, we show that for code of rate $R$, when $\rho = 1 - R - \epsilon$
approaches capacity, the list size $L$ is at most
$(\ell/(R+\epsilon))^{O(R/\epsilon)}$. These results also apply in the
average-radius regime. Our result resolves a long-standing open question on
whether $L$ can be bounded by a polynomial in $\ell$. In the zero-error regime,
our bound on $L$ perfectly matches known lower bounds.
  The primary technique is a novel application of a discrete entropic
Brascamp--Lieb inequality to the problem of list recovery, allowing us to
relate the local structure of each coordinate with the global structure of the
recovered list. As a result of independent interest, we show that a recent
result by Chen and Zhang (STOC 2025) on the list decodability of folded
Reed--Solomon codes can be generalized into a novel Brascamp--Lieb type
inequality.

</details>


### [80] [From Random to Explicit via Subspace Designs With Applications to Local Properties and Matroids](https://arxiv.org/abs/2510.13777)
*Joshua Brakensiek,Yeyuan Chen,Manik Dhar,Zihan Zhang*

Main category: cs.IT

TL;DR: 本文扩展了Levi等人的框架，研究子空间可设计码的局部性质，建立了随机线性码与最优子空间设计码之间的局部等价性，并在拟阵理论中应用，改进了Jackson和Tanigawa的复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 扩展局部性质阈值率计算框架到子空间可设计码，包括显式折叠Reed-Solomon码和单变量重数码，建立随机线性码与子空间设计码的局部等价性，并应用于拟阵理论中的可纠正擦除模式识别问题。

Method: 扩展Levi等人的统一框架，研究子空间可设计码的局部性质，建立随机线性码与子空间设计码的局部等价性，并应用于拟阵理论中的确定性多项式时间算法设计。

Result: 证明了随机线性码与最优子空间设计码之间的局部等价性，给出了同时具备随机线性码所有局部性质的显式折叠线性码构造，并在拟阵理论中改进了可纠正擦除模式识别的复杂度结果。

Conclusion: 子空间可设计码与随机线性码在局部性质上具有等价性，这一结果不仅为编码理论提供了新的显式构造，还在拟阵理论中带来了复杂度改进，深化了对子空间设计存在性和局限性的理解。

Abstract: In coding theory, a common question is to understand the threshold rates of
various local properties of codes, such as their list decodability and list
recoverability. A recent work Levi, Mosheiff, and Shagrithaya (FOCS 2025) gave
a novel unified framework for calculating the threshold rates of local
properties for random linear and random Reed--Solomon codes.
  In this paper, we extend their framework to studying the local properties of
subspace designable codes, including explicit folded Reed-Solomon and
univariate multiplicity codes. Our first main result is a local equivalence
between random linear codes and (nearly) optimal subspace design codes up to an
arbitrarily small rate decrease. We show any local property of random linear
codes applies to all subspace design codes. As such, we give the first explicit
construction of folded linear codes that simultaneously attain all local
properties of random linear codes. Conversely, we show that any local property
which applies to all subspace design codes also applies to random linear codes.
  Our second main result is an application to matroid theory. We show that the
correctable erasure patterns in a maximally recoverable tensor code can be
identified in deterministic polynomial time, assuming a positive answer to a
matroid-theoretic question due to Mason (1981). This improves on a result of
Jackson and Tanigawa (JCTB 2024) who gave a complexity characterization of
$\mathsf{RP} \cap \mathsf{coNP}$ assuming a stronger conjecture. Our result
also applies to the generic bipartite rigidity and matrix completion matroids.
  As a result of additional interest, we study the existence and limitations of
subspace designs. In particular, we tighten the analysis of family of subspace
designs constructioned by Guruswami and Kopparty (Combinatorica 2016) and show
that better subspace designs do not exist over algebraically closed fields.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [81] [The Beautiful Deception: How 256 Bits Pretend to be Infinity](https://arxiv.org/abs/2510.12802)
*Alexander Towell*

Main category: cs.CR

TL;DR: 该论文探讨了计算密码学中的核心欺骗：使用有限信息模拟无限随机性。证明了真随机预言机不可能存在，展示了惰性评估如何创建有限自动机来成功伪装成无限系统，揭示了密码学中的"随机性"实际上是计算难度的伪装。


<details>
  <summary>Details</summary>
Motivation: 研究如何用有限信息（如256位熵）来模拟无限随机性，这是计算密码学的基础问题。探索计算密码学中随机性的本质和实现机制。

Method: 通过证明真随机预言机的不可能性，使用惰性评估技术创建有限自动机，并通过Python实现展示256位熵如何生成与无限随机性无法区分的序列。

Result: 证明了有限信息可以成功模拟无限随机性，只要观察者在计算上是有限的。展示了256位熵足以生成对计算有限观察者来说与无限随机性无法区分的序列。

Conclusion: 密码学中的随机性实际上是计算难度的伪装，通过惰性评估技术，有限系统可以有效地模拟无限随机性，这是现代密码学安全性的基础。

Abstract: How do you store infinity in 256 bits? This paper explores the fundamental
deception at the heart of computational cryptography: using finite information
to simulate infinite randomness. We prove why true random oracles are
impossible, then show how lazy evaluation creates a beautiful lie -- a finite
automaton that successfully pretends to be infinite. We reveal that
``randomness'' in cryptography is actually computational hardness in disguise,
demonstrating through Python implementations how 256 bits of entropy can
generate sequences indistinguishable from infinite randomness to any
computationally bounded observer.How do you store infinity in 256 bits? This
paper explores the fundamental deception at the heart of computational
cryptography: using finite information to simulate infinite randomness. We
prove why true random oracles are impossible, then show how lazy evaluation
creates a beautiful lie -- a finite automaton that successfully pretends to be
infinite. We reveal that ``randomness'' in cryptography is actually
computational hardness in disguise, demonstrating through Python
implementations how 256 bits of entropy can generate sequences
indistinguishable from infinite randomness to any computationally bounded
observer.

</details>


### [82] [We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice](https://arxiv.org/abs/2510.12812)
*Aleksandar Petrov,Pierre Fernandez,Tomáš Souček,Hady Elsahar*

Main category: cs.CR

TL;DR: 该论文分析了当前深度学习图像水印方法的容量限制，建立了在PSNR和线性鲁棒性约束下的理论上界，发现理论容量比现有方法高出几个数量级。作者训练了ChunkySeal模型，将容量提升4倍至1024比特，证明了仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的鲁棒图像水印方法容量仅能达到几百比特的水平，这种进展停滞引发了关于图像水印基本极限的疑问。

Method: 建立了在PSNR和线性鲁棒性约束下图像消息携带容量的上界分析，并训练了ChunkySeal模型（VideoSeal的扩展版本）来验证更大容量的可行性。

Result: 理论分析表明图像水印的理论容量比当前模型实现的高出几个数量级。实验验证了ChunkySeal能够将容量提升4倍至1024比特，同时保持图像质量和鲁棒性。

Conclusion: 现代水印方法尚未达到容量极限，在架构创新和训练策略方面仍有显著的提升机会。

Abstract: Despite rapid progress in deep learning-based image watermarking, the
capacity of current robust methods remains limited to the scale of only a few
hundred bits. Such plateauing progress raises the question: How far are we from
the fundamental limits of image watermarking? To this end, we present an
analysis that establishes upper bounds on the message-carrying capacity of
images under PSNR and linear robustness constraints. Our results indicate
theoretical capacities are orders of magnitude larger than what current models
achieve. Our experiments show this gap between theoretical and empirical
performance persists, even in minimal, easily analysable setups. This suggests
a fundamental problem. As proof that larger capacities are indeed possible, we
train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4
times to 1024 bits, all while preserving image quality and robustness. These
findings demonstrate modern methods have not yet saturated watermarking
capacity, and that significant opportunities for architectural innovation and
training strategies remain.

</details>


### [83] [ARTeX: Anonymity Real-world-assets Token eXchange](https://arxiv.org/abs/2510.12821)
*Jaeseong Lee,Junghee Lee*

Main category: cs.CR

TL;DR: 本文提出ARTeX平台解决RWA代币交易中的隐私问题，在保护交易者匿名性的同时增强对非法活动的防范。


<details>
  <summary>Details</summary>
Motivation: 虚拟资产市场中RWA代币交易存在隐私泄露风险，现有方法因RWA代币特性和技术限制难以有效保护匿名性。

Method: 设计新的代币交易平台ARTeX，克服现有方法的不足，确保交易者匿名性。

Result: ARTeX平台能够有效解决RWA代币交易的隐私问题。

Conclusion: ARTeX平台为RWA代币交易提供了有效的匿名保护解决方案，同时兼顾非法活动防范。

Abstract: This paper addresses one of the most noteworthy issues in the recent virtual
asset market, the privacy concerns related to token transactions of Real-World
Assets tokens, known as RWA tokens. Following the advent of Bitcoin, the
virtual asset market has experienced explosive growth, spawning movements to
link real-world assets with virtual assets. However, due to the transparency
principle of blockchain technology, the anonymity of traders cannot be
guaranteed. In the existing blockchain environment, there have been instances
of protecting the privacy of fungible tokens (FTs) using mixer services.
Moreover, numerous studies have been conducted to secure the privacy of
non-fungible tokens (NFTs). However, due to the unique characteristics of RWA
tokens and the limitations of each study, it has been challenging to achieve
the goal of anonymity protection effectively. This paper proposes a new token
trading platform, the ARTeX, designed to resolve these issues. This platform
not only addresses the shortcomings of existing methods but also ensures the
anonymity of traders while enhancing safeguards against illegal activities.

</details>


### [84] [SimKey: A Semantically Aware Key Module for Watermarking Language Models](https://arxiv.org/abs/2510.12828)
*Shingo Kodama,Haya Diwan,Lucas Rosenblatt,R. Teal Witter,Niv Cohen*

Main category: cs.CR

TL;DR: SimKey是一种语义密钥模块，通过将密钥生成与上下文语义绑定来增强水印鲁棒性，解决了现有水印方法对改写攻击脆弱和有害内容误归属的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法存在两个问题：(i) 对改写、重排等表面编辑脆弱；(ii) 攻击者可以附加无关有害文本继承水印，给模型所有者带来声誉风险。

Method: 使用局部敏感哈希对语义嵌入进行处理，确保改写文本产生相同水印密钥，而无关或语义偏移文本产生不同密钥，并与最先进水印方案集成。

Result: SimKey提高了水印对改写和翻译的鲁棒性，同时防止有害内容的误归属。

Conclusion: 语义感知密钥生成是实用且可扩展的水印方向。

Abstract: The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM's next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model's output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.

</details>


### [85] [Local Differential Privacy for Federated Learning with Fixed Memory Usage and Per-Client Privacy](https://arxiv.org/abs/2510.12908)
*Rouzbeh Behnia,Jeremiah Birrell,Arman Riasi,Reza Ebrahimi,Kaushik Dutta,Thang Hoang*

Main category: cs.CR

TL;DR: 提出L-RDP方法，解决联邦学习中本地差分隐私的高资源需求和异步参与下的隐私保证问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护数据隐私，但客户端更新和全局模型仍可能泄露信息，限制了在医疗等敏感领域的应用。现有LDP方法在FL中面临高资源需求导致客户端退出、异步参与下缺乏可靠隐私保证等问题

Method: 提出L-RDP方法，专为LDP设计，确保恒定的较低内存使用以减少客户端退出，并通过考虑间歇性参与提供严格的每客户端隐私保证

Result: L-RDP解决了现有LDP方法在联邦学习中的挑战，包括资源需求和隐私保证问题

Conclusion: L-RDP方法能够有效解决联邦学习中本地差分隐私的实际应用问题，提升模型泛化性、公平性，并符合HIPAA和GDPR等法规要求

Abstract: Federated learning (FL) enables organizations to collaboratively train models
without sharing their datasets. Despite this advantage, recent studies show
that both client updates and the global model can leak private information,
limiting adoption in sensitive domains such as healthcare. Local differential
privacy (LDP) offers strong protection by letting each participant privatize
updates before transmission. However, existing LDP methods were designed for
centralized training and introduce challenges in FL, including high resource
demands that can cause client dropouts and the lack of reliable privacy
guarantees under asynchronous participation. These issues undermine model
generalizability, fairness, and compliance with regulations such as HIPAA and
GDPR. To address them, we propose L-RDP, a DP method designed for LDP that
ensures constant, lower memory usage to reduce dropouts and provides rigorous
per-client privacy guarantees by accounting for intermittent participation.

</details>


### [86] [From base cases to backdoors: An Empirical Study of Unnatural Crypto-API Misuse](https://arxiv.org/abs/2510.13102)
*Victor Olaiya,Adwait Nadkarni*

Main category: cs.CR

TL;DR: 该论文首次大规模研究密码API的非自然使用，通过分析20,508个Android应用中的140,431个API调用，开发复杂度指标对5,704个代表性调用进行定性分析，揭示了高度不寻常的误用、规避代码以及流行工具的检测局限性。


<details>
  <summary>Details</summary>
Motivation: 现有密码API误用检测工具只能检测基本表达形式，无法检测非平凡变体。需要了解开发者如何在真实环境中使用和误用密码API，特别是非自然使用的特征，以改进工具设计。

Method: 开发直观的复杂度指标对140,431个密码API调用进行分层，从中采样5,704个代表性调用进行定性分析，包括手动逆向工程、开发最小示例和探索本地代码。

Result: 研究产生了两个详细的非自然密码API误用分类法，以及17个关键发现，揭示了高度不寻常的误用、规避代码的存在，以及流行工具无法检测甚至轻微非常规使用的问题。

Conclusion: 研究得出四个关键启示，为未来检测非自然密码API误用的工作提供指导，强调了需要更先进的检测方法来应对复杂的API误用模式。

Abstract: Tools focused on cryptographic API misuse often detect the most basic
expressions of the vulnerable use, and are unable to detect non-trivial
variants. The question of whether tools should be designed to detect such
variants can only be answered if we know how developers use and misuse
cryptographic APIs in the wild, and in particular, what the unnatural usage of
such APIs looks like. This paper presents the first large-scale study that
characterizes unnatural crypto-API usage through a qualitative analysis of
5,704 representative API invocations. We develop an intuitive complexity metric
to stratify 140,431 crypto-API invocations obtained from 20,508 Android
applications, allowing us to sample 5,704 invocations that are representative
of all strata, with each stratum consisting of invocations with similar
complexity/naturalness. We qualitatively analyze the 5,704 sampled invocations
using manual reverse engineering, through an in-depth investigation that
involves the development of minimal examples and exploration of native code.
Our study results in two detailed taxonomies of unnatural crypto-API misuse,
along with 17 key findings that show the presence of highly unusual misuse,
evasive code, and the inability of popular tools to reason about even mildly
unconventional usage. Our findings lead to four key takeaways that inform
future work focused on detecting unnatural crypto-API misuse.

</details>


### [87] [Towards Trusted Service Monitoring: Verifiable Service Level Agreements](https://arxiv.org/abs/2510.13370)
*Fernando Castillo,Eduardo Brito,Sebastian Werner,Pille Pullonen-Raudvere,Jonathan Heiss*

Main category: cs.CR

TL;DR: 提出一个基于可信硬件和零知识证明的SLA监控框架，通过密码学方法解决服务提供商自报告指标时的信任冲突问题，实现可验证的SLA违规声明。


<details>
  <summary>Details</summary>
Motivation: 解决面向服务环境中SLA监控存在的固有信任冲突，当提供商自报告指标时会激励其少报违规情况，需要建立密码学基础来确保服务生态系统的真正可信度。

Method: 将机器可读的SLA条款转换为可验证谓词，在可信执行环境中监控；收集带时间戳的遥测数据，组织成Merkle树并生成签名证明；使用零知识证明聚合服务级别指标来评估合规性。

Result: 原型系统展示了对每小时超过100万个事件的线性扩展能力，单次违规声明的证明生成和验证时间接近常数，支持无信任的SLA执行。

Conclusion: 该框架通过密码学保证实现了三个安全属性：完整性、真实性和有效性，为服务监控中的自动化合规验证提供了密码学保障。

Abstract: Service Level Agreement (SLA) monitoring in service-oriented environments
suffers from inherent trust conflicts when providers self-report metrics,
creating incentives to underreport violations. We introduce a framework for
generating verifiable SLA violation claims through trusted hardware monitors
and zero-knowledge proofs, establishing cryptographic foundations for genuine
trustworthiness in service ecosystems. Our approach starts with
machine-readable SLA clauses converted into verifiable predicates and monitored
within Trusted Execution Environments. These monitors collect timestamped
telemetry, organize measurements into Merkle trees, and produce signed
attestations. Zero-knowledge proofs aggregate Service-Level Indicators to
evaluate compliance, generating cryptographic proofs verifiable by
stakeholders, arbitrators, or insurers in disputes, without accessing
underlying data. This ensures three security properties: integrity,
authenticity, and validity. Our prototype demonstrates linear scaling up to
over 1 million events per hour for measurements with near constant-time proof
generation and verification for single violation claims, enabling trustless SLA
enforcement through cryptographic guarantees for automated compliance
verification in service monitoring.

</details>


### [88] [Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning](https://arxiv.org/abs/2510.13322)
*Baogang Song,Dongdong Zhao,Jianwen Xiang,Qiben Xu,Zizhuo Yu*

Main category: cs.CR

TL;DR: 本文提出了首个可撤销的后门攻击范式，通过双层优化方法设计触发器，使得后门在攻击目标达成后可以被主动彻底移除。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击策略在模型遗忘机制下仍会留下可被静态分析检测的持久痕迹，需要开发能够主动撤销的后门攻击方法。

Method: 将触发器优化建模为双层优化问题，通过模拟后门注入和遗忘过程，优化触发器生成器；采用确定性划分毒化和遗忘样本减少方差，并使用PCGrad技术解决梯度冲突。

Result: 在CIFAR-10和ImageNet上的实验表明，该方法保持了与最先进后门攻击相当的攻击成功率，同时能够在遗忘后有效移除后门行为。

Conclusion: 这项工作为后门攻击研究开辟了新方向，并为机器学习系统安全带来了新的挑战。

Abstract: Backdoor attacks pose a persistent security risk to deep neural networks
(DNNs) due to their stealth and durability. While recent research has explored
leveraging model unlearning mechanisms to enhance backdoor concealment,
existing attack strategies still leave persistent traces that may be detected
through static analysis. In this work, we introduce the first paradigm of
revocable backdoor attacks, where the backdoor can be proactively and
thoroughly removed after the attack objective is achieved. We formulate the
trigger optimization in revocable backdoor attacks as a bilevel optimization
problem: by simulating both backdoor injection and unlearning processes, the
trigger generator is optimized to achieve a high attack success rate (ASR)
while ensuring that the backdoor can be easily erased through unlearning. To
mitigate the optimization conflict between injection and removal objectives, we
employ a deterministic partition of poisoning and unlearning samples to reduce
sampling-induced variance, and further apply the Projected Conflicting Gradient
(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on
CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to
state-of-the-art backdoor attacks, while enabling effective removal of backdoor
behavior after unlearning. This work opens a new direction for backdoor attack
research and presents new challenges for the security of machine learning
systems.

</details>


### [89] [Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers](https://arxiv.org/abs/2510.13462)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Haoyu Gao,Zhendong Zhao,Yilong Chen*

Main category: cs.CR

TL;DR: BadSwitch是一种针对MoE架构LLM的新型后门攻击框架，通过利用专家路由偏好，将恶意触发器嵌入到具有强任务亲和力的专家路由路径中，实现精确隐蔽的模型操控。


<details>
  <summary>Details</summary>
Motivation: MoE架构中的稀疏路由机制由于专家专业化而表现出任务偏好，这引入了一个未被充分探索的后门攻击漏洞。本研究旨在探索通过利用专家路由偏好在MoE-based LLM中注入后门的可行性和有效性。

Method: 提出BadSwitch框架，集成了任务耦合动态触发器优化和敏感度引导的Top-S专家追踪机制。在预训练期间联合优化触发器嵌入，同时识别S个最敏感专家，随后将Top-K门控机制限制在这些目标专家上。

Result: 在三种主流MoE架构上的综合评估显示，BadSwitch可以高效劫持预训练模型，成功率高达100%，同时在所有基线中保持最高的清洁准确率。在AGNews数据集上达到94.07%攻击成功率和87.18%清洁准确率，对文本级和模型级防御机制表现出强韧性。

Conclusion: BadSwitch暴露了MoE系统的安全风险，通过分析专家激活模式揭示了MoE漏洞的基本见解，有助于推进AI安全研究。

Abstract: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures
achieve impressive performance and efficiency by dynamically routing inputs to
specialized subnetworks, known as experts. However, this sparse routing
mechanism inherently exhibits task preferences due to expert specialization,
introducing a new and underexplored vulnerability to backdoor attacks. In this
work, we investigate the feasibility and effectiveness of injecting backdoors
into MoE-based LLMs by exploiting their inherent expert routing preferences. We
thus propose BadSwitch, a novel backdoor framework that integrates task-coupled
dynamic trigger optimization with a sensitivity-guided Top-S expert tracing
mechanism. Our approach jointly optimizes trigger embeddings during pretraining
while identifying S most sensitive experts, subsequently constraining the Top-K
gating mechanism to these targeted experts. Unlike traditional backdoor attacks
that rely on superficial data poisoning or model editing, BadSwitch primarily
embeds malicious triggers into expert routing paths with strong task affinity,
enabling precise and stealthy model manipulation. Through comprehensive
evaluations across three prominent MoE architectures (Switch Transformer,
QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack
pre-trained models with up to 100% success rate (ASR) while maintaining the
highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch
exhibits strong resilience against both text-level and model-level defense
mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our
analysis of expert activation patterns reveals fundamental insights into MoE
vulnerabilities. We anticipate this work will expose security risks in MoE
systems and contribute to advancing AI safety.

</details>


### [90] [How Blind and Low-Vision Users Manage Their Passwords](https://arxiv.org/abs/2510.13538)
*Alexander Ponticello,Filipo Sharevski,Simon Anell,Katharina Krombholz*

Main category: cs.CR

TL;DR: 本研究调查了盲人和低视力用户如何使用密码管理器，发现虽然所有参与者都使用密码管理器，但主要是出于便利性而非安全性考虑，因为安全功能缺乏实际可访问性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明用户在密码管理方面存在安全顾虑导致不安全实践，但缺乏对盲人和低视力用户特定需求的研究。

Method: 采用定性访谈研究方法，对33名盲人和低视力参与者进行了访谈。

Result: 所有参与者都使用密码管理器，但主要出于存储和检索密码的便利性。安全优势（生成强随机密码）因缺乏实际可访问性而被避免使用。

Conclusion: 需要改进密码管理器的实际可访问性和可用性，以建立信任和安全实践，同时保持盲人和低视力用户的自主性。

Abstract: Managing passwords securely and conveniently is still an open problem for
many users. Existing research has examined users' password management
strategies and identified pain points, such as security concerns, leading to
insecure practices. We investigate how Blind and Low-Vision (BLV) users tackle
this problem and how password managers can assist them. This paper presents the
results of a qualitative interview study with N = 33 BLV participants. We found
that all participants utilize password managers to some extent, which they
perceive as fairly accessible. However, the adoption is mainly driven by the
convenience of storing and retrieving passwords. The security advantages -
generating strong, random passwords - were avoided mainly due to the absence of
practical accessibility. Password managers do not adhere to BLV users'
underlying needs for agency, which stem from experiences with inaccessible
software and vendors who deprioritize accessibility issues. Underutilization of
password managers leads BLV users to adopt insecure practices, such as reusing
predictable passwords or resorting to 'security through obscurity' by writing
important credentials in braille. We conclude our analysis by discussing the
need to implement practical accessibility and usability improvements for
password managers as a way of establishing trust and secure practices while
maintaining BLV users' agency.

</details>


### [91] [In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](https://arxiv.org/abs/2510.13543)
*Avihay Cohen*

Main category: cs.CR

TL;DR: 提出了一个在浏览器中运行的LLM引导模糊测试框架，用于实时发现基于LLM的智能浏览器中的间接提示注入漏洞。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的智能浏览器虽然能自动化网页任务，但容易受到间接提示注入攻击，恶意指令可绕过传统网页安全边界，利用用户权限跨站点执行不当操作。

Method: 开发了一个完全在浏览器中运行的模糊测试框架，使用LLM引导来自动发现实时提示注入漏洞。

Result: 该框架能够有效检测智能浏览器中的间接提示注入安全漏洞。

Conclusion: 提出的模糊测试方法为保护智能浏览器免受提示注入攻击提供了有效的安全检测工具。

Abstract: Large Language Model (LLM) based agents integrated into web browsers (often
called agentic AI browsers) offer powerful automation of web tasks. However,
they are vulnerable to indirect prompt injection attacks, where malicious
instructions hidden in a webpage deceive the agent into unwanted actions. These
attacks can bypass traditional web security boundaries, as the AI agent
operates with the user privileges across sites. In this paper, we present a
novel fuzzing framework that runs entirely in the browser and is guided by an
LLM to automatically discover such prompt injection vulnerabilities in real
time.

</details>
