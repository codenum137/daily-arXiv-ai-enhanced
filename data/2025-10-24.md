<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.CR](#cs.CR) [Total: 13]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Communication to Completion: Modeling Collaborative Workflows with Intelligent Multi-Agent Communication](https://arxiv.org/abs/2510.19995)
*Yiming Lu,Xun Wang,Simin Ma,Shujian Liu,Sathish Reddy Indurthi,Song Wang,Haoyun Deng,Fei Liu,Kaiqiang Song*

Main category: cs.MA

TL;DR: C2C是一个可扩展的多智能体LLM系统框架，通过Alignment Factor指标和Sequential Action Framework，使智能体能够做出成本感知的通信决策，在复杂编码任务中减少约40%的完成时间。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM系统缺乏面向任务的系统化通信框架，而复杂工作空间中的团队协作需要多样化的通信策略。

Method: 提出两个关键创新：(1) Alignment Factor(AF) - 量化智能体任务对齐的新指标；(2) Sequential Action Framework - 将逐步执行与智能通信决策相结合。

Result: 在5到17个智能体的编码工作流中，C2C相比无通信和固定步骤基线，任务完成时间减少约40%，通信成本可接受，所有任务在标准配置下成功完成。

Conclusion: C2C为多智能体系统中的通信有效性测量建立了理论基础，并为复杂协作任务提供了实用框架。

Abstract: Teamwork in workspace for complex tasks requires diverse communication
strategies, but current multi-agent LLM systems lack systematic frameworks for
task oriented communication. We introduce Communication to Completion (C2C), a
scalable framework that addresses this gap through two key innovations: (1) the
Alignment Factor (AF), a novel metric quantifying agent task alignment that
directly impacts work efficiency, and (2) a Sequential Action Framework that
integrates stepwise execution with intelligent communication decisions. C2C
enables agents to make cost aware communication choices, dynamically improving
task understanding through targeted interactions. We evaluated C2C on realistic
coding workflows across three complexity tiers and team sizes from 5 to 17
agents, comparing against no communication and fixed steps baselines. The
results show that C2C reduces the task completion time by about 40% with
acceptable communication costs. The framework completes all tasks successfully
in standard configurations and maintains effectiveness at scale. C2C
establishes both a theoretical foundation for measuring communication
effectiveness in multi-agent systems and a practical framework for complex
collaborative tasks.

</details>


### [2] [Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks](https://arxiv.org/abs/2510.20469)
*Horacio Paggi,Juan A. Lara,Javier Soriano*

Main category: cs.MA

TL;DR: 本文探讨了信息融合从传统分层结构向协作式全息融合的范式转变，分析了在资源受限条件下全息结构的形成机制及其优势。


<details>
  <summary>Details</summary>
Motivation: 随着信息融合在非军事领域和边缘组织中应用增多，传统分层融合方法已不适应需求，需要更灵活的全息融合结构来应对资源约束和不确定性。

Method: 基于多智能体系统模型，研究在资源受限条件下全息结构的形成过程，分析完全互联元素如何通过信息融合优化模糊性和不确定性的影响。

Result: 研究表明全息结构在资源约束下自然形成，具有适应性、自主性和协作能力，能够在环境突变或组件故障时保持系统功能。

Conclusion: 全息融合结构比传统分层结构更适合现代信息融合需求，特别是在资源受限和动态变化的环境中，能够有效应对通信限制和系统故障。

Abstract: There has recently been a major advance with respect to how information
fusion is performed. Information fusion has gone from being conceived as a
purely hierarchical procedure, as is the case of traditional military
applications, to now being regarded collaboratively, as holonic fusion, which
is better suited for civil applications and edge organizations. The above
paradigm shift is being boosted as information fusion gains ground in different
non-military areas, and human-computer and machine-machine communications,
where holarchies, which are more flexible structures than ordinary, static
hierarchies, become more widespread. This paper focuses on showing how holonic
structures tend to be generated when there are constraints on resources
(energy, available messages, time, etc.) for interactions based on a set of
fully intercommunicating elements (peers) whose components fuse information as
a means of optimizing the impact of vagueness and uncertainty present message
exchanges. Holon formation is studied generically based on a multiagent system
model, and an example of its possible operation is shown. Holonic structures
have a series of advantages, such as adaptability, to sudden changes in the
environment or its composition, are somewhat autonomous and are capable of
cooperating in order to achieve a common goal. This can be useful when the
shortage of resources prevents communications or when the system components
start to fail.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 本文简化了Khoury和Schild的轮消除自约简技术，并应用该技术证明了最大b-匹配和边着色的随机LOCAL算法下界。


<details>
  <summary>Details</summary>
Motivation: Khoury和Schild关于最大匹配的随机LOCAL算法下界证明长达25页且技术复杂，难以理解和推广。历史上证明简化对理解图问题复杂性有重要推动作用。

Method: 提出了轮消除自约简技术的简化版本，并应用该技术分析最大b-匹配和边着色问题。

Result: 证明了最大b-匹配需要Ω(min{log₁₊ᵦΔ, logΔn})和Ω(√log₁₊ᵦn)轮；边着色需要Ω(min{logΔ, logΔn})和Ω(√logn)轮。

Conclusion: 简化了轮消除自约简技术，为理解LOCAL模型中图问题的复杂性提供了更易消化的证明框架，并推广到更一般的问题。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [4] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 提出AsyncHZP（异步分层零并行），一种新颖的异步ZeRO变体，通过自适应分片和多流异步调度，在保持内存效率的同时显著减少通信开销，实现优于传统ND并行的大规模训练性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模集群上语言模型的训练效率和可扩展性存在瓶颈，主流ND并行方法复杂繁琐，而灵活替代方案如ZeRO则受通信开销限制。

Method: AsyncHZP采用自适应分片策略，在不同副本组间重新分片参数、梯度和优化器状态，并设计多流异步调度方法，在专用后台线程中执行参数全收集和梯度归约分散操作。

Result: 在密集模型和混合专家模型上的实证评估表明，AsyncHZP在大规模下保持稳健稳定性，持续优于经典ND并行，无需复杂策略调优即可达到最先进性能。

Conclusion: AsyncHZP简化了实现高效大规模训练的路径，在保持内存效率的同时显著提升训练性能。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [5] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 提出了一个HPC-QC全栈框架，通过模块化硬件/设备无关的软件集成方法实现高性能计算与量子计算的混合工作负载开发，支持在现有成熟HPC编程环境中进行量子编程、调度和编译。


<details>
  <summary>Details</summary>
Motivation: 满足对可扩展高性能计算和量子计算集成日益增长的需求，构建统一的量子-经典编程环境。

Method: 采用自适应电路编织虚拟机将大型量子电路分割为适合较小噪声量子设备的子电路，利用Cray LLVM编译框架以可重定向方式转换LLVM IR和量子IR，支持从HPC元程序中调用量子内核。

Result: 在HPE EX超级计算机上成功演示了多个混合HPC-QC多节点多CPU和GPU工作负载，包括求解线性方程组、量子优化和模拟量子相变。

Conclusion: 该工作为基于经典HPC软件栈构建统一量子-经典编程环境提供了框架。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [6] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: NCCLX是一个专为大规模语言模型优化的集体通信框架，支持超过10万GPU集群，显著提升通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统通信方法在扩展到数十万GPU规模时面临吞吐量和延迟限制，阻碍了最先进模型的发展和部署。

Method: 开发NCCLX集体通信框架，针对LLM全生命周期优化，从大规模训练的同步需求到推理的低延迟要求。

Result: 在Llama4模型上的实证评估显示通信效率显著提升。

Conclusion: 该研究为下一代LLM在空前规模上运行提供了稳健解决方案。

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [7] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: FLAS是一种结合主动和反应式方法的自动扩缩容系统，用于分布式服务，通过预测模型和反应式应急系统确保服务级别协议。


<details>
  <summary>Details</summary>
Motivation: 云计算弹性需求日益增长，现有自动扩缩容系统在应对动态负载变化时存在局限性，需要更智能的解决方案来确保服务性能。

Method: FLAS结合预测模型（预测高级指标趋势）和反应式应急系统（从资源使用指标估计高级指标），减少必要检测，可适配不同应用。

Result: 在基于内容发布-订阅中间件的评估中，FLAS在99%以上时间确保性能要求得到满足，包括最坏场景测试。

Conclusion: FLAS是首个针对基于内容发布-订阅分布式系统的自动扩缩容解决方案，具有通用性，能有效确保服务级别协议合规性。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [8] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 提出了一种在动态边缘环境中自动构建和评估性能预测器的方法，该方法优先考虑准确性和推理时间，在电子显微镜工作流等实时应用中实现高达90%的预测精度，推理时间小于往返时间的1%。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的动态边缘环境中，由于多个应用程序的共存和节点异构性，实现可预测的性能仍然具有挑战性，需要有效的性能预测来支持调度和资源管理。

Method: 自动构建和评估各种性能预测器的方法，通过历史监控指标状态训练预测器，并在动态共存场景下跨多个服务器进行评估，优先考虑准确性和推理时间。

Result: 预测器在动态共存场景中达到高达90%的准确率，同时保持推理时间小于往返时间的1%，能够有效预测电子显微镜工作流等实时应用的性能。

Conclusion: 需要系统性的方法来选择服务器特定的预测器，通过联合优化准确性和推理延迟，将此类预测器集成到边缘环境中可以提高资源利用率并实现可预测的性能。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [9] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 该论文研究了在边缘和云环境中使用RTT预测器来改进请求路由，通过预测应用延迟实现性能感知的负载均衡。


<details>
  <summary>Details</summary>
Motivation: 分布式应用对低端到端延迟的需求日益增长，而传统的负载均衡策略通常是反应式的，依赖过时或粗粒度的指标，导致次优的路由决策和增加的尾部延迟。

Method: 开发轻量级且准确的RTT预测器，基于从Kubernetes管理的GPU集群收集的时间序列监控数据进行训练，利用高度相关的监控指标子集保持低开销。

Result: 预测器达到高达95%的准确率，预测延迟保持在应用RTT的10%以内。通过基于仿真的评估显示，性能感知的负载均衡能显著减少应用RTT并最小化资源浪费。

Conclusion: 研究结果表明将预测性负载均衡集成到未来生产系统中的可行性，并确定了确保在资源受限集群中有效部署预测器所需的最小预测准确度阈值和关键系统级因素。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: ReGraphT是一个无需训练的检索增强生成框架，通过构建CUDA优化轨迹的推理图，将大语言模型的推理能力迁移到小模型上，使小模型在CUDA代码生成任务中接近大模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在生成优化CUDA代码方面表现出潜力，但云端API存在代码泄露风险，本地部署计算成本高且效率低。小模型虽然更轻量且保护隐私，但在复杂CUDA生成任务中推理能力有限导致性能不佳。

Method: 提出ReGraphT框架，将CUDA优化轨迹组织成结构化推理图，将组合CUDA优化建模为状态转换，并利用蒙特卡洛图搜索进行高效探索。

Result: ReGraphT在CUDAEval和ParEval基准测试中平均实现2.33倍加速，优于HPC专用微调模型和其他检索增强方法。当与DeepSeek-Coder-V2-Lite-Instruct和Qwen2.5-Coder-7B-Instruct配合使用时，小模型能够接近大模型性能。

Conclusion: ReGraphT成功地将大语言模型的推理能力迁移到小模型上，使小模型在CUDA代码生成任务中达到接近大模型的性能，同时避免了隐私风险和过高的计算开销。

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>


### [11] [FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning](https://arxiv.org/abs/2510.19893)
*Shiqi Dai,Wei Dai,Jiaee Cheong,Paul Pu Liang*

Main category: cs.LG

TL;DR: FairGRPO是一种分层强化学习方法，通过自适应重要性加权和自动发现潜在人口群体来解决医疗AI系统中的公平性问题，在7个临床数据集上显著减少了预测差异并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统在诊断能力上表现出色，但在不同人口群体间存在性能差异，对代表性不足的群体造成实际伤害。现有的多模态推理基础模型通过强化学习训练时会继承并放大训练数据中的偏见。

Method: 提出FairGRPO分层强化学习方法，采用基于代表性、任务难度和数据来源的自适应重要性加权优势。当缺乏人口统计标签时，使用无监督聚类自动发现潜在人口群体。

Result: 在7个临床诊断数据集上，FairGRPO将预测差异减少了27.2%，同时F1分数提高了12.49%。训练动态分析显示FairGRPO在优化过程中持续改善公平性。

Conclusion: FairGRPO有效解决了医疗AI中的公平性问题，并发布了FairMedGemma-4B模型，在保持最先进性能的同时显著减少了跨人口群体的差异。

Abstract: Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.

</details>


### [12] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: FINDER是一个用于分析噪声数据集的分类框架，通过随机特征和KLE分解来处理低信噪比、小样本等挑战性数据，在阿尔茨海默病分类和森林砍伐遥感检测中取得突破性成果。


<details>
  <summary>Details</summary>
Motivation: 处理噪声数据集（低信噪比、小样本、数据收集错误等）是分类方法的重要研究前沿，需要开发能够有效处理数据随机性的新方法。

Method: 将经验数据集视为随机场的实现，映射到希尔伯特空间构建随机特征，使用Kosambi-Karhunen-Loève展开分解为可计算不可约分量，通过特征分解实现分类。

Result: 在数据稀缺的科学领域验证了FINDER，在阿尔茨海默病分期分类和森林砍伐遥感检测方面取得了最先进的突破性成果。

Conclusion: FINDER通过将基本随机分析思想融入特征学习和推理阶段，为噪声数据集提供了优化的分类解决方案，并讨论了其适用条件和局限性。

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [13] [Beyond the Ideal: Analyzing the Inexact Muon Update](https://arxiv.org/abs/2510.19933)
*Egor Shulgin,Sultan AlRashed,Francesco Orabona,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文首次分析了Muon优化器中核心的不精确正交化更新，揭示了实际近似方案的不精确性与最优步长和动量参数之间的基本耦合关系。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在实践中依赖快速近似正交化，但所有先前的理论研究都分析了理想化的、计算不可行的精确SVD更新版本，存在理论与实践的脱节。

Method: 在线性最小化Oracle（LMO）优化框架内，引入现实加性误差模型来捕捉实际近似方案的不精确性，分析不精确正交化更新的性能边界。

Result: 分析得出明确的性能边界，量化了性能下降与LMO不精确性/误差的函数关系，揭示了不精确性与最优步长和动量参数的基本耦合：较低的Oracle精度需要更小的步长但更大的动量参数。

Conclusion: 近似过程（如牛顿-舒尔茨步数）从实现细节提升为必须与学习调度共同调优的关键参数，NanoGPT实验直接证实了预测的耦合关系。

Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware
alternative to AdamW, demonstrating strong performance in large-scale training
of neural networks. However, a critical theory-practice disconnect exists:
Muon's efficiency relies on fast, approximate orthogonalization, yet all prior
theoretical work analyzes an idealized, computationally intractable version
assuming exact SVD-based updates. This work moves beyond the ideal by providing
the first analysis of the inexact orthogonalized update at Muon's core. We
develop our analysis within the general framework of Linear Minimization Oracle
(LMO)-based optimization, introducing a realistic additive error model to
capture the inexactness of practical approximation schemes. Our analysis yields
explicit bounds that quantify performance degradation as a function of the LMO
inexactness/error. We reveal a fundamental coupling between this inexactness
and the optimal step size and momentum: lower oracle precision requires a
smaller step size but larger momentum parameter. These findings elevate the
approximation procedure (e.g., the number of Newton-Schulz steps) from an
implementation detail to a critical parameter that must be co-tuned with the
learning schedule. NanoGPT experiments directly confirm the predicted coupling,
with optimal learning rates clearly shifting as approximation precision
changes.

</details>


### [14] [Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy](https://arxiv.org/abs/2510.19934)
*Xiang Li,Buxin Su,Chendi Wang,Qi Long,Weijie J. Su*

Main category: cs.LG

TL;DR: 本文针对去中心化联邦学习中的隐私预算量化问题，提出了两种基于f-差分隐私的隐私计算方法，能够更准确地捕捉稀疏通信、本地迭代和相关性噪声带来的隐私放大效应。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习允许用户在不与中央服务器共享数据的情况下协作，但由于存在复杂的算法组件（如去中心化通信和本地更新），准确量化隐私预算具有挑战性。

Method: 开发了两种新的f-DP计算方法：基于随机游走通信的用户对隐私泄露量化方法（PN-f-DP），以及支持通过共享秘密进行结构化噪声注入的秘密基础f-本地DP方法（Sec-f-LDP）。

Result: 在合成和真实数据集上的实验表明，与基于Rényi DP的方法相比，本文方法能够产生更紧致的(ε,δ)边界和更好的效用。

Conclusion: 研究证明了f-DP在去中心化隐私计算中的优势，为去中心化联邦学习提供了更准确的隐私保护量化框架。

Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows
local users to collaborate without sharing their data with a central server.
However, accurately quantifying the privacy budget of private FL algorithms is
challenging due to the co-existence of complex algorithmic components such as
decentralized communication and local updates. This paper addresses privacy
accounting for two decentralized FL algorithms within the $f$-differential
privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods
tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which
quantifies privacy leakage between user pairs under random-walk communication,
and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise
injection via shared secrets. By combining tools from $f$-DP theory and Markov
chain concentration, our accounting framework captures privacy amplification
arising from sparse communication, local iterations, and correlated noise.
Experiments on synthetic and real datasets demonstrate that our methods yield
consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared
to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in
decentralized privacy accounting.

</details>


### [15] [Are Greedy Task Orderings Better Than Random in Continual Linear Regression?](https://arxiv.org/abs/2510.19941)
*Matan Tsipory,Ran Levinstein,Itay Evron,Mark Kong,Deanna Needell,Daniel Soudry*

Main category: cs.LG

TL;DR: 本文分析了持续学习中线性回归的任务排序问题，特别关注贪心最大化连续任务间差异性的排序策略。研究发现贪心排序比随机排序收敛更快，但在不同秩条件下表现不同：高秩时贪心排序有与随机排序类似的损失界，而一般秩时单次贪心排序可能失败，但允许重复的贪心排序以O(1/∛k)速率收敛。


<details>
  <summary>Details</summary>
Motivation: 研究任务排序在持续学习中的作用，特别是贪心最大化任务间差异性的排序策略。虽然之前工作简要探索过这个概念，但仍存在许多开放性问题需要深入分析。

Method: 使用Kaczmarz方法文献中的工具，形式化贪心排序策略，并围绕其建立几何和代数直觉。通过线性回归随机数据和CIFAR-100分类任务的线性探测进行实证分析，并在不同秩条件下进行理论分析。

Result: 实证显示贪心排序在跨任务平均损失上比随机排序收敛更快。理论分析表明：高秩回归设置下贪心排序有与随机排序类似的损失界；一般秩条件下，单次贪心排序可能灾难性失败，而允许重复的贪心排序以O(1/∛k)速率收敛。

Conclusion: 揭示了贪心和随机排序内部及之间的细微差别，贪心排序在特定条件下能提供更好的性能，但需要谨慎处理重复策略以避免灾难性失败。

Abstract: We analyze task orderings in continual learning for linear regression,
assuming joint realizability of training data. We focus on orderings that
greedily maximize dissimilarity between consecutive tasks, a concept briefly
explored in prior work but still surrounded by open questions. Using tools from
the Kaczmarz method literature, we formalize such orderings and develop
geometric and algebraic intuitions around them. Empirically, we demonstrate
that greedy orderings converge faster than random ones in terms of the average
loss across tasks, both for linear regression with random data and for linear
probing on CIFAR-100 classification tasks. Analytically, in a high-rank
regression setting, we prove a loss bound for greedy orderings analogous to
that of random ones. However, under general rank, we establish a
repetition-dependent separation. Specifically, while prior work showed that for
random orderings, with or without replacement, the average loss after $k$
iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass
greedy orderings may fail catastrophically, whereas those allowing repetition
converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances
within and between greedy and random orderings.

</details>


### [16] [Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets](https://arxiv.org/abs/2510.19950)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出一种新颖的椭圆不确定性集合来处理金融市场中RL训练与部署环境不匹配问题，特别是市场影响的定向特性，通过封闭形式解实现高效稳健策略评估。


<details>
  <summary>Details</summary>
Motivation: 传统稳健RL方法依赖对称结构，无法捕捉市场影响的定向特性，导致训练与部署环境不匹配问题显著影响性能。

Method: 开发椭圆不确定性集合，建立隐式和显式封闭形式解来计算最坏情况不确定性，实现高效稳健策略评估。

Result: 在单资产和多资产交易任务中，该方法实现了更优的夏普比率，并在交易量增加时保持稳健性。

Conclusion: 该方法为金融市场中的RL提供了更忠实和可扩展的稳健方法，有效解决了市场影响带来的环境不匹配问题。

Abstract: In financial applications, reinforcement learning (RL) agents are commonly
trained on historical data, where their actions do not influence prices.
However, during deployment, these agents trade in live markets where their own
transactions can shift asset prices, a phenomenon known as market impact. This
mismatch between training and deployment environments can significantly degrade
performance. Traditional robust RL approaches address this model
misspecification by optimizing the worst-case performance over a set of
uncertainties, but typically rely on symmetric structures that fail to capture
the directional nature of market impact. To address this issue, we develop a
novel class of elliptic uncertainty sets. We establish both implicit and
explicit closed-form solutions for the worst-case uncertainty under these sets,
enabling efficient and tractable robust policy evaluation. Experiments on
single-asset and multi-asset trading tasks demonstrate that our method achieves
superior Sharpe ratio and remains robust under increasing trade volumes,
offering a more faithful and scalable approach to RL in financial markets.

</details>


### [17] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出了一种新的无偏梯度估计器家族，通过将方向导数重新表述为伸缩级数并精心设计采样分布，消除了现有零阶优化方法中的偏差问题，在保持有利方差的同时实现了无偏估计。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法存在固有的偏差问题，除非扰动步长趋近于零，否则大多数梯度估计器都是有偏的，这限制了方法的性能和应用。

Method: 通过将方向导数重新表述为伸缩级数，从精心设计的分布中采样，构建无偏梯度估计器，分析了理论性质，推导了四种具体构造的最优缩放分布和扰动步长。

Result: 理论证明使用所提估计器的SGD在光滑非凸目标上达到最优复杂度，在合成任务和语言模型微调实验中的准确性和收敛性优于标准方法。

Conclusion: 提出的无偏梯度估计器家族有效解决了零阶优化中的偏差问题，在理论和实验上都表现出优越性能，为梯度不可得或计算昂贵的优化问题提供了更好的解决方案。

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [18] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 本文研究了零点梯度估计器，发现当扰动步长趋近于零时，能够最小化渐近方差的扰动分布是方向对齐的扰动，而非传统固定长度的扰动。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定长度的随机扰动，但忽略了扰动方向与真实梯度对齐可能带来的优势。本文旨在探索这种方向对齐扰动(DAP)的理论和实证特性。

Method: 将问题建模为扰动分布空间上的约束函数优化问题，提出方向对齐扰动(DAP)方案，该方案能够自适应地在关键方向上提供更高精度。

Result: 理论分析表明DAP方案优于传统方法，实证评估在合成问题和实际任务中都验证了DAP在特定条件下的优越性能。

Conclusion: 方向对齐的随机扰动在零点梯度估计中具有显著优势，为随机梯度下降算法提供了更广泛的扰动选择范围，并扩展了现有的复杂度界限。

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [19] [Towards Strong Certified Defense with Universal Asymmetric Randomization](https://arxiv.org/abs/2510.19977)
*Hanbin Hong,Ashish Kundu,Ali Payani,Binghui Wang,Yuan Hong*

Main category: cs.LG

TL;DR: UCAN提出了一种使用各向异性噪声的通用认证对抗鲁棒性方法，通过将现有随机平滑方法从对称噪声分布转换为非对称分布，显著提升了认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有随机平滑方法主要使用各向同性噪声分布，忽略了输入和数据维度的异质性，限制了鲁棒性认证的效果。

Method: 开发了通用理论框架，支持多种噪声分布用于不同ℓ_p范数的认证鲁棒性，并设计了三个噪声参数生成器来优化各向异性噪声参数。

Result: 在MNIST、CIFAR10和ImageNet数据集上，UCAN在大认证半径下的认证准确率比现有最先进方法提升了高达182.6%。

Conclusion: UCAN通过各向异性噪声显著提升了随机平滑方法的认证对抗鲁棒性，为机器学习模型提供了更有效的防御机制。

Abstract: Randomized smoothing has become essential for achieving certified adversarial
robustness in machine learning models. However, current methods primarily use
isotropic noise distributions that are uniform across all data dimensions, such
as image pixels, limiting the effectiveness of robustness certification by
ignoring the heterogeneity of inputs and data dimensions. To address this
limitation, we propose UCAN: a novel technique that \underline{U}niversally
\underline{C}ertifies adversarial robustness with \underline{A}nisotropic
\underline{N}oise. UCAN is designed to enhance any existing randomized
smoothing method, transforming it from symmetric (isotropic) to asymmetric
(anisotropic) noise distributions, thereby offering a more tailored defense
against adversarial attacks. Our theoretical framework is versatile, supporting
a wide array of noise distributions for certified robustness in different
$\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the
classifier's prediction over perturbed inputs with provable robustness bounds
through tailored noise injection. Additionally, we develop a novel framework
equipped with three exemplary noise parameter generators (NPGs) to optimally
fine-tune the anisotropic noise parameters for different data dimensions,
allowing for pursuing different levels of robustness enhancements in
practice.Empirical evaluations underscore the significant leap in UCAN's
performance over existing state-of-the-art methods, demonstrating up to
$182.6\%$ improvement in certified accuracy at large certified radii on MNIST,
CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at
\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}

</details>


### [20] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: 提出了一种名为ADP-VRSGP的自适应差分隐私去中心化学习方法，通过动态调整噪声方差和学习率来提升模型性能和训练效率，同时提供节点级个性化隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有使用固定方差噪声的差分隐私方法会降低模型性能和训练效率，需要一种能够动态调整噪声和学习率的自适应方法来解决这些限制。

Method: 采用逐步衰减调度动态调整噪声方差和学习率，引入渐进梯度融合策略利用历史梯度，结合去中心化push-sum和聚合技术，适用于时变通信拓扑。

Result: 理论分析表明ADP-VRSGP在适当学习率下实现鲁棒收敛，显著提高训练稳定性和速度。实验验证该方法在多种场景下优于现有基线方法。

Conclusion: ADP-VRSGP方法有效解决了隐私保护去中心化学习中的挑战，在保持隐私保护的同时显著提升了模型性能和训练效率。

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [21] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: 该研究挑战了时间序列预测中的"长序列信息增益假设"，发现适当截断历史数据反而能提高预测精度，提出了基于信息瓶颈理论的AMRC方法，通过动态掩码损失和表示一致性约束来抑制冗余特征学习。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测模型基于"长序列信息增益假设"，但研究发现现有模型在训练中会学习大量冗余特征（如噪声或不相关波动），从而影响有效信号提取。

Method: 提出AMRC方法，包含两个核心组件：1）动态掩码损失，自适应识别高区分度的时间段来指导模型训练；2）表示一致性约束，稳定输入、标签和预测之间的映射关系。

Result: 实验结果表明AMRC有效抑制了冗余特征学习，同时显著提高了模型性能。

Conclusion: 这项工作不仅挑战了时间建模中的传统假设，还为开发高效鲁棒的预测模型提供了新的理论见解和方法突破。

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [22] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: 本研究使用监督学习和决策树分类器，在CAD建模的平面图中基于RSSI数据进行RFID定位分类，旨在解决国防资产存储中的安全监控问题。


<details>
  <summary>Details</summary>
Motivation: 射频识别(RFID)跟踪技术可用于国防资产的安全存储，但由于传感器特异性差（包括长距离检测、欺骗和伪造等漏洞），可能导致错误检测和操作安全事件。

Method: 采用监督学习模拟，使用真实的接收信号强度指示器(RSSI)数据和决策树分类，在CAD建模的平面图中对12个实验室区域进行分类定位。处理了约98万条原始数据，使用类别权重解决类别不平衡问题，并在5000个平衡观测值上进行训练。

Result: 模型整体准确率为34.2%，多个区域（F、G、H等）的F1分数超过0.40。但稀有类别（特别是LabZoneC）经常被误分类，即使使用了类别权重。计算了邻接感知混淆矩阵以更好地解释物理相邻区域。

Conclusion: 基于RSSI的决策树可以在现实模拟中应用，实现国防供应链物流的区域级异常检测或错位监控。通过更好的天线布局或额外传感器以及与其他模态的传感器融合，可以改善低覆盖和低信号区域的可靠分类性能。

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [23] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: 提出了SALT框架，通过构建轨迹图来提供更细粒度的优势分配，解决了基于群体的强化学习算法中稀疏奖励导致的训练不稳定问题，无需修改rollout过程且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮任务中表现出色，但在复杂多步任务中面临挑战。基于群体的强化学习算法如GRPO仅依赖稀疏结果奖励，导致训练不稳定和次优策略，因为多步交互中有利和不利动作相互交织。

Method: SALT框架从相同提示的轨迹构建图，量化每个步骤的质量并相应分配优势。这是一个即插即用的模块，可与现有基于群体的RL算法无缝集成，无需修改rollout过程且计算开销可忽略。

Result: 在WebShop、ALFWorld和AppWorld基准测试上的广泛实验表明，SALT在不同模型规模下都能持续提升性能。

Conclusion: SALT通过细粒度的优势分配有效解决了基于群体RL算法的训练不稳定问题，提供了可操作的见解并验证了设计选择。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [24] [Speculative Sampling for Parametric Temporal Point Processes](https://arxiv.org/abs/2510.20031)
*Marin Biloš,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 提出了一种基于拒绝采样的新算法，能够从现有TPP模型中并行精确采样多个未来值，无需架构更改或重新训练，在真实数据集上实现了经验加速。


<details>
  <summary>Details</summary>
Motivation: 传统时间点过程模型采用自回归方法，使得采样过程本质上是顺序的，限制了效率。需要解决表达性建模与高效并行生成之间的差距。

Method: 基于拒绝采样的算法，能够并行生成多个未来事件值，同时保持与原始模型相同的分布特性。

Result: 该方法在真实世界数据集上展示了经验加速效果，为大规模TPP应用提供了高效的并行生成能力。

Conclusion: 该算法成功弥合了表达性建模与高效并行生成之间的差距，为时间点过程的大规模应用提供了实用的解决方案。

Abstract: Temporal point processes are powerful generative models for event sequences
that capture complex dependencies in time-series data. They are commonly
specified using autoregressive models that learn the distribution of the next
event from the previous events. This makes sampling inherently sequential,
limiting efficiency. In this paper, we propose a novel algorithm based on
rejection sampling that enables exact sampling of multiple future values from
existing TPP models, in parallel, and without requiring any architectural
changes or retraining. Besides theoretical guarantees, our method demonstrates
empirical speedups on real-world datasets, bridging the gap between expressive
modeling and efficient parallel generation for large-scale TPP applications.

</details>


### [25] [Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards](https://arxiv.org/abs/2510.20055)
*Yuwei Cheng,Zifeng Zhao,Haifeng Xu*

Main category: cs.LG

TL;DR: 该论文提出了一种考虑广告延迟效应、累积影响和用户异质性的个性化竞价策略，通过上下文马尔可夫决策过程建模，结合两阶段最大似然估计和强化学习算法，实现了接近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在线广告平台需要有效的竞价策略来最大化利润，但现有研究往往未能同时考虑广告的延迟和长期效应、累积影响（如强化或疲劳效应）以及用户异质性这三个关键因素。

Method: 将广告竞价建模为具有延迟泊松奖励的上下文马尔可夫决策过程，提出两阶段最大似然估计器结合数据分割策略，并基于此设计强化学习算法来推导个性化竞价策略。

Result: 该方法实现了接近最优的遗憾界 $\tilde{O}{(dH^2\sqrt{T})}$，其中 $d$ 是上下文维度，$H$ 是轮数，$T$ 是客户数量。理论结果通过仿真实验得到验证。

Conclusion: 所提出的方法能够有效处理广告竞价中的延迟效应、累积影响和用户异质性，为在线广告平台提供了高效的个性化竞价策略解决方案。

Abstract: Online advertising platforms use automated auctions to connect advertisers
with potential customers, requiring effective bidding strategies to maximize
profits. Accurate ad impact estimation requires considering three key factors:
delayed and long-term effects, cumulative ad impacts such as reinforcement or
fatigue, and customer heterogeneity. However, these effects are often not
jointly addressed in previous studies. To capture these factors, we model ad
bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson
rewards. For efficient estimation, we propose a two-stage maximum likelihood
estimator combined with data-splitting strategies, ensuring controlled
estimation error based on the first-stage estimator's (in)accuracy. Building on
this, we design a reinforcement learning algorithm to derive efficient
personalized bidding strategies. This approach achieves a near-optimal regret
bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension,
$H$ is the number of rounds, and $T$ is the number of customers. Our
theoretical findings are validated by simulation experiments.

</details>


### [26] [Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs](https://arxiv.org/abs/2510.20064)
*Hongyi Liu,Jiaji Huang,Zhen Jia,Youngsuk Park,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一种在线草稿模型选择算法，在推测解码中与最佳草稿模型竞争，显著提升推理速度，特别适用于需要长推理链的场景。


<details>
  <summary>Details</summary>
Motivation: 解决推测解码中的在线草稿模型选择问题，旨在为每个查询选择最优的草稿模型，提高大语言模型推理效率。

Method: 设计在线学习算法，能够评估所有草稿模型而无需额外查询目标模型，适用于单草稿、多草稿和草稿树等推测解码方法。

Result: 实验表明该方法在开源大语言模型和多样化数据集上显著优于现有最佳方法EAGLE3和BanditSpec基线，特别在需要长推理链的领域表现突出。

Conclusion: 提出的在线草稿模型选择方法能够有效加速大语言模型推理，在多种推测解码场景下都表现出优越性能。

Abstract: Speculative decoding is widely used in accelerating large language model
(LLM) inference. In this work, we focus on the online draft model selection
problem in speculative decoding. We design an algorithm that provably competes
with the best draft model in hindsight for each query in terms of either the
token acceptance probability or expected acceptance length. In particular, we
show that we can accurately evaluate all draft models, instead of only the
chosen model without incurring additional queries to the target model, which
allows us to improve exponentially over the existing bandit-based approach as
the number of draft models increases. Our approach is generically applicable
with any speculative decoding methods (single draft, multi-drafts and
draft-trees). Moreover, we design system-efficient versions of online learners
and demonstrate that the overhead in computation and latency can be
substantially reduced. We conduct extensive experiments on open-source LLMs and
diverse datasets, demonstrating that our methods substantially outperform the
state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains
where specialized domain-expert drafters are available, especially when long
reasoning chains are required.

</details>


### [27] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: 本文提出了耦合Transformer自编码器（CTAE），这是一个能够同时处理多脑区神经记录的非平稳非线性动态，并将共享信号与区域特定信号分离的序列模型。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐或多视图方法忽略了时间结构，而动态潜变量模型虽然能捕捉时间依赖性，但通常局限于单一脑区、假设线性读出或混淆共享和私有信号。

Method: CTAE使用Transformer编码器和解码器来捕捉长程神经动态，并明确将每个区域的潜空间划分为正交的共享和私有子空间。

Result: 在两个高密度电生理数据集上的实验表明，CTAE提取的表征比现有方法能更好地解码行为变量。

Conclusion: CTAE为多脑区神经记录分析提供了一个有效的框架，能够同时处理复杂动态和信号分离问题。

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [28] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: ShapeX是一个创新的时间序列分类解释框架，通过将时间序列分割为有意义的shapelet驱动片段并使用Shapley值评估其显著性，解决了现有方法忽视关键shapelets的问题。


<details>
  <summary>Details</summary>
Motivation: 在医疗和金融等高风险应用中，时间序列分类模型的透明度至关重要。现有后验时间序列解释方法主要关注时间步级别的特征归因，忽视了分类结果主要由关键shapelets驱动的基本先验。

Method: 提出了ShapeX框架，核心是Shapelet Describe-and-Detect (SDD)框架，有效学习分类所需的各种shapelets，将时间序列分割为shapelet驱动片段，并使用Shapley值评估其显著性。

Result: 在合成和真实数据集上的实验结果表明，ShapeX在识别最相关子序列方面优于现有方法，提高了时间序列解释的精度和因果保真度。

Conclusion: ShapeX能够产生揭示因果关系而非仅仅是相关性的解释，这得益于shapelets的原子性特性，为高风险应用中的时间序列分类提供了更可靠的解释。

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [29] [Competition is the key: A Game Theoretic Causal Discovery Approach](https://arxiv.org/abs/2510.20106)
*Amartya Roy,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种基于博弈论强化学习的因果发现框架，通过DDQN智能体与强基线方法（GES或GraN-DAG）竞争，实现了理论保证与可扩展性的统一。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法存在根本性差距：GES和GraN-DAG等方法具有强实证性能但缺乏有限样本保证，而理论上有原则的方法无法扩展到大规模图。

Method: 采用博弈论强化学习框架，DDQN智能体直接与强基线方法竞争，始终从对手的解决方案进行热启动。

Result: 在合成SEMs（30节点）上，观察到的错误概率随样本量n衰减，与理论紧密匹配。在真实世界基准测试中，始终改进GES和GraN-DAG，同时保持理论安全性，可扩展到Hepar2（70节点）、Dream（100节点）和Andes（220节点）等大型图。

Conclusion: 建立了一类新的基于强化学习的因果发现算法，同时具备可证明的一致性、样本效率和实际可扩展性，标志着将实证性能与严格有限样本理论统一的关键一步。

Abstract: Causal discovery remains a central challenge in machine learning, yet
existing methods face a fundamental gap: algorithms like GES and GraN-DAG
achieve strong empirical performance but lack finite-sample guarantees, while
theoretically principled approaches fail to scale. We close this gap by
introducing a game-theoretic reinforcement learning framework for causal
discovery, where a DDQN agent directly competes against a strong baseline (GES
or GraN-DAG), always warm-starting from the opponent's solution. This design
yields three provable guarantees: the learned graph is never worse than the
opponent, warm-starting strictly accelerates convergence, and most importantly,
with high probability the algorithm selects the true best candidate graph. To
the best of our knowledge, our result makes a first-of-its-kind progress in
explaining such finite-sample guarantees in causal discovery: on synthetic SEMs
(30 nodes), the observed error probability decays with n, tightly matching
theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,
Dream, and Andes, our method consistently improves upon GES and GraN-DAG while
remaining theoretically safe. Remarkably, it scales to large graphs such as
Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these
results establish a new class of RL-based causal discovery algorithms that are
simultaneously provably consistent, sample-efficient, and practically scalable,
marking a decisive step toward unifying empirical performance with rigorous
finite-sample theory.

</details>


### [30] [On pattern classification with weighted dimensions](https://arxiv.org/abs/2510.20107)
*Ayatullah Faruk Mollah*

Main category: cs.LG

TL;DR: 本文提出了一种新的维度加权方案，并将其融入KNN分类器中，通过加权Minkowski距离改进传统KNN分类器，在基因表达等高维小样本数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在多维样本的模式分类中，基于权重的维度距离度量至关重要，但传统欧氏距离存在诸多问题。需要开发更有效的维度加权方案来准确反映样本间的相似度。

Method: 开发了新颖的维度加权方案，并将其整合到KNN分类器中，使用加权Minkowski距离来调节包含k个参考样本区域的大小和形状。

Result: 在合成和真实数据集上表现优于传统KNN，特别是在基因表达数据集上，分类准确率在所有交叉验证实验中均有显著提升（约10%）。

Conclusion: 该方法通过加权Minkowski距离和维度加权方案，为KNN分类器提供了重要的一般化改进，特别适用于高维小样本数据的模式分类。

Abstract: Studies on various facets of pattern classification is often imperative while
working with multi-dimensional samples pertaining to diverse application
scenarios. In this notion, weighted dimension-based distance measure has been
one of the vital considerations in pattern analysis as it reflects the degree
of similarity between samples. Though it is often presumed to be settled with
the pervasive use of Euclidean distance, plethora of issues often surface. In
this paper, we present (a) a detail analysis on the impact of distance measure
norms and weights of dimensions along with visualization, (b) a novel weighting
scheme for each dimension, (c) incorporation of this dimensional weighting
schema into a KNN classifier, and (d) pattern classification on a variety of
synthetic as well as realistic datasets with the developed model. It has
performed well across diverse experiments in comparison to the traditional KNN
under the same experimental setups. Specifically, for gene expression datasets,
it yields significant and consistent gain in classification accuracy (around
10%) in all cross-validation experiments with different values of k. As such
datasets contain limited number of samples of high dimensions, meaningful
selection of nearest neighbours is desirable, and this requirement is
reasonably met by regulating the shape and size of the region enclosing the k
number of reference samples with the developed weighting schema and appropriate
norm. It, therefore, stands as an important generalization of KNN classifier
powered by weighted Minkowski distance with the present weighting schema.

</details>


### [31] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 提出一种完全解耦的训练策略来防止原型自监督学习中的原型崩溃问题，通过将原型和编码器分别在不同目标下学习，使用在线EM风格过程更新原型，无需显式正则化即可获得多样化原型和更好的下游性能。


<details>
  <summary>Details</summary>
Motivation: 原型自监督学习方法普遍存在部分原型崩溃问题，多个原型收敛到几乎相同的表示，这破坏了提供多样化目标来指导编码器学习丰富表示的核心目的，现有方法只是通过过参数化或临时正则化来缓解症状而非解决根本原因。

Method: 引入完全解耦的训练策略，将原型和编码器在不同目标下分别学习。具体将原型建模为高斯混合模型，使用在线EM风格过程独立于编码器损失进行更新。

Result: 这种简单而原则性的解耦方法无需显式正则化即可消除原型崩溃，产生持续多样化的原型和更强的下游性能。

Conclusion: 通过打破原型和编码器的联合优化，从根本上解决了原型崩溃问题，为原型自监督学习提供了更有效和原则性的解决方案。

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [32] [There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance](https://arxiv.org/abs/2510.20119)
*Arian Prabowo,Flora D. Salim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Timeseries foundation models (TSFMs) have multiplied, yet lightweight
supervised baselines and even classical models often match them. We argue this
gap stems from the naive importation of NLP or CV pipelines. In language and
vision, large web-scale corpora densely capture human concepts i.e. there are
countless images and text of apples. In contrast, timeseries data is built to
complement the image and text modalities. There are no timeseries dataset that
contains the concept apple. As a result, the scrape-everything-online paradigm
fails for TS. We posit that progress demands a shift from opportunistic
aggregation to principled design: constructing datasets that systematically
span the space of invariance that preserve temporal semantics. To this end, we
suggest that the ontology of timeseries invariances should be built based on
first principles. Only by ensuring representational completeness through
invariance coverage can TSFMs achieve the aligned structure necessary for
generalisation, reasoning, and truly emergent behaviour.

</details>


### [33] [Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP](https://arxiv.org/abs/2510.20169)
*Tongkai Lu,Shuai Ma,Chongyang Tao*

Main category: cs.LG

TL;DR: 提出了一种基于超路径引导的邻域搜索方法（HyperNS）来解决大规模旅行商问题，通过聚类和超路径引导来减少搜索空间并提高求解质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经方法在大规模TSP实例中面临内存限制、初始解质量不高和全局引导不足的挑战，需要更有效的求解方法。

Method: 采用'先聚类后路由'策略，使用稀疏热图图将TSP实例划分为簇并抽象为超节点，生成超路径来指导初始化和优化过程，专注于与超路径相关的边来减少搜索空间。

Result: 在合成和真实数据集上的实验表明，该方法优于现有神经方法，特别是在处理更大规模实例时，显著减小了与最优解的差距。

Conclusion: HyperNS方法通过超路径引导的邻域搜索有效解决了大规模TSP实例的求解挑战，提供了更高效和有效的优化策略。

Abstract: Traveling Salesman Problem (TSP) is a classic NP-hard problem that has
garnered significant attention from both academia and industry. While
neural-based methods have shown promise for solving TSPs, they still face
challenges in scaling to larger instances, particularly in memory constraints
associated with global heatmaps, edge weights, or access matrices, as well as
in generating high-quality initial solutions and insufficient global guidance
for efficiently navigating vast search spaces. To address these challenges, we
propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for
large-scale TSP instances. Inspired by the ``clustering first, route second"
strategy, our approach initially divides the TSP instance into clusters using a
sparse heatmap graph and abstracts them as supernodes, followed by the
generation of a hyper tour to guide both the initialization and optimization
processes. This method reduces the search space by focusing on edges relevant
to the hyper tour, leading to more efficient and effective optimization.
Experimental results on both synthetic and real-world datasets demonstrate that
our approach outperforms existing neural-based methods, particularly in
handling larger-scale instances, offering a significant reduction in the gap to
the optimal solution.

</details>


### [34] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: RLEV方法通过将人类定义的价值信号直接整合到奖励函数中，扩展了基于验证奖励的强化学习框架，使大语言模型优化与可量化的人类价值信号对齐。


<details>
  <summary>Details</summary>
Motivation: 虽然基于验证奖励的强化学习在客观领域使用二元正确性奖励有效训练模型，但它忽略了并非所有任务都具有同等重要性。需要将人类价值信号直接纳入奖励函数。

Method: 使用带有明确真实价值标签的考试风格数据，将人类定义的价值信号直接整合到奖励函数中，通过价值加权梯度放大机制实现价值敏感的终止策略。

Result: RLEV在多种强化学习算法和模型规模上始终优于仅基于正确性的基线方法，不仅提高了价值加权准确性，还学习了价值敏感的终止策略：对低价值提示简洁，对高价值提示详尽。

Conclusion: RLEV在噪声价值信号下保持稳健，表明优化显式效用函数为将大语言模型与人类优先级对齐提供了实用路径。

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [35] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 本文提出了一种基于优化确定性等价的风险感知约束强化学习框架，通过处理奖励分布的尾部风险来应对高风险应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统约束强化学习主要关注期望累积奖励，但忽略了奖励分布尾部的风险事件，这对于高风险应用场景是不够的。

Method: 使用优化确定性等价来构建具有每阶段稳健性的风险感知约束强化学习框架，在强拉格朗日对偶性下确保与原问题的等价性，并可与标准RL求解器结合使用。

Result: 建立了所提算法的收敛性，并通过数值实验验证了方法的风险感知特性。

Conclusion: 该框架能够有效处理强化学习中的风险问题，为高风险应用提供了更安全的解决方案。

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [36] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [37] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: 该研究评估了使用金毛寻回犬寿命研究队列的常规实验室数据进行癌症风险分类的可行性。通过系统比较126种分析流程，发现最佳模型具有中等排序能力但临床分类性能较差，表明常规实验室数据中的癌症信号太弱且存在混淆，无法可靠区分癌症与正常衰老或其他炎症状况。


<details>
  <summary>Details</summary>
Motivation: 开发用于犬类早期癌症检测的筛查工具面临重大挑战。常规实验室数据是低成本来源，但其效用受到单个生物标志物非特异性和筛查人群中严重类别不平衡的限制。

Method: 使用金毛寻回犬寿命研究队列，在现实约束下进行癌症风险分类可行性评估。系统比较了126种分析流程，包括各种机器学习模型、特征选择方法和数据平衡技术。数据在患者层面进行分区以防止泄漏。

Result: 最佳模型（带类别加权的逻辑回归分类器和递归特征消除）表现出中等排序能力（AUROC = 0.815）但临床分类性能较差（F1-score = 0.25，阳性预测值 = 0.15）。虽然实现了高阴性预测值（0.98），但召回率不足（0.79）使其无法作为可靠的排除测试。

Conclusion: 常规实验室数据中存在统计可检测的癌症信号，但该信号太弱且存在混淆，无法可靠区分癌症与正常衰老或其他炎症状况。这项工作确立了该数据模态单独使用的关键性能上限，并强调计算兽医肿瘤学的有意义的进展需要多模态数据源的整合。

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [38] [CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks](https://arxiv.org/abs/2510.20219)
*Ke Xing,Yanjie Dong,Xiaoyi Fan,Runhao Zeng,Victor C. M. Leung,M. Jamal Deen,Xiping Hu*

Main category: cs.LG

TL;DR: 本文提出了CO-PFL算法，通过动态评估客户端贡献来解决个性化联邦学习中的数据异质性问题，采用双子空间分析和参数级个性化机制，显著提升个性化准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在数据异质性场景下存在聚合偏差问题，无法准确评估每个客户端更新的实际效用和可靠性，导致次优的个性化效果。

Method: CO-PFL算法通过分析梯度方向差异和预测偏差来动态估计客户端贡献，结合参数级个性化机制和掩码感知动量优化，实现高质量聚合和稳定更新。

Result: 在CIFAR10、CIFAR10C、CINIC10和Mini-ImageNet四个基准数据集上的实验表明，CO-PFL在个性化准确性、鲁棒性、可扩展性和收敛稳定性方面均优于现有最优方法。

Conclusion: CO-PFL通过贡献导向的聚合策略和参数级个性化机制，有效缓解了聚合偏差，增强了全局协调能力，提升了本地性能，为个性化联邦学习提供了有效的解决方案。

Abstract: Personalized federated learning (PFL) addresses a critical challenge of
collaboratively training customized models for clients with heterogeneous and
scarce local data. Conventional federated learning, which relies on a single
consensus model, proves inadequate under such data heterogeneity. Its standard
aggregation method of weighting client updates heuristically or by data volume,
operates under an equal-contribution assumption, failing to account for the
actual utility and reliability of each client's update. This often results in
suboptimal personalization and aggregation bias. To overcome these limitations,
we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that
dynamically estimates each client's contribution for global aggregation. CO-PFL
performs a joint assessment by analyzing both gradient direction discrepancies
and prediction deviations, leveraging information from gradient and data
subspaces. This dual-subspace analysis provides a principled and discriminative
aggregation weight for each client, emphasizing high-quality updates.
Furthermore, to bolster personalization adaptability and optimization
stability, CO-PFL cohesively integrates a parameter-wise personalization
mechanism with mask-aware momentum optimization. Our approach effectively
mitigates aggregation bias, strengthens global coordination, and enhances local
performance by facilitating the construction of tailored submodels with stable
updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,
CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses
state-of-the-art methods in in personalization accuracy, robustness,
scalability and convergence stability.

</details>


### [39] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: 本文提出QKCV注意力机制，在传统QKV框架中引入静态类别嵌入C来增强类别特定信息，提升时序预测模型的准确性，并支持高效微调预训练模型。


<details>
  <summary>Details</summary>
Motivation: 在现实世界时序预测任务中，类别信息对于捕捉数据固有模式至关重要，但传统注意力机制未能充分利用此类信息。

Method: 提出QKCV注意力机制，在传统QKV框架基础上引入静态类别嵌入C，作为可插拔模块增强现有注意力模型。

Result: QKCV显著提升了多种注意力模型在多样化真实数据集上的预测精度，并能在微调单变量时序基础模型时仅更新静态嵌入C，降低计算开销。

Conclusion: QKCV是一种有效的注意力机制扩展，既能提升预测性能，又能实现高效模型微调，具有良好的实用价值。

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [40] [Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](https://arxiv.org/abs/2510.20468)
*Tomáš Souček,Sylvestre-Alvise Rebuffi,Pierre Fernandez,Nikola Jovanović,Hady Elsahar,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.LG

TL;DR: 本文研究图像水印伪造问题，提出一种基于偏好模型的水印检测方法，能够仅用单张水印图像实现水印移除和伪造，无需了解水印模型细节，挑战当前水印技术的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型普及和法律压力增加，数字内容水印技术日益重要。虽然已有许多研究评估水印抗移除攻击的鲁棒性，但水印伪造（即从真实内容窃取水印并应用于恶意内容）的研究仍不足。

Method: 1. 引入偏好模型来评估图像是否包含水印，该模型使用排序损失在纯程序生成的图像上训练，无需真实水印数据；2. 通过反向传播优化输入图像，展示模型移除和伪造水印的能力；3. 该方法仅需单张水印图像，且无需了解水印模型细节。

Result: 在多种后处理图像水印模型上的评估表明，该方法能有效伪造水印，质疑当前水印方法的安全性。

Conclusion: 提出的水印伪造方法简单实用，仅需单张水印图像且无需水印模型知识，对现有水印技术的安全性构成挑战。

Abstract: Recent years have seen a surge in interest in digital content watermarking
techniques, driven by the proliferation of generative models and increased
legal pressure. With an ever-growing percentage of AI-generated content
available online, watermarking plays an increasingly important role in ensuring
content authenticity and attribution at scale. There have been many works
assessing the robustness of watermarking to removal attacks, yet, watermark
forging, the scenario when a watermark is stolen from genuine content and
applied to malicious content, remains underexplored. In this work, we
investigate watermark forging in the context of widely used post-hoc image
watermarking. Our contributions are as follows. First, we introduce a
preference model to assess whether an image is watermarked. The model is
trained using a ranking loss on purely procedurally generated images without
any need for real watermarks. Second, we demonstrate the model's capability to
remove and forge watermarks by optimizing the input image through
backpropagation. This technique requires only a single watermarked image and
works without knowledge of the watermarking model, making our attack much
simpler and more practical than attacks introduced in related work. Third, we
evaluate our proposed method on a variety of post-hoc image watermarking
models, demonstrating that our approach can effectively forge watermarks,
questioning the security of current watermarking approaches. Our code and
further resources are publicly available.

</details>


### [41] [Sparse Local Implicit Image Function for sub-km Weather Downscaling](https://arxiv.org/abs/2510.20228)
*Yago del Valle Inclan Redondo,Enrique Arriaga-Varela,Dmitry Lyamzin,Pablo Cervantes,Tiago Ramalho*

Main category: cs.LG

TL;DR: SpLIIF模型通过隐式神经表示生成天气变量的任意尺度下采样，在日本稀疏气象站和地形数据上训练，在温度和风速预测方面比CorrDiff和插值基线表现更好。


<details>
  <summary>Details</summary>
Motivation: 开发能够从稀疏气象站和地形数据生成高分辨率天气变量的方法，实现任意尺度的下采样。

Method: 使用隐式神经表示（SpLIIF）从稀疏天气站和地形数据训练模型，并与插值基线和CorrDiff方法进行比较。

Result: 在温度下采样方面比CorrDiff和基线好50%，在风速下采样方面好10-20%。

Conclusion: SpLIIF在天气变量下采样方面优于现有方法，特别是在温度预测方面表现突出。

Abstract: We introduce SpLIIF to generate implicit neural representations and enable
arbitrary downscaling of weather variables. We train a model from sparse
weather stations and topography over Japan and evaluate in- and
out-of-distribution accuracy predicting temperature and wind, comparing it to
both an interpolation baseline and CorrDiff. We find the model to be up to 50%
better than both CorrDiff and the baseline at downscaling temperature, and
around 10-20% better for wind.

</details>


### [42] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: 本文提出了一个可证明收敛且实用的多目标强化学习框架，采用最大最小准则。从博弈论角度将问题重新表述为两人零和正则化连续博弈，并基于镜像下降设计高效算法。


<details>
  <summary>Details</summary>
Motivation: 解决多目标强化学习中最大最小准则的收敛性和实用性问题，传统方法在策略更新和全局收敛性方面存在挑战。

Method: 将最大最小多目标强化学习重新表述为两人零和正则化连续博弈，使用镜像下降算法简化策略更新，并引入自适应正则化增强性能。

Result: 算法在表格设置中展示收敛行为，在深度强化学习实现中显著优于先前基线方法，在多个MORL环境中表现优异。

Conclusion: 提出的框架在理论和实践上都取得了成功，为多目标强化学习提供了可证明收敛且高效的解决方案。

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [43] [Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction](https://arxiv.org/abs/2510.20236)
*Teng Jiek See,Daokun Zhang,Mario Boley,David K. Chalmers*

Main category: cs.LG

TL;DR: LKM是一种新颖的自知识蒸馏方法，通过最小化GNN层间隐藏嵌入的平均绝对距离，有效提升分子性质预测精度，同时几乎不增加计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型在分子性质预测中精度仍有提升空间，但增加模型复杂度会显著提高计算成本和内存需求，需要一种既能提高精度又不增加显著计算负担的方法。

Method: 开发了层到层知识混合（LKM）方法，通过最小化GNN层间预存在隐藏嵌入的平均绝对距离，有效聚合多跳和多尺度信息，改善局部和全局分子特征表示。

Result: 在三种GNN架构（DimeNet++、MXMNet、PAMNet）和量子化学数据集（QM9、MD17、Chignolin）上评估，LKM将量子化学和生物物理性质预测的平均绝对误差分别降低了9.8%、45.3%和22.9%。

Conclusion: LKM方法能够在不显著增加训练和推理成本的情况下，显著提高GNN在化学性质预测中的准确性。

Abstract: Graph Neural Networks (GNNs) are the currently most effective methods for
predicting molecular properties but there remains a need for more accurate
models. GNN accuracy can be improved by increasing the model complexity but
this also increases the computational cost and memory requirement during
training and inference. In this study, we develop Layer-to-Layer Knowledge
Mixing (LKM), a novel self-knowledge distillation method that increases the
accuracy of state-of-the-art GNNs while adding negligible computational
complexity during training and inference. By minimizing the mean absolute
distance between pre-existing hidden embeddings of GNN layers, LKM efficiently
aggregates multi-hop and multi-scale information, enabling improved
representation of both local and global molecular features. We evaluated LKM
using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using
datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found
that the LKM method effectively reduces the mean absolute error of quantum
chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17
Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to
significantly improve the accuracy of GNNs for chemical property prediction
without any substantial increase in training and inference cost.

</details>


### [44] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 本文提出了选择性分类差距的概念，并首次将其分解为五个具体误差源：贝叶斯噪声、近似误差、排序误差、统计噪声和实施/偏移导致的松弛。研究发现单调后校准对缩小这一差距作用有限，真正需要的是能够有效重新排序预测的评分机制。


<details>
  <summary>Details</summary>
Motivation: 当前选择性分类器很少能达到完美排序oracle的性能，即按照正确性顺序接受样本。本文旨在形式化分析这一性能差距的来源，为构建更接近理想oracle行为的选择性分类器提供理论指导和实践建议。

Method: 通过有限样本分解方法，将选择性分类差距分解为五个误差源，并在合成数据（双月数据）和真实世界视觉与语言基准上进行验证，通过控制实验隔离每个误差成分。

Result: 验证结果显示：(i) 贝叶斯噪声和有限模型容量可导致显著差距；(ii) 只有更丰富的特征感知校准器能显著改善分数排序；(iii) 数据偏移引入了需要分布鲁棒训练的单独松弛项。

Conclusion: 该分解提供了量化的误差预算和可操作的设计指南，帮助实践者构建更接近理想oracle行为的选择性分类器，强调需要重新排序预测而非仅重新缩放分数的评分机制。

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [45] [FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2510.20250)
*Zhiqin Yang,Yonggang Zhang,Chenxin Li,Yiu-ming Cheung,Bo Han,Yixuan Yuan*

Main category: cs.LG

TL;DR: FedGPS是一个联邦学习框架，通过整合统计分布和梯度信息来解决数据异构性问题，在各种异构场景下表现出优越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在数据异构性场景下的鲁棒性不足，需要一种能够适应多样化异构场景的解决方案。

Method: FedGPS静态修改客户端学习目标以隐式建模全局数据分布，同时动态调整本地更新方向，整合统计分布和梯度信息。

Result: 广泛实验表明FedGPS在多样化异构场景下优于现有最先进方法，验证了其有效性和鲁棒性。

Conclusion: FedGPS通过整合统计和梯度信息有效缓解了联邦学习中的数据异构性问题，在各种场景下都表现出色。

Abstract: Federated Learning (FL) confronts a significant challenge known as data
heterogeneity, which impairs model performance and convergence. Existing
methods have made notable progress in addressing this issue. However, improving
performance in certain heterogeneity scenarios remains an overlooked question:
\textit{How robust are these methods to deploy under diverse heterogeneity
scenarios?} To answer this, we conduct comprehensive evaluations across varied
heterogeneity scenarios, showing that most existing methods exhibit limited
robustness. Meanwhile, insights from these experiments highlight that sharing
statistical information can mitigate heterogeneity by enabling clients to
update with a global perspective. Motivated by this, we propose \textbf{FedGPS}
(\textbf{Fed}erated \textbf{G}oal-\textbf{P}ath \textbf{S}ynergy), a novel
framework that seamlessly integrates statistical distribution and gradient
information from others. Specifically, FedGPS statically modifies each client's
learning objective to implicitly model the global data distribution using
surrogate information, while dynamically adjusting local update directions with
gradient information from other clients at each round. Extensive experiments
show that FedGPS outperforms state-of-the-art methods across diverse
heterogeneity scenarios, validating its effectiveness and robustness. The code
is available at: https://github.com/CUHK-AIM-Group/FedGPS.

</details>


### [46] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: OpTI-BFM是一种乐观决策标准，通过直接建模奖励函数的不确定性来指导行为基础模型在测试时通过与环境交互进行任务推断，减少对预计算奖励数据的需求。


<details>
  <summary>Details</summary>
Motivation: 传统行为基础模型需要计算大量推理数据集的奖励，假设可以访问奖励函数形式或需要大量标注工作。为了缓解这些限制，研究旨在通过测试时与环境交互来完成任务推断。

Method: 提出OpTI-BFM乐观决策标准，直接建模奖励函数的不确定性，引导BFMs进行数据收集以进行任务推断。该方法与线性bandits的上置信界算法有直接联系。

Result: 在已建立的零样本基准测试中评估OpTI-BFM，观察到它使基于后继特征的BFMs能够在少量episode中识别和优化未见过的奖励函数，且计算开销最小。

Conclusion: OpTI-BFM为行为基础模型提供了一种高效的任务推断方法，通过乐观决策标准减少了对预计算奖励数据的依赖，在零样本强化学习场景中表现出色。

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [47] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: 本文提出了优化的GPU内核用于欧拉特征曲线计算，相比之前的GPU实现获得16-2000倍加速，并引入了可微分的PyTorch层支持端到端学习。


<details>
  <summary>Details</summary>
Motivation: 拓扑特征能捕捉成像数据中的全局几何结构，但在深度学习中的实际应用需要计算效率和可微分性。

Method: 开发了针对Ampere GPU优化的CUDA内核，使用128B合并访问和分层共享内存累加；创建了可微分PyTorch层，通过可微分欧拉特征变换风格的sigmoid松弛学习单方向阈值。

Result: 在合成网格上实现了16-2000倍的GPU加速，并成功集成了可微分拓扑特征到深度学习框架中。

Conclusion: 该工作为拓扑特征在深度学习中的实际应用提供了高效且可微分的解决方案，并讨论了批处理和多GPU扩展以扩大采用范围。

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [48] [Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2510.20272)
*Tristan Cinquin,Geoff Pleiss,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 本文研究了PRM引导的树搜索在数学推理中的效果，发现尽管成本更高，但与传统的Best-of-N方法相比没有显著改进，主要原因是PRM在深度推理中可靠性下降且泛化能力差。


<details>
  <summary>Details</summary>
Motivation: 传统的链式思维提示与Best-of-N选择方法无法捕捉复杂问题解决的分支和探索性质，因此研究PRM引导的树搜索是否能通过探索多个部分解路径来改进数学推理。

Method: 提出自适应算法来最大化过程奖励模型(PRM)分数，使用Qwen2.5-Math-7B-Instruct及其关联的PRM作为案例研究，在23个多样化数学问题上比较PRM引导的树搜索与Best-of-N方法。

Result: PRM引导的树搜索相比Best-of-N没有统计显著改进；蒙特卡洛树搜索和束搜索优于其他PRM引导方法；PRM对状态值的近似效果差且可靠性随推理深度下降；PRM在分布外泛化能力差。

Conclusion: 树搜索对不可靠PRM分数的更大依赖导致性能不佳，表明在树搜索能有效增强LLM数学推理之前需要不同的奖励建模方法。

Abstract: While chain-of-thought prompting with Best-of-N (BoN) selection has become
popular for mathematical reasoning in large language models (LLMs), its linear
structure fails to capture the branching and exploratory nature of complex
problem-solving. In this work, we propose an adaptive algorithm to maximize
process reward model (PRM) scores over the intractable action space, and
investigate whether PRM-guided tree search can improve mathematical reasoning
by exploring multiple partial solution paths. Across $23$ diverse mathematical
problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case
study, we find that: (1) PRM-guided tree search shows no statistically
significant improvements over BoN despite higher costs, (2) Monte Carlo tree
search and beam search outperform other PRM-guided tree search methods, (3)
PRMs poorly approximate state values and their reliability degrades with
reasoning depth, and (4) PRMs generalize poorly out of distribution. This
underperformance stems from tree search's greater reliance on unreliable PRM
scores, suggesting different reward modeling is necessary before tree search
can effectively enhance mathematical reasoning in LLMs.

</details>


### [49] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出了SynTSBench评估框架，通过可编程特征配置系统评估时间序列预测模型的基本建模能力，包括时间特征分解、鲁棒性分析和理论最优基准测试。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在标准基准数据集上表现良好，但在实际应用中性能不稳定，这源于模型的黑盒性质和现有评估框架的局限性，缺乏对模型具体优缺点的清晰量化分析。

Method: 采用合成数据驱动的评估范式，通过三个核心分析维度：时间特征分解与能力映射、数据异常下的鲁棒性分析、理论最优基准测试，建立可解释的评估系统。

Result: 实验表明当前深度学习模型并非在所有类型的时间特征上都接近最优基准，揭示了模型在不同模式类型上的性能差异。

Conclusion: SynTSBench框架能够系统评估时间序列预测模型的基本能力，为模型选择提供更清晰的指导，并揭示了现有模型在特定模式类型上的局限性。

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [50] [ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows](https://arxiv.org/abs/2510.20279)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Ziheng Qin,Bangyuan Zhu,Shengbin Huang,Xuanlei Zhao,Panpan Zhang,Xiaojiang Peng,Yuzhang Shang,Jianfei Yang,Zheng Zhu,Tianlong Chen,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 本文提出了ResearchGPT愿景和CS-54k语料库，包含CS-4k评估基准和CS-50k训练数据集，用于评估和训练AI作为科学研究助手的能力。实验表明，使用高质量领域数据训练的7B模型可以超越更大的专有系统。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，需要构建能够协助整个科学研究过程的AI协作系统，这需要端到端的工作流程评估基准，而不仅仅是孤立的子任务。

Method: 构建了CS-54k高质量计算机科学问答对语料库，采用可扩展的基于论文的流水线，结合检索增强生成和多阶段质量控制来确保事实基础。从中衍生出CS-4k评估基准和CS-50k训练数据集。

Result: CS-4k能够将最先进的LLM分层为不同的能力层级。在CS-50k上使用监督训练和强化学习训练的开放模型显示出显著改进。即使是7B规模的模型，经过适当训练也能超越许多更大的专有系统。

Conclusion: 使AI模型成为更好的研究助手更多地依赖于使用高质量数据进行领域对齐训练，而不是预训练规模或通用基准性能。释放CS-4k和CS-50k旨在促进AI系统作为CS研究中可靠协作者的发展。

Abstract: As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.

</details>


### [51] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: 提出InvDec混合架构，结合基于patch的时间编码器和在变量维度上操作的倒置解码器，实现时间建模和变量依赖的分离，在多元时间序列预测中显著提升高维数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多元时间序列预测中存在局限性：通道独立方法如PatchTST擅长时间建模但忽略变量相关性，而纯变量注意力方法如iTransformer牺牲时间编码。需要一种能同时有效建模时间模式和跨变量依赖的架构。

Method: InvDec采用混合架构：1) 基于patch的时间编码器进行时间建模；2) 倒置解码器通过变量级自注意力在变量维度操作；3) 延迟变量嵌入在时间编码后丰富变量特定表示；4) 自适应残差融合机制动态平衡时间和变量信息。

Result: 在7个基准数据集上的实验显示显著提升：电力数据集(321变量)MSE降低20.9%，天气数据集提升4.3%，交通数据集提升2.7%，同时在低维ETT数据集上保持竞争力。消融研究验证了各组件有效性。

Conclusion: InvDec成功分离了时间编码和变量级解码，在高维多元时间序列预测中表现优异。分析表明其优势随数据集维度增加而增长，证实跨变量建模在变量数量增加时变得至关重要。

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [52] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: 提出了LEGO框架，用于推荐系统中的多属性遗忘，解决现有单属性遗忘方法无法同时处理多个遗忘请求和适应动态遗忘需求的问题。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统中保护敏感用户信息需求的增长，现实世界的隐私保护通常涉及多个敏感属性且是动态的，现有单属性遗忘方法无法满足这些需求。

Method: 将多属性遗忘过程分为两个步骤：嵌入校准（从用户嵌入中移除特定属性信息）和灵活组合（将这些嵌入组合成单个嵌入以保护所有敏感属性），并将遗忘过程建模为互信息最小化问题。

Result: 在三个真实世界数据集和三种代表性推荐模型上的广泛实验证明了所提出框架的有效性和效率。

Conclusion: LEGO框架通过两步骤方法解决了多属性遗忘的挑战，提供了同时遗忘的理论保证，并具有灵活性和高效性。

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [53] [Synthetic Data for Robust Runway Detection](https://arxiv.org/abs/2510.20349)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin*

Main category: cs.LG

TL;DR: 该论文提出了一种基于商用飞行模拟器的图像生成方法，用于解决跑道检测任务中真实数据不足的问题，通过结合少量真实标注图像和合成图像，并采用定制化的域适应策略，提高了目标检测模型在恶劣条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶导航等关键应用中，深度视觉模型需要覆盖所有可能条件，包括罕见场景，但真实数据的收集和标注成本过高。特别是对于飞机跑道检测等关键任务，需要处理夜间等恶劣条件，而真实数据往往缺乏这些场景。

Method: 使用商用飞行模拟器生成合成图像，结合少量真实标注图像，通过控制图像生成过程和真实/合成数据的集成，采用定制化的域适应策略来缓解合成到真实的分布偏移问题。

Result: 标准目标检测模型能够实现准确预测，在未在真实数据中出现的夜间图像等恶劣条件下也表现出良好的鲁棒性。

Conclusion: 基于飞行模拟器的图像生成方法是一种有效的解决方案，能够以较低成本覆盖所有条件和环境，通过适当的域适应策略可以显著提高模型在关键应用中的性能。

Abstract: Deep vision models are now mature enough to be integrated in industrial and
possibly critical applications such as autonomous navigation. Yet, data
collection and labeling to train such models requires too much efforts and
costs for a single company or product. This drawback is more significant in
critical applications, where training data must include all possible conditions
including rare scenarios. In this perspective, generating synthetic images is
an appealing solution, since it allows a cheap yet reliable covering of all the
conditions and environments, if the impact of the synthetic-to-real
distribution shift is mitigated. In this article, we consider the case of
runway detection that is a critical part in autonomous landing systems
developed by aircraft manufacturers. We propose an image generation approach
based on a commercial flight simulator that complements a few annotated real
images. By controlling the image generation and the integration of real and
synthetic data, we show that standard object detection models can achieve
accurate prediction. We also evaluate their robustness with respect to adverse
conditions, in our case nighttime images, that were not represented in the real
data, and show the interest of using a customized domain adaptation strategy.

</details>


### [54] [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)
*Zhenghao Xu,Qin Lu,Qingru Zhang,Liang Qiu,Ilgee Hong,Changlong Yu,Wenlin Yao,Yao Liu,Haoming Jiang,Lihong Li,Hyokun Yun,Tuo Zhao*

Main category: cs.LG

TL;DR: 提出基于不确定性的路由框架，通过将优势估计建模为成对偏好分类，实现高效结合快速奖励模型和强大但昂贵的LLM法官，以提升在线RLHF效果。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型容易受到奖励攻击且泛化能力差，而强大的LLM法官虽然泛化能力强但推理成本高，限制了在在线RLHF中的应用。

Method: 基于不确定性的路由框架，将策略梯度方法中的优势估计建模为成对偏好分类，通过不确定性量化指导路由决策，不确定的样本对由LLM法官处理，确定的样本对由奖励模型处理。

Result: 在奖励模型基准测试中，该方法在相同成本下显著优于随机法官调用，下游对齐结果展示了其在改进在线RLHF方面的有效性。

Conclusion: 提出的不确定性路由框架能够高效结合快速奖励模型和强大LLM法官，显著提升在线RLHF的性能和效率。

Abstract: Reward model (RM) plays a pivotal role in reinforcement learning with human
feedback (RLHF) for aligning large language models (LLMs). However, classical
RMs trained on human preferences are vulnerable to reward hacking and
generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM
judges equipped with reasoning capabilities demonstrate superior
generalization, even without additional training, but incur significantly
higher inference costs, limiting their applicability in online RLHF. In this
work, we propose an uncertainty-based routing framework that efficiently
complements a fast RM with a strong but costly LLM judge. Our approach
formulates advantage estimation in policy gradient (PG) methods as pairwise
preference classification, enabling principled uncertainty quantification to
guide routing. Uncertain pairs are forwarded to the LLM judge, while confident
ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our
uncertainty-based routing strategy significantly outperforms random judge
calling at the same cost, and downstream alignment results showcase its
effectiveness in improving online RLHF.

</details>


### [55] [Hierarchical Time Series Forecasting with Robust Reconciliation](https://arxiv.org/abs/2510.20383)
*Shuhei Aikawa,Aru Suzuki,Kei Yoshitake,Kanata Teshigawara,Akira Iwabuchi,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: 提出了一种用于层次时间序列预测的鲁棒优化框架，通过考虑协方差矩阵估计的不确定性来提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有层次预测方法需要估计协方差矩阵，但真实协方差矩阵未知，估计误差会降低预测性能。

Method: 引入协方差矩阵的不确定性集合，构建最小化最坏情况期望平方误差的鲁棒优化问题，可转化为半定优化问题求解。

Result: 数值实验表明，所提出的鲁棒协调方法比现有层次预测方法具有更好的预测性能。

Conclusion: 在协调过程中整合不确定性是有效的，能够提升层次时间序列预测的鲁棒性和准确性。

Abstract: This paper focuses on forecasting hierarchical time-series data, where each
higher-level observation equals the sum of its corresponding lower-level time
series. In such contexts, the forecast values should be coherent, meaning that
the forecast value of each parent series exactly matches the sum of the
forecast values of its child series. Existing hierarchical forecasting methods
typically generate base forecasts independently for each series and then apply
a reconciliation procedure to adjust them so that the resulting forecast values
are coherent across the hierarchy. These methods generally derive an optimal
reconciliation, using a covariance matrix of the forecast error. In practice,
however, the true covariance matrix is unknown and has to be estimated from
finite samples in advance. This gap between the true and estimated covariance
matrix may degrade forecast performance. To address this issue, we propose a
robust optimization framework for hierarchical reconciliation that accounts for
uncertainty in the estimated covariance matrix. We first introduce an
uncertainty set for the estimated covariance matrix and formulate a
reconciliation problem that minimizes the worst-case expected squared error
over this uncertainty set. We show that our problem can be cast as a
semidefinite optimization problem. Numerical experiments demonstrate that the
proposed robust reconciliation method achieved better forecast performance than
existing hierarchical forecasting methods, which indicates the effectiveness of
integrating uncertainty into the reconciliation process.

</details>


### [56] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 本文提出了基于相对排序的缩放定律，通过相对概率指标来补充传统交叉熵指标的不足，从相对排序角度研究语言模型的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究主要依赖交叉熵作为评估指标，但交叉熵只关注正确token的绝对概率，忽略了正确token与错误token之间的相对排序关系，而相对排序在语言模型的实际应用中至关重要。

Method: 提出了相对概率指标，量化正确token在top预测中的排名概率，并基于此建立了相对缩放定律，通过四个数据集和四个模型家族的大规模实验验证了该定律的鲁棒性和准确性。

Result: 实验证明相对缩放定律能够准确预测模型性能随规模的变化，并成功应用于解释涌现现象和寻找缩放定律的基本理论。

Conclusion: 相对缩放定律补充了交叉熵视角，为大规模语言模型的缩放提供了更完整的理解，对实际开发和理论探索都具有重要价值。

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [57] [Why DPO is a Misspecified Estimator and How to Fix It](https://arxiv.org/abs/2510.20413)
*Aditya Gopalan,Sayak Ray Chowdhury,Debangshu Banerjee*

Main category: cs.LG

TL;DR: 本文分析了DPO算法的局限性，提出了AuxDPO方法来解决DPO在奖励函数不可实现时的错误设定问题，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 直接对齐算法如DPO仅使用监督学习进行微调，但存在错误设定问题。当真实奖励函数无法通过策略类实现时，DPO会出现偏好顺序反转、策略奖励恶化等失败模式。

Method: 研究了两阶段RLHF在参数类中的局部行为，提出了AuxDPO方法，在DPO损失函数中引入辅助变量，以原则性方式向RLHF解移动，缓解DPO的错误设定问题。

Result: 在示范性赌博机设置和LLM对齐任务中，AuxDPO表现出比DPO更优越的性能。

Conclusion: AuxDPO通过引入辅助变量有效缓解了DPO的错误设定问题，在理论和实验上都证明了其有效性。

Abstract: Direct alignment algorithms such as Direct Preference Optimization (DPO)
fine-tune models based on preference data, using only supervised learning
instead of two-stage reinforcement learning with human feedback (RLHF). We show
that DPO encodes a statistical estimation problem over reward functions induced
by a parametric policy class. When the true reward function that generates
preferences cannot be realized via the policy class, DPO becomes misspecified,
resulting in failure modes such as preference order reversal, worsening of
policy reward, and high sensitivity to the input preference data distribution.
On the other hand, we study the local behavior of two-stage RLHF for a
parametric class and relate it to a natural gradient step in policy space. Our
fine-grained geometric characterization allows us to propose AuxDPO, which
introduces additional auxiliary variables in the DPO loss function to help move
towards the RLHF solution in a principled manner and mitigate the
misspecification in DPO. We empirically demonstrate the superior performance of
AuxDPO on didactic bandit settings as well as LLM alignment tasks.

</details>


### [58] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种针对标记时间点过程(MTPP)中事件标记分布不平衡问题的解决方案，通过阈值方法优化标记预测，并开发了新的神经MTPP模型来支持有效的时间采样和标记概率估计。


<details>
  <summary>Details</summary>
Motivation: 现有MTPP研究忽视了现实应用中事件标记分布高度不平衡的问题，其中一些标记频繁出现而其他标记罕见。这种不平衡对下一个事件的预测性能，特别是罕见标记事件的预测，构成了重大挑战。

Method: 提出阈值方法，学习阈值来调整由标记先验概率归一化的标记概率，以优化标记预测；开发新的神经MTPP模型，支持有效的时间采样和标记概率估计，无需计算昂贵的数值不当积分；采用先预测标记再预测时间的策略。

Result: 在真实世界数据集上的广泛实验表明，该解决方案在下一个事件标记和时间预测方面优于各种基线方法。

Conclusion: 所提出的方法有效解决了MTPP中事件标记分布不平衡的问题，提高了对罕见标记事件的预测性能，同时保持了计算效率。

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [59] [An Empirical Study of Sample Selection Strategies for Large Language Model Repair](https://arxiv.org/abs/2510.20428)
*Xuran Li,Jingyi Wang*

Main category: cs.LG

TL;DR: 本文系统分析了LLM修复中的样本优先级策略，比较了五种选择方法，发现提出的SAPS方法在解毒、效用保持和效率方面达到最佳平衡，随机采样对大型模型仍然有效。


<details>
  <summary>Details</summary>
Motivation: LLM在现实系统中部署时可能产生有毒或有偏见的输出，影响安全性和信任度。参数更新的高成本促使需要选择性使用修复数据，但现有数据选择方法在大型生成模型行为修复中的效果和效率尚不明确。

Method: 评估了五种代表性选择方法：随机采样、K-Center、基于梯度范数的选择(GraNd)、分层覆盖(CCS)以及提出的语义感知优先级采样(SAPS)。通过毒性减少、WikiText-2和LAMBADA上的困惑度以及三个复合指标来评估修复效果和权衡。

Result: 实验结果显示SAPS在解毒、效用保持和效率方面达到最佳平衡，用显著更少的数据实现了相当或更优的修复结果。随机采样对大型或鲁棒模型仍然有效，而高开销方法如CCS和GraNd提供的益处有限。最优数据比例取决于模型规模和修复方法。

Conclusion: 基于选择的修复是维护LLM可靠性的高效可扩展范式，样本选择应被视为修复管道中的可调组件。

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems,
yet they can produce toxic or biased outputs that undermine safety and trust.
Post-hoc model repair provides a practical remedy, but the high cost of
parameter updates motivates selective use of repair data. Despite extensive
prior work on data selection for model training, it remains unclear which
sampling criteria are most effective and efficient when applied specifically to
behavioral repair of large generative models. Our study presents a systematic
analysis of sample prioritization strategies for LLM repair. We evaluate five
representative selection methods, including random sampling, K-Center,
gradient-norm-based selection(GraNd), stratified coverage (CCS), and a
Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair
effectiveness and trade-offs are assessed through toxicity reduction,
perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair
Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair
Efficiency Score (RES). Experimental results show that SAPS achieves the best
balance between detoxification, utility preservation, and efficiency,
delivering comparable or superior repair outcomes with substantially less data.
Random sampling remains effective for large or robust models, while
high-overhead methods such as CCS and GraNd provide limited benefit. The
optimal data proportion depends on model scale and repair method, indicating
that sample selection should be regarded as a tunable component of repair
pipelines. Overall, these findings establish selection-based repair as an
efficient and scalable paradigm for maintaining LLM reliability.

</details>


### [60] [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)
*Quannian Zhang,Michael Röder,Nikit Srivastava,N'Dah Jean Kouagou,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 本文提出可解释基准测试的新范式，旨在自动生成系统在基准测试中性能的解释，并以知识图谱问答系统为例进行了实例化。


<details>
  <summary>Details</summary>
Motivation: 当前系统性能评估通常仅通过少量指标进行总结，分析评估细节和推导进一步开发或使用的见解仍然是一个繁琐的手动任务，且结果往往存在偏差。

Method: 使用为大型知识图谱开发的新概念学习方法PruneCEL来计算解释，该方法在可解释基准测试任务上优于现有最先进的概念学习方法。

Result: PruneCEL在可解释基准测试任务上的F1分数比现有最先进方法高出0.55分；用户研究表明，在80%的情况下，大多数参与者能够基于解释准确预测系统行为。

Conclusion: 可解释基准测试是一个有前景的研究方向，能够自动生成系统性能的解释，为系统开发和选择提供有价值的见解。

Abstract: Evaluating competing systems in a comparable way, i.e., benchmarking them, is
an undeniable pillar of the scientific method. However, system performance is
often summarized via a small number of metrics. The analysis of the evaluation
details and the derivation of insights for further development or use remains a
tedious manual task with often biased results. Thus, this paper argues for a
new type of benchmarking, which is dubbed explainable benchmarking. The aim of
explainable benchmarking approaches is to automatically generate explanations
for the performance of systems in a benchmark. We provide a first instantiation
of this paradigm for knowledge-graph-based question answering systems. We
compute explanations by using a novel concept learning approach developed for
large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL
outperforms state-of-the-art concept learners on the task of explainable
benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41
participants shows that in 80\% of the cases, the majority of participants can
accurately predict the behavior of a system based on our explanations. Our code
and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025

</details>


### [61] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: 提出了一种名为Bi-CoG的半监督微调方法，通过同时利用模型间和模型内一致性以及错误感知动态伪标签分配策略，解决现有方法存在的模型偏差和超参数敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督微调方法往往存在模型偏差和超参数敏感性问题，主要依赖于预测一致性或预定义的置信度阈值。

Method: Bi-CoG方法通过同时利用模型间一致性和模型内一致性，结合错误感知动态伪标签分配策略，为未标记数据分配高质量、低偏差的伪标签。

Result: 在14个数据集上的广泛实验表明，Bi-CoG方法能够持续且显著提升现有方法的性能。

Conclusion: Bi-CoG是一种简单有效的即插即用方法，能够有效解决半监督微调中的模型偏差和超参数敏感性问题。

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [62] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: 提出Hurdle-IMDL框架解决遥感降雨反演中的标签不平衡问题，通过分解为零膨胀和长尾分布两部分，分别用hurdle模型和IMDL方法处理，显著改善了强降雨反演性能。


<details>
  <summary>Details</summary>
Motivation: 人工智能在定量遥感中受限于标签分布不平衡问题，传统训练模型偏向常见样本，导致稀有样本（如强降雨）的反演性能下降。

Method: 采用分而治之策略：1）用hurdle模型处理零膨胀（非降雨样本占主导）；2）提出IMDL方法处理长尾分布（轻降雨样本相对于强降雨样本不成比例），将学习目标转化为无偏的理想逆模型。

Result: 通过统计指标和案例研究验证，Hurdle-IMDL优于传统方法、成本敏感方法、生成方法和多任务学习方法，有效缓解系统性低估，显著改善强到极端降雨的反演。

Conclusion: IMDL为处理环境变量分布不平衡提供了通用方法，能够增强对稀有但高影响事件的检索能力。

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [63] [SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment](https://arxiv.org/abs/2510.20540)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: SheafAlign是一个基于层理论的去中心化多模态对齐框架，通过多个比较空间替代单一空间对齐，不需要所有模态间相互冗余，在零样本泛化、跨模态对齐和模态缺失鲁棒性方面表现优异，通信成本降低50%。


<details>
  <summary>Details</summary>
Motivation: 传统多模态对齐方法假设所有模态间存在相互冗余，这在现实分布式场景中不成立。需要一种不要求所有模态相互冗余的新方法。

Method: 采用层理论框架，通过层结构建模模态间的成对关系，利用去中心化对比学习目标进行训练，使用多个比较空间替代单一空间对齐。

Result: 在多模态传感数据集上的实验显示，该方法在零样本泛化、跨模态对齐和模态缺失鲁棒性方面优于现有方法，通信成本比最先进基线降低50%。

Conclusion: SheafAlign通过层理论方法成功解决了传统多模态对齐的局限性，能够同时保留共享和独特信息，在分布式场景中具有显著优势。

Abstract: Conventional multimodal alignment methods assume mutual redundancy across all
modalities, an assumption that fails in real-world distributed scenarios. We
propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal
alignment that replaces single-space alignment with multiple comparison spaces.
This approach models pairwise modality relations through sheaf structures and
leverages decentralized contrastive learning-based objectives for training.
SheafAlign overcomes the limitations of prior methods by not requiring mutual
redundancy among all modalities, preserving both shared and unique information.
Experiments on multimodal sensing datasets show superior zero-shot
generalization, cross-modal alignment, and robustness to missing modalities,
with 50\% lower communication cost than state-of-the-art baselines.

</details>


### [64] [A Unified Framework for Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.20542)
*Jacopo Di Ventura,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文提出了零样本强化学习的首个统一框架，通过引入一致的符号和分类法来组织现有方法，将算法分为直接表示和组合表示两类，为未来研究提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 零样本强化学习领域缺乏共同的分析框架，现有方法难以直接比较，需要统一的理论基础来推动更通用智能体的发展。

Method: 提出统一框架，引入一致符号系统和分类法，将算法分为直接表示（端到端映射）和组合表示（利用价值函数子结构分解）两类，并推导了后继特征方法的扩展边界。

Result: 建立了零样本强化学习的统一分析框架，能够直接比较不同方法，揭示了共享原则和关键差异，为后继特征方法提供了新的性能视角。

Conclusion: 该框架为零样本强化学习提供了原则性基础，为开发更通用智能体指明了清晰路径，有助于推动该领域的未来发展。

Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.

</details>


### [65] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: 该论文分析了将MLOps实践集成到操作技术(OT)系统中的挑战，提出了使用现有参考模型（RAMI 4.0和ISA-95）进行系统化集成的框架，并通过实际案例验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着MLOps在工业环境中的广泛应用，其与OT系统的集成面临显著挑战，需要找到有效的集成方法。

Method: 分析MLOps与OT集成的根本障碍，提出将MLOps实践嵌入到已建立的OT参考模型中的系统化方法，评估RAMI 4.0和ISA-95标准对MLOps集成的适用性，并通过真实案例展示MLOps生命周期组件到RAMI 4.0的详细映射。

Result: 研究发现标准MLOps实践不能直接移植到OT环境，但通过使用现有参考模型进行结构化适配，可以为成功集成提供途径。

Conclusion: 通过结构化地使用现有OT参考模型，可以实现MLOps与OT系统的有效集成，尽管需要定制化适配而非直接移植标准实践。

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [66] [Generalizable Reasoning through Compositional Energy Minimization](https://arxiv.org/abs/2510.20607)
*Alexandru Oarga,Yilun Du*

Main category: cs.LG

TL;DR: 提出一种通过组合子问题能量函数来构建全局能量景观的新方法，以解决机器学习推理任务中的泛化挑战，并在复杂问题上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端推理模型在训练分布之外泛化能力有限，需要一种能够处理比训练数据更复杂问题的新方法。

Method: 学习子问题的能量景观，在测试时通过组合多个子问题的能量函数构建全局能量景观，并引入并行能量最小化来提升采样质量。

Result: 在广泛的推理问题上评估，该方法超越了现有最先进方法，能够泛化到更大更复杂的问题。

Conclusion: 通过组合子问题能量景观的方法有效提升了推理模型的泛化能力，为解决复杂推理问题提供了新途径。

Abstract: Generalization is a key challenge in machine learning, specifically in
reasoning tasks, where models are expected to solve problems more complex than
those encountered during training. Existing approaches typically train
reasoning models in an end-to-end fashion, directly mapping input instances to
solutions. While this allows models to learn useful heuristics from data, it
often results in limited generalization beyond the training distribution. In
this work, we propose a novel approach to reasoning generalization by learning
energy landscapes over the solution spaces of smaller, more tractable
subproblems. At test time, we construct a global energy landscape for a given
problem by combining the energy functions of multiple subproblems. This
compositional approach enables the incorporation of additional constraints
during inference, allowing the construction of energy landscapes for problems
of increasing difficulty. To improve the sample quality from this newly
constructed energy landscape, we introduce Parallel Energy Minimization (PEM).
We evaluate our approach on a wide set of reasoning problems. Our method
outperforms existing state-of-the-art methods, demonstrating its ability to
generalize to larger and more complex problems. Project website can be found
at: https://alexoarga.github.io/compositional_reasoning/

</details>


### [67] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文对随机梯度下降（SGD）在期望光滑性（ES）条件下的收敛性进行了系统分析，统一并扩展了近期相关研究，提供了详细的收敛证明和显式残差误差。


<details>
  <summary>Details</summary>
Motivation: 传统SGD分析依赖于过强（有界方差）或过粗糙（均匀噪声）的假设，而期望光滑性条件提供了更灵活的替代方案，将随机梯度的二阶矩与目标函数值和全梯度联系起来。

Method: 通过细化ES条件并引入采样相关常数，推导了全梯度平方范数期望的界，并针对不同步长调度证明了O(1/K)收敛速率。

Result: 获得了显式残差误差的收敛界，所有证明在附录中详细给出，统一并扩展了Khaled和Richtárik（2020）以及Umeda和Iiduka（2025）的工作。

Conclusion: 在期望光滑性条件下，SGD能够实现O(1/K)的收敛速率，该分析框架为大规模学习提供了更精确的理论基础。

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [68] [Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets](https://arxiv.org/abs/2510.20609)
*Timur Galimzyanov,Olga Kolomyttseva,Egor Bogomolov*

Main category: cs.LG

TL;DR: 该论文系统研究了代码生成任务中的检索设计，比较了不同检索配置在代码补全和bug定位任务中的表现，发现BM25+词级分割在PL-PL任务中最有效，而专有密集编码器在NL-PL任务中表现更好但延迟高。


<details>
  <summary>Details</summary>
Motivation: 研究在现实计算预算下，针对代码生成任务的检索设计优化，为构建高效的代码导向RAG系统提供实证依据。

Method: 使用Long Code Arena的两个互补任务（代码补全和bug定位），系统比较不同上下文窗口大小下的检索配置，包括分块策略、相似性评分和分割粒度三个维度。

Result: PL-PL任务中稀疏BM25+词级分割最有效且快一个数量级；NL-PL任务中专有密集编码器表现更好但延迟高100倍；最佳分块大小随可用上下文扩展；基于行的分块与语法感知分割效果相当；检索延迟差异可达200倍。

Conclusion: 提供了基于任务需求、模型约束和计算效率的代码导向RAG系统实施建议，强调BM25+词级分割在质量-延迟权衡中的优势。

Abstract: We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.

</details>


### [69] [MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation](https://arxiv.org/abs/2510.20615)
*Yang Han,Pengyu Wang,Kai Yu,Xin Chen,Lu Chen*

Main category: cs.LG

TL;DR: MS-BART是一个统一建模框架，通过大规模预训练将质谱和分子结构映射到共享词汇表中，实现跨模态学习，在质谱结构解析任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 质谱在分子识别中起关键作用，但由于注释谱图的稀缺，从质谱数据解析分子结构仍然具有挑战性。现有大规模预训练方法难以直接应用于质谱领域，因为原始谱信号复杂且异构。

Method: 提出MS-BART框架：1）将质谱和分子结构映射到共享词汇表；2）通过大规模预训练在可靠计算的指纹-分子数据集上进行跨模态学习；3）多任务预训练目标联合优化去噪和翻译任务；4）使用MIST模型生成指纹预测进行微调；5）引入化学反馈机制减少分子幻觉。

Result: 在MassSpecGym和NPLIB1基准测试中，MS-BART在5/12关键指标上达到SOTA性能，比基于扩散的竞争方法快一个数量级，并通过消融研究系统验证了模型的有效性和鲁棒性。

Conclusion: MS-BART通过统一的跨模态学习框架成功解决了质谱数据稀缺和信号复杂性的挑战，为质谱结构解析提供了高效可靠的解决方案。

Abstract: Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART's generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model's
effectiveness and robustness.

</details>


### [70] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: 该论文分析了差分隐私迁移学习中的两个关键超参数：裁剪边界C和批次大小B，揭示了理论选择与实证结果之间的不匹配，并指出现有启发式方法在有限计算预算下的失效。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私迁移学习实践中，裁剪边界C和批次大小B的选择存在理论指导与实证结果之间的矛盾，特别是在强隐私约束下，需要重新理解这些超参数的作用机制。

Method: 通过分析梯度分布变化，研究在有限计算预算下裁剪边界C和批次大小B对模型性能的影响，将裁剪视为梯度重新加权形式，并考察累积DP噪声的作用。

Result: 发现强隐私约束下较大的C表现更好，与理论预期相反；在固定训练轮次下，现有批次大小启发式方法失效，累积DP噪声能更好解释批次大小选择；跨任务使用单一(C,B)设置会导致性能下降。

Conclusion: 需要重新审视差分隐私迁移学习中的超参数选择策略，考虑梯度分布变化和累积DP噪声的影响，避免跨任务使用单一设置，特别是在隐私约束严格和计算资源有限的情况下。

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [71] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: H-SPLID算法通过将显著和非显著特征显式分解到不同空间来学习显著特征表示，促进学习低维任务相关特征，并建立鲁棒性与潜在表示压缩之间的联系。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特征表示学习中未能明确区分显著和非显著特征，导致模型可能依赖非任务相关的特征，影响鲁棒性和泛化能力。

Method: 提出H-SPLID算法，将输入特征显式分解为显著和非显著两个子空间，利用Hilbert-Schmidt独立性准则(HSIC)来约束表示与输入之间的关系。

Result: 理论证明输入扰动下的预期预测偏差受显著子空间维度和HSIC上界约束；实验表明H-SPLID训练的模型主要依赖显著输入成分，对背景等非显著特征的扰动敏感性降低。

Conclusion: H-SPLID通过显式特征分解有效促进任务相关特征学习，建立了表示压缩与模型鲁棒性之间的理论联系，在图像分类任务中表现出更好的鲁棒性。

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [72] [Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges](https://arxiv.org/abs/2510.20637)
*Hyun Jong Yang,Hyunsoo Kim,Hyeonho Noh,Seungnyun Kim,Byonghyo Shim*

Main category: cs.LG

TL;DR: 本文概述了基于LLMs/LMMs的任务导向自主通信，通过三个案例研究展示了其在多模态感知集成、自适应重构和无线任务提示/微调策略方面的优势，显著优于传统方法和判别性深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多模态模型在自然语言理解和复杂推理方面取得了突破性进展，使其成为6G机器、车辆和人形机器人之间自主通信的关键推动者。

Method: 提出了基于LLMs/LMMs的任务导向自主通信框架，包括多模态感知集成、自适应重构以及针对无线任务的提示/微调策略，并通过三个案例研究进行验证：基于LMM的交通控制、基于LLM的机器人调度和基于LMM的环境感知信道估计。

Result: 实验结果表明，提出的LLM/LMM辅助自主系统显著优于传统和判别性深度学习模型，在动态目标、变化输入参数和异构多模态条件下保持鲁棒性，而传统静态优化方法在这些条件下性能会下降。

Conclusion: LLMs和LMMs在6G自主通信中具有巨大潜力，能够有效处理动态环境和多模态条件，为未来的智能通信系统提供了新的解决方案。

Abstract: Large language models (LLMs) and large multimodal models (LMMs) have achieved
unprecedented breakthrough, showcasing remarkable capabilities in natural
language understanding, generation, and complex reasoning. This transformative
potential has positioned them as key enablers for 6G autonomous communications
among machines, vehicles, and humanoids. In this article, we provide an
overview of task-oriented autonomous communications with LLMs/LMMs, focusing on
multimodal sensing integration, adaptive reconfiguration, and
prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework
through three case studies: LMM-based traffic control, LLM-based robot
scheduling, and LMM-based environment-aware channel estimation. From
experimental results, we show that the proposed LLM/LMM-aided autonomous
systems significantly outperform conventional and discriminative deep learning
(DL) model-based techniques, maintaining robustness under dynamic objectives,
varying input parameters, and heterogeneous multimodal conditions where
conventional static optimization degrades.

</details>


### [73] [Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems](https://arxiv.org/abs/2510.20640)
*Fiza Hussain,Anson Bastos,Anjaly Parayil,Ayush Choure,Chetan Bansal,Rujia Wang,Saravan Rajmohan*

Main category: cs.LG

TL;DR: DiRecGNN是一个用于微软云服务监控的注意力增强实体推荐框架，通过构建监控异构图，利用多头注意力机制和随机游走路径采样来捕获长距离依赖关系，在实体推荐任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 云服务监控中需要推荐最优的属性子集进行跟踪，但现有方法在处理有限结构信息和长距离依赖关系时表现不佳。

Method: 构建生产规模的监控异构图，提出基于transformer架构的注意力增强实体排序模型，使用多头注意力关注异构邻居和属性，并通过随机游走路径采样捕获长距离依赖，采用多面损失函数优化推荐。

Result: 经验评估显示模型在MRR指标上提升了43.1%，产品团队对该功能的评分为4.5/5，认为非常有用。

Conclusion: DiRecGNN框架在云服务监控实体推荐任务中表现出色，能够有效处理异构图中的复杂依赖关系，获得了实际部署的积极反馈。

Abstract: In this paper, we present DiRecGNN, an attention-enhanced entity
recommendation framework for monitoring cloud services at Microsoft. We provide
insights on the usefulness of this feature as perceived by the cloud service
owners and lessons learned from deployment. Specifically, we introduce the
problem of recommending the optimal subset of attributes (dimensions) that
should be tracked by an automated watchdog (monitor) for cloud services. To
begin, we construct the monitor heterogeneous graph at production-scale. The
interaction dynamics of these entities are often characterized by limited
structural and engagement information, resulting in inferior performance of
state-of-the-art approaches. Moreover, traditional methods fail to capture the
dependencies between entities spanning a long range due to their homophilic
nature. Therefore, we propose an attention-enhanced entity ranking model
inspired by transformer architectures. Our model utilizes a multi-head
attention mechanism to focus on heterogeneous neighbors and their attributes,
and further attends to paths sampled using random walks to capture long-range
dependencies. We also employ multi-faceted loss functions to optimize for
relevant recommendations while respecting the inherent sparsity of the data.
Empirical evaluations demonstrate significant improvements over existing
methods, with our model achieving a 43.1% increase in MRR. Furthermore, product
teams who consumed these features perceive the feature as useful and rated it
4.5 out of 5.

</details>


### [74] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: xTime是一个用于时间序列极端事件预测的新框架，通过知识蒸馏和专家混合机制提升对罕见极端事件的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列中的极端事件（如洪水、热浪、医疗紧急事件）具有重大实际影响，但现有预测模型因数据不平衡和忽略中间事件信息而难以准确预测。

Method: 提出xTime框架，利用知识蒸馏从低稀有度事件模型中转移信息，并引入专家混合机制动态选择和融合不同稀有度级别的专家模型输出。

Result: 在多个数据集上的实验表明，xTime显著提升了极端事件的预测准确性，改进幅度从3%到78%。

Conclusion: xTime通过知识蒸馏和专家混合机制有效解决了极端事件预测中的数据不平衡问题，显著提升了预测性能。

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [75] [From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/abs/2510.20668)
*Jinbin Bai,Yu Lei,Hecong Wu,Yuchen Zhu,Shufan Li,Yi Xin,Xiangtai Li,Molei Tao,Aditya Grover,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: 这是一篇关于构建世界模型的指南性论文，重点介绍了从早期掩码模型到统一架构、交互式生成模型，再到记忆增强系统的演进路径，强调生成核心、交互循环和记忆系统是实现真正世界模型的关键要素。


<details>
  <summary>Details</summary>
Motivation: 为想要构建世界模型的研究者提供清晰的指导路线，而不是简单地综述所有相关文献。作者认为通过关注生成核心、交互循环和记忆系统这三个关键要素，是最有希望实现真正世界模型的路径。

Method: 遵循一条明确的演进路线：从早期跨模态统一表示学习的掩码模型，到共享单一范式的统一架构，再到闭合动作-感知循环的交互式生成模型，最后到维持时间一致性的记忆增强系统。

Result: 提出了构建世界模型的系统性框架，识别了生成核心、交互循环和记忆系统作为实现真正世界模型的关键技术要素。

Conclusion: 通过专注于生成核心、交互循环和记忆系统这三个核心要素的演进路径，是构建真正世界模型的最有希望的方法，避免了分散关注松散相关的分支技术。

Abstract: This is not a typical survey of world models; it is a guide for those who
want to build worlds. We do not aim to catalog every paper that has ever
mentioned a ``world model". Instead, we follow one clear road: from early
masked models that unified representation learning across modalities, to
unified architectures that share a single paradigm, then to interactive
generative models that close the action-perception loop, and finally to
memory-augmented systems that sustain consistent worlds over time. We bypass
loosely related branches to focus on the core: the generative heart, the
interactive loop, and the memory system. We show that this is the most
promising path towards true world models.

</details>


### [76] [Separating the what and how of compositional computation to enable reuse and continual learning](https://arxiv.org/abs/2510.20709)
*Haozhe Shan,Sun Minni,Lea Duncker*

Main category: cs.LG

TL;DR: 该论文提出了一种双系统方法来实现持续学习和技能组合重用：一个系统推断要执行什么计算，另一个系统执行如何计算。通过概率生成模型描述任务组合性，并使用无监督在线学习构建任务词汇表，实现RNN低秩组件的组合使用，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 研究持续学习和灵活组合技能的神经机制，解决智能行为中持续学习、保持和部署技能的关键问题。

Method: 采用双系统方法：what系统通过概率生成模型推断计算上下文，how系统作为RNN根据上下文组合低秩组件。使用无监督在线学习构建任务词汇表，实现单次试验学习。

Result: 该方法在示例任务集上表现出高效性和竞争力，实现了正向和反向迁移，以及快速组合泛化到未见任务的能力。

Conclusion: 双系统学习框架通过上下文推断促进了低秩RNN组件的创建、学习和重用，实现了持续学习而不会发生灾难性遗忘。

Abstract: The ability to continually learn, retain and deploy skills to accomplish
goals is a key feature of intelligent and efficient behavior. However, the
neural mechanisms facilitating the continual learning and flexible
(re-)composition of skills remain elusive. Here, we study continual learning
and the compositional reuse of learned computations in recurrent neural network
(RNN) models using a novel two-system approach: one system that infers what
computation to perform, and one that implements how to perform it. We focus on
a set of compositional cognitive tasks commonly studied in neuroscience. To
construct the what system, we first show that a large family of tasks can be
systematically described by a probabilistic generative model, where
compositionality stems from a shared underlying vocabulary of discrete task
epochs. The shared epoch structure makes these tasks inherently compositional.
We first show that this compositionality can be systematically described by a
probabilistic generative model. Furthermore, We develop an unsupervised online
learning approach that can learn this model on a single-trial basis, building
its vocabulary incrementally as it is exposed to new tasks, and inferring the
latent epoch structure as a time-varying computational context within a trial.
We implement the how system as an RNN whose low-rank components are composed
according to the context inferred by the what system. Contextual inference
facilitates the creation, learning, and reuse of low-rank RNN components as new
tasks are introduced sequentially, enabling continual learning without
catastrophic forgetting. Using an example task set, we demonstrate the efficacy
and competitive performance of this two-system learning framework, its
potential for forward and backward transfer, as well as fast compositional
generalization to unseen tasks.

</details>


### [77] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 本研究通过数据驱动方法优化约翰霍普金斯跌倒风险评估工具(JHFRAT)，结合电子健康记录变量，使用约束评分优化模型显著提升了跌倒风险预测性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数据驱动建模方法，将JHFRAT跌倒风险预测与更多临床有意义指标对齐，改进住院患者跌倒预防方案。

Method: 对54,209例住院患者进行回顾性分析，使用约束评分优化(CSO)模型结合JHFRAT评估数据和电子健康记录变量。

Result: CSO模型预测性能显著优于当前JHFRAT(AUC-ROC=0.91 vs 0.86)，在风险标签变化时表现出更强的鲁棒性。

Conclusion: 这种循证方法为医疗系统提供了坚实基础，可使用数据驱动优化技术系统性增强住院患者跌倒预防方案和患者安全。

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [78] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: 本文为Thompson采样在强化学习中的理论分析，建立了具有高斯边际分布的模型在序列决策中的无遗憾保证，证明了在具有联合高斯过程先验的episodic强化学习中的遗憾界。


<details>
  <summary>Details</summary>
Motivation: Thompson采样在序列决策中应用广泛，但在复杂时间结构设置（如强化学习）中的理论基础仍然有限，需要填补这一空白。

Method: 考虑在episodic强化学习中使用联合高斯过程先验对奖励和转移概率进行建模的Thompson采样，分析非高斯价值函数和Bellman更新的递归结构等挑战。

Result: 证明了在K个episode、每个episode长度为H的情况下，遗憾界为$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$，其中$\Gamma(\cdot)$捕获高斯过程模型的复杂度。

Conclusion: 这项工作推进了对Thompson采样在强化学习中理解，突出了结构假设和模型不确定性如何影响其在有限时域马尔可夫决策过程中的性能。

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [79] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于狄利克雷过程的多模态学习框架，通过DP的富者愈富特性自动平衡模态内特征学习和跨模态对齐，在多个数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态融合中如何在保持各模态特征表达能力的同时学习跨模态交互的关键挑战，避免过度强调模态边缘分布对齐导致的过度正则化问题。

Method: 采用狄利克雷过程混合模型，假设每个模态遵循多元高斯混合分布，利用DP计算各分量的混合权重，动态分配特征贡献并选择最显著特征。

Result: 在多个多模态数据集上的广泛实验表明，该模型优于其他竞争方法，消融分析验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。

Conclusion: 提出的DP驱动多模态学习框架能够自动实现模态内显著表示学习和跨模态对齐之间的最优平衡，为多模态特征融合提供了有效解决方案。

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [80] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: MEIcoder是一种基于生物信息的解码方法，利用神经元特异性最兴奋输入(MEIs)、结构相似性指数损失和对抗训练，在小数据集上实现视觉刺激重建的先进性能。


<details>
  <summary>Details</summary>
Motivation: 灵长类或人类等生物数据通常稀缺，高吞吐量记录技术难以应用，这对深度学习解码技术构成挑战。

Method: 引入MEIcoder方法，结合神经元特异性最兴奋输入(MEIs)、结构相似性指数损失和对抗训练。

Result: 在初级视觉皮层单细胞活动中重建视觉刺激方面达到最先进性能，特别是在神经元记录较少的小数据集上表现优异。通过消融研究证明MEIs是性能主要驱动因素，能够在1,000-2,500个神经元和少于1,000个训练数据点的情况下重建高保真自然图像。

Conclusion: 结果证明了早期视觉系统中可靠解码的可行性，并为神经科学和神经工程应用提供了实用见解。

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [81] [Out-of-distribution Tests Reveal Compositionality in Chess Transformers](https://arxiv.org/abs/2510.20783)
*Anna Mészáros,Patrik Reizinger,Ferenc Huszár*

Main category: cs.LG

TL;DR: 该研究训练了一个270M参数的象棋Transformer模型，通过测试其在分布外场景下的表现来评估其对象棋规则的系统性泛化能力。研究发现模型在规则外推方面表现出组合泛化能力，但在更具挑战性的象棋变体（如Chess960）中表现不如基于符号搜索的AI算法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究现代决策Transformer在象棋任务中是否真正掌握了游戏规则，而非仅仅学习表面模式。通过设计分布外测试场景来揭示模型的系统性泛化失败。

Method: 训练270M参数的象棋Transformer模型，并在多种分布外场景下进行测试，包括规则外推测试和更具挑战性的象棋变体（如Chess960），同时分析训练动态。

Result: 模型表现出组合泛化能力，能够一致地选择有效移动，即使在训练数据中未见过的情境下。在OOD谜题中生成高质量移动。但在Chess960等变体中表现不如符号AI算法，尽管在Lichess对战中差距较小。训练动态显示模型最初只学习移动己方棋子，表明对游戏出现了组合性理解。

Conclusion: Transformer模型能够学习象棋的基本组合结构，在规则外推方面表现良好，但在需要更复杂策略适应的变体游戏中仍不如基于搜索的符号AI方法。训练过程揭示了模型逐步发展出对游戏规则的系统性理解。

Abstract: Chess is a canonical example of a task that requires rigorous reasoning and
long-term planning. Modern decision Transformers - trained similarly to LLMs -
are able to learn competent gameplay, but it is unclear to what extent they
truly capture the rules of chess. To investigate this, we train a 270M
parameter chess Transformer and test it on out-of-distribution scenarios,
designed to reveal failures of systematic generalization. Our analysis shows
that Transformers exhibit compositional generalization, as evidenced by strong
rule extrapolation: they adhere to fundamental syntactic rules of the game by
consistently choosing valid moves even in situations very different from the
training data. Moreover, they also generate high-quality moves for OOD puzzles.
In a more challenging test, we evaluate the models on variants including
Chess960 (Fischer Random Chess) - a variant of chess where starting positions
of pieces are randomized. We found that while the model exhibits basic strategy
adaptation, they are inferior to symbolic AI algorithms that perform explicit
search, but gap is smaller when playing against users on Lichess. Moreover, the
training dynamics revealed that the model initially learns to move only its own
pieces, suggesting an emergent compositional understanding of the game.

</details>


### [82] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: BadGraph是一种针对文本引导图生成的潜在扩散模型的隐蔽后门攻击方法，通过文本触发词植入后门，在推理时生成攻击者指定的子图，同时保持对干净输入的正常性能。


<details>
  <summary>Details</summary>
Motivation: 随着图生成技术的快速发展，出现了新的安全担忧，特别是后门漏洞问题。之前的工作主要探索了图像扩散和无条件图生成中的后门攻击，但文本引导的条件图生成领域尚未得到充分研究。

Method: BadGraph利用文本触发词毒化训练数据，在VAE和扩散训练阶段隐蔽地植入后门，当推理时出现触发词时诱导生成攻击者指定的子图。

Result: 在四个基准数据集上的实验表明，攻击具有高效性和隐蔽性：低于10%的毒化率即可达到50%的攻击成功率，24%的毒化率足以实现超过80%的成功率，而对良性样本的性能影响可忽略不计。

Conclusion: 这些发现揭示了文本引导图生成的潜在扩散模型中的安全漏洞，突显了在药物发现等模型应用中的严重风险，并强调需要针对此类扩散模型中的后门攻击开发鲁棒的防御措施。

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [83] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: 提出了一种快速、无需微调的LLM适配方法，通过选择性矩阵降维和聚类分解技术，仅需100个样本和单次梯度计算即可有效提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决LASER方法中逐层搜索和全数据集前向传播带来的计算开销问题，实现快速部署LLM到下游任务。

Method: 使用梯度分析识别关键矩阵，通过聚类分解扩展搜索空间，仅需少量样本计算梯度和评估性能。

Result: 相比原始方法，准确率提升高达24.6个百分点，同时大幅减少计算时间和数据需求。

Conclusion: 结合梯度分析、选择性矩阵检查和聚类分解，实现了无需微调的快速LLM适配，仅需100个样本即可有效工作。

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [84] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: 本文挑战了关于反向KL散度和前向KL散度在强化学习中作用的传统直觉，发现KL散度类型主要决定最优目标分布的参数化形式，而模式覆盖度主要取决于正则化强度等其他因素。作者基于这些洞察开发了一个简单算法，在语言模型和化学语言模型上实现了更高的解决方案质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为优化反向KL散度会导致"模式寻找"，而优化前向KL散度会导致"质量覆盖"，后者在需要从多个不同模式采样时更受青睐。但作者发现这种直觉在强化学习与KL正则化的结合中并不成立。

Method: 作者通过数学分析和实证研究，揭示了反向/前向KL选择主要决定最优目标分布的参数化形式。基于这些洞察，他们构建了一个简单、可扩展且理论上有依据的算法，该算法对奖励幅度进行最小修改，但优化出一个在所有高质量采样模式上都有高概率的目标分布。

Result: 实验表明，这个简单修改在大型语言模型和化学语言模型的后训练中都有效，提高了解决方案的质量和多样性，无需任何外部多样性信号，并且在单独使用前向或反向KL失败的情况下都能工作。

Conclusion: KL散度类型的选择并不直接决定模式覆盖度，模式覆盖主要取决于正则化强度、奖励与参考概率的相对尺度等因素。作者的方法通过最小化奖励修改实现了对高质量采样模式的更好覆盖。

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [85] [Moving or Predicting? RoleAware-MAPP: A Role-Aware Transformer Framework for Movable Antenna Position Prediction to Secure Wireless Communications](https://arxiv.org/abs/2510.20293)
*Wenxu Wang,Xiaowu Liu,Wei Gong,Yujia Zhao,Kaixuan Li,Qixun Zhang,Zhiyong Feng,Kan Yu*

Main category: cs.IT

TL;DR: 本文提出RoleAware-MAPP框架，通过Transformer模型和领域知识集成，解决可移动天线在物理层安全中的实时优化和时延不匹配问题，显著提升保密性能。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术面临实时优化计算复杂度高和机械运动与信道变化时延不匹配的挑战，现有数据驱动方法缺乏对通信领域知识特别是合法用户与窃听者不对称角色的考虑。

Method: 将可移动天线定位问题重构为预测任务，提出基于Transformer的RoleAware-MAPP框架，包含角色感知嵌入、物理信息语义特征和复合损失函数三个关键组件。

Result: 在3GPP兼容场景下，RoleAware-MAPP实现平均保密率0.3569 bps/Hz，严格正保密容量81.52%，分别比最强基线提升48.4%和5.39个百分点，在不同用户速度和噪声条件下保持稳健性能。

Conclusion: RoleAware-MAPP通过集成领域知识和Transformer架构，有效解决了可移动天线在物理层安全中的关键挑战，显著提升了保密性能并保持稳健性。

Abstract: Movable antenna (MA) technology provides a promising avenue for actively
shaping wireless channels through dynamic antenna positioning, thereby enabling
electromagnetic radiation reconstruction to enhance physical layer security
(PLS). However, its practical deployment is hindered by two major challenges:
the high computational complexity of real time optimization and a critical
temporal mismatch between slow mechanical movement and rapid channel
variations. Although data driven methods have been introduced to alleviate
online optimization burdens, they are still constrained by suboptimal training
labels derived from conventional solvers or high sample complexity in
reinforcement learning. More importantly, existing learning based approaches
often overlook communication-specific domain knowledge, particularly the
asymmetric roles and adversarial interactions between legitimate users and
eavesdroppers, which are fundamental to PLS. To address these issues, this
paper reformulates the MA positioning problem as a predictive task and
introduces RoleAware-MAPP, a novel Transformer based framework that
incorporates domain knowledge through three key components: role-aware
embeddings that model user specific intentions, physics-informed semantic
features that encapsulate channel propagation characteristics, and a composite
loss function that strategically prioritizes secrecy performance over mere
geometric accuracy. Extensive simulations under 3GPP-compliant scenarios show
that RoleAware-MAPP achieves an average secrecy rate of 0.3569 bps/Hz and a
strictly positive secrecy capacity of 81.52%, outperforming the strongest
baseline by 48.4% and 5.39 percentage points, respectively, while maintaining
robust performance across diverse user velocities and noise conditions.

</details>


### [86] [Robust Analog Lagrange Coded Computing: Theory and Algorithms via Discrete Fourier Transforms](https://arxiv.org/abs/2510.20379)
*Rimpi Borah,J. Harshan*

Main category: cs.IT

TL;DR: 提出了一个安全的模拟拉格朗日编码计算框架，能够抵御拜占庭工作节点的完整性威胁，通过DFT码错误校正算法提高计算精度，并利用工作节点信任档案优化任务分配。


<details>
  <summary>Details</summary>
Motivation: 现有的ALCC框架虽然能保护数据隐私并应对延迟节点，但无法抵御拜占庭工作节点返回错误结果的威胁，需要增强其安全性。

Method: 使用DFT码的错误校正算法构建新的重构策略，并基于DFT解码器性能理论结果提出任务分配策略，利用工作节点信任档案优化计算精度。

Result: 提出的安全ALCC框架显著提高了在拜占庭工作节点存在时的计算精度，并能有效应对合谋攻击。

Conclusion: 该框架为ALCC提供了强大的安全性保障，能够抵御多种完整性威胁，同时保持计算效率。

Abstract: Analog Lagrange Coded Computing (ALCC) is a recently proposed computational
paradigm wherein certain computations over analog datasets are efficiently
performed using distributed worker nodes through floating point representation.
While the vanilla version of ALCC is known to preserve the privacy of the
datasets from the workers and also achieve resilience against stragglers, it is
not robust against Byzantine workers that return erroneous results.
Highlighting this vulnerability, we propose a secure ALCC framework that is
resilient against a wide range of integrity threats from the Byzantine workers.
As a foundational step, we use error-correction algorithms for Discrete Fourier
Transform (DFT) codes to build novel reconstruction strategies for ALCC thereby
improving its computational accuracy in the presence of a bounded number of
Byzantine workers. Furthermore, capitalizing on some theoretical results on the
performance of the DFT decoders, we propose novel strategies for distributing
the ALCC computational tasks to the workers, and show that such methods
significantly improve the accuracy when the workers' trust profiles are
available at the master server. Finally, we study the robustness of the
proposed framework against colluding attacks, and show that interesting attack
strategies can be executed by exploiting the inherent precision noise owing to
floating point implementation.

</details>


### [87] [Simultaneous Wireless Information and Power Transfer for Fluid Antenna Systems](https://arxiv.org/abs/2510.20569)
*Feilong Zhang,Jianxin Dai,Zhaohui Yang,Kai-Kit Wong,Lingyuxiu Li,Jianglin Ye*

Main category: cs.IT

TL;DR: 提出了一种结合MISO流体天线与传统固定位置天线的新型通信系统，通过优化天线位置来提高能量收集效率，在SWIPT场景下显著提升了能量接收器的性能。


<details>
  <summary>Details</summary>
Motivation: 流体天线技术通过改变天线位置可以提高通信速率，本文旨在利用这一特性来提升SWIPT系统中的能量收集效率，解决传统固定位置天线在能量传输方面的局限性。

Method: 采用MISO流体天线与传统固定位置天线相结合的系统架构，通过优化发射和接收流体天线位置以及发射协方差矩阵，在满足信息接收器最小SINR约束条件下最大化能量接收器的功率。

Result: 仿真结果表明，相比传统固定位置天线系统，流体天线系统能够显著提升能量接收器的能量收集效率。

Conclusion: 流体天线系统在SWIPT场景中具有显著优势，通过天线位置优化可以有效提升能量传输性能，为未来无线通信系统的能量收集提供了新的技术路径。

Abstract: Fluid antenna is a promising wireless communication technology that enhances
communication rate by changing the antenna positions. This article proposes a
new communication system that combines multiple-input single-output (MISO)
fluid antennas with traditional fixed-position antennas, utilizing antenna
position optimization to improve energy harvesting efficiency. In this model,
we consider simultaneous wireless information and power transfer (SWIPT) which
transmits identical signals from the base station to both information receiver
(IR) and energy receiver (ER). We strive to enhance the power delivered to the
ER by fine-tuning the positions of transmit and receive fluid antennas, along
with optimizing the transmit covariance matrix, subject to a given minimum
signal-to-interference-plus-noise ratio (SINR) constraint at the IR. Simulation
results indicate that fluid antenna systems significantly enhance the energy
harvesting efficiency of the ER compared to traditional fixed-position
antennas.

</details>


### [88] [Super-Linear Growth of the Capacity-Achieving Input Support for the Amplitude-Constrained AWGN Channel](https://arxiv.org/abs/2510.20723)
*Haiyang Wang*

Main category: cs.IT

TL;DR: 本文研究了幅度受限AWGN信道中容量最优输入分布的支撑点数量增长问题，证明了支撑点数量K(A)随幅度约束A的增加呈超线性增长。


<details>
  <summary>Details</summary>
Motivation: 虽然已知幅度受限AWGN信道的最优输入是有限个离散点，但关于支撑点数量K(A)随幅度约束A增加的紧界仍然开放，需要深入研究。

Method: 结合输出分布在总变差意义下收敛到均匀分布的性质，以及高斯混合近似的定量限制，推导新的解析下界。

Result: 证明了支撑点数量K(A)随幅度约束A的增加呈超线性增长，提供了新的解析下界。

Conclusion: 幅度受限AWGN信道容量最优输入分布的支撑点数量随幅度约束增加呈超线性增长，这一结果深化了对信道容量优化问题的理解。

Abstract: We study the growth of the support size of the capacity-achieving input
distribution for the amplitude-constrained additive white Gaussian noise (AWGN)
channel. While it is known since Smith (1971) that the optimal input is
discrete with finitely many mass points, tight bounds on the number of support
points $K(A)$ as the amplitude constraint $A$ increases remain open. Building
on recent work by Dytso \emph{et al.} (2019) and Mattingly \emph{et al.}
(2018), we derive a new analytical lower bound showing that $K(A)$ grows
super-linearly in $A$. Our approach combines total-variation convergence of the
output distribution to the uniform law with quantitative limits on Gaussian
mixture approximation.

</details>


### [89] [MIMO-Zak-OTFS with Superimposed Spread Pilots](https://arxiv.org/abs/2510.20734)
*Abhishek Bairwa,Ananthanarayanan Chockalingam*

Main category: cs.IT

TL;DR: 本文提出了一种用于MIMO-Zak-OTFS系统的叠加扩频导频设计和有效信道估计方法，通过在交叉模糊域分离导频序列并使用turbo迭代来提升性能。


<details>
  <summary>Details</summary>
Motivation: 在MIMO-Zak-OTFS系统中，数据和导频信号叠加在同一帧内，需要设计有效的导频方案来分离不同发射天线的导频序列并实现准确的信道估计。

Method: 提出在交叉模糊域分离导频序列的设计，通过简单的读取操作估计有效信道抽头，并采用信道估计与检测之间的turbo迭代来减轻导频-数据干扰。

Result: 在2×2和3×3 MIMO-Zak-OTFS系统中，使用高斯sinc脉冲整形滤波器和Veh-A信道模型进行仿真，经过三次turbo迭代后能够实现非常好的估计/检测性能。

Conclusion: 所提出的导频设计和估计方案在MIMO-Zak-OTFS系统中能够有效分离导频序列并实现高性能的信道估计和检测。

Abstract: In this paper, we consider the problem of spread pilot design and effective
channel estimation in multiple-input multiple-output Zak-OTFS (MIMO-Zak-OTFS)
with superimposed spread pilots, where data and spread pilot signals are
superimposed in the same frame. To achieve good estimation performance in a
MIMO setting, the spread pilots at different transmit antennas need to be
effectively separated at the receiver. Towards this, we propose a spread pilot
design that separates the pilot sequences in the cross-ambiguity domain and
enables the estimation of the effective channel taps by a simple read-off
operation. To further alleviate the effect of pilot-data interference on
performance, we carry out turbo iterations between channel estimation and
detection. Simulation results for $2\times 2$ and $3\times 3$ MIMO-Zak-OTFS
with Gaussian-sinc pulse shaping filter for vehicular-A channel model show that
the proposed pilot design and estimation scheme with three turbo iterations can
achieve very good estimation/detection performance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [90] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: CourtGuard是一个本地运行的多智能体提示注入分类器，采用法庭式多智能体LLM系统来检测提示注入攻击，具有较低的误报率但整体检测性能较差。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在敏感应用中的集成，提示注入攻击风险日益增加，可能导致数据泄露、错误信息传播等危害行为，需要有效的防御机制。

Method: 设计法庭式多智能体系统：辩护律师模型论证提示是良性的，检察官模型论证提示是提示注入，法官模型给出最终分类。

Result: 与Direct Detector相比，CourtGuard具有更低的误报率，但整体上是一个较差的提示注入检测器。

Conclusion: 多智能体系统作为防御提示注入攻击的方法具有潜力，考虑对抗性和良性场景对提示分类很重要。

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [91] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: 链式思维（CoT）监控在无混淆压力下表现良好，但在强混淆压力下，部分模型能够完成对抗任务并逃避检测。模型对外部CoT的混淆程度高于内部CoT。


<details>
  <summary>Details</summary>
Motivation: 研究模型是否能够通过混淆链式思维来逃避监控，从而测试CoT监控的鲁棒性。

Method: 开发可组合和可量化的提示分类法来引发CoT混淆，在玩具任务和SHADE-Arena现实环境中评估内部和外部CoT。

Result: 无混淆压力时CoT监控准确高效；强混淆压力下部分模型能逃避检测；模型对外部CoT的混淆程度高于内部CoT。

Conclusion: CoT在良性设置下提供有价值的监督，但鲁棒部署需要对模型的可监控性进行特定压力测试。

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [92] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: 提出了一种面向监管工作流的政策治理RAG架构，通过三重机制确保审计就绪的生成：合同控制、清单追踪和收据验证，使政策检查可审计、可重放且收据支持。


<details>
  <summary>Details</summary>
Motivation: 在医药、医疗器械、金融、法律和公共部门等监管严格领域，错误成本可能超过数千欧元，且法规要求必须提供审计追踪。现有RAG/护栏系统缺乏可审计的政策检查机制。

Method: 采用三重架构：(I)合同控制确保输出符合法律和内部政策；(II)清单追踪通过加密锚定所有引用来源证据确保可验证来源；(III)收据验证为审计员提供最终便携合规证明。

Result: 设定了未经验证的目标：置信错误相对减少≥20%；p95延迟≤900毫秒；服务成本≤2.2倍。采用预注册的可选试点和NO-GO门控机制。

Conclusion: 该设计通过使政策检查可审计、可重放且收据支持，补充了现有RAG/护栏系统。未来评估可能承诺在NO-GO门控未满足时发布负面结果。

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [93] [MAC Aggregation over Lossy Channels in DTLS 1.3](https://arxiv.org/abs/2510.20419)
*Eric Wagner,David Heye,Jan Bauer,Klaus Wehrle,Martin Serror*

Main category: cs.CR

TL;DR: 本文研究了在DTLS 1.3协议中实现MAC聚合的实际性能，发现在有损信道中，MAC聚合能提高50%的有效吞吐量并节省17%的传输能耗。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，通过聚合消息认证码来节省带宽资源，尽管之前研究基于模拟表明在无线有损场景下也有益处，但需要在实际部署中验证这些说法。

Method: 设计了DTLS 1.3协议的MAC聚合扩展，并在嵌入式硬件上的完整通信协议栈中进行了广泛性能评估。

Result: 即使在有损信道中，MAC聚合确实能提高短消息传输的有效吞吐量达50%，并节省17%的能耗。

Conclusion: MAC聚合在现实部署中能够显著提升性能并节省能源，验证了其在资源受限环境中的实际价值。

Abstract: Aggregating Message Authentication Codes (MACs) promises to save valuable
bandwidth in resource-constrained environments. The idea is simple: Instead of
appending an authentication tag to each message in a communication stream, the
integrity protection of multiple messages is aggregated into a single tag.
Recent studies postulate, e.g., based on simulations, that these benefits also
spread to wireless, and thus lossy, scenarios despite each lost packet
typically resulting in the loss of integrity protection information for
multiple messages. In this paper, we investigate these claims in a real
deployment. Therefore, we first design a MAC aggregation extension for the
Datagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we
extensively evaluate the performance of MAC aggregation on a complete
communication protocol stack on embedded hardware. We find that MAC aggregation
can indeed increase goodput by up to 50% and save up to 17% of energy
expenditure for the transmission of short messages, even in lossy channels.

</details>


### [94] [Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers](https://arxiv.org/abs/2510.19885)
*James Kim*

Main category: cs.CR

TL;DR: 比较数学构造的S盒与随机生成S盒的性能差异，建立随机排列的平均性能指标，并在简单SPN结构中与传统设计进行对比


<details>
  <summary>Details</summary>
Motivation: 数学构造的S盒具有可证明的密码学特性，而完全随机方法缺乏理论保证但复杂度低且结果多样，需要评估随机方法的相对弱点

Method: 比较数学构造与随机生成的S盒，建立随机排列的平均性能度量，包括强制循环约束的随机生成，在简单SPN设置中与传统设计对比

Result: 数学构造的S盒能生成数千个具有高非线性度、APN特性和平衡雪崩特性的S盒，而随机方法缺乏理论保证

Conclusion: 数学构造方法在密码学特性方面优于随机生成方法，但随机方法在复杂度和结果多样性方面具有优势

Abstract: Mathematically constructed S-boxes arise from algebraic structures and finite
field theory to ensure strong, provable cryptographic properties. These
mathematically grounded constructions allow for generation of thousands of
S-Boxes with high nonlinearity, APN properties, and balanced avalanche
characteristics, unlike fully random methods, which lack such theoretical
guarantees in exchange for low complexity and more varied results. In this
work, we compare mathematically constructed constructions with randomly
generated ones to evaluate the relative weakness of the latter. We also
establish an average measure of performance for randomly generated
permutations, as well as random with forced cycle constraints, and compare them
to well-established designs in a simple SPN setting.

</details>


### [95] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: 提出了一种硬件加速的混合同态加密架构，通过专用硬件加速器和微架构优化，显著降低了边缘设备上的加密延迟和功耗，在PYNQ-Z2平台上实现了50倍客户端加密延迟降低和2倍硬件吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 完全同态加密在隐私保护机器学习中具有潜力，但给客户端带来显著的通信和计算开销，不适用于边缘设备。混合同态加密通过结合对称加密和FHE来解决这一限制，但需要硬件加速来进一步提高性能。

Method: 设计了一个围绕轻量级对称密码构建的硬件加速HHE架构，该密码针对FHE兼容性进行了优化，并作为专用硬件加速器实现。提出了多种微架构优化以实现更高性能和能效，并将该工作集成到完整的PPML流程中。

Result: 在PYNQ-Z2平台上的MNIST数据集实验中，与现有基于FPGA的HHE加速器相比，客户端加密延迟降低了50倍以上，硬件吞吐量提升了近2倍。

Conclusion: 验证了低功耗硬件加速HHE在边缘部署中的可行性，为在资源受限环境中构建可扩展的安全机器学习系统提供了硬件-软件协同设计方法。

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [96] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: SecureInfer是一个混合框架，利用异构可信执行环境（TEEs）-GPU架构，在保护LLM模型隐私的同时，将计算密集型操作卸载到不可信加速器上执行。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在移动和边缘平台的部署增加，保护模型免受模型提取攻击变得迫切。需要在保护模型隐私的同时不牺牲不可信AI加速器（如GPU）的性能优势。

Method: 采用信息论和威胁感知的分区策略：安全敏感组件（非线性层、注意力头投影、FNN变换和LoRA适配器）在SGX enclave内执行，其他线性操作（矩阵乘法）在GPU上加密执行并在enclave内安全恢复。

Result: 使用LLaMA-2模型实现SecureInfer原型，评估显示其在提供强大安全保证的同时具有合理的性能。

Conclusion: SecureInfer为安全设备端模型推理提供了一个实用的解决方案，在安全性和性能之间取得了良好平衡。

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [97] [zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements](https://arxiv.org/abs/2510.20007)
*To-Wen Liu,Matthew Green*

Main category: cs.CR

TL;DR: zk-agreements协议通过结合零知识证明、安全多方计算和智能合约，在保护隐私的同时实现可计算的法律合同自动执行和验证，解决了区块链协议中透明性与机密性的根本矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统纸质合同和自然语言合同存在歧义、需要人工处理且缺乏可计算验证性，阻碍了数字商务效率。基于区块链的智能合约虽然增强了可执行性，但会暴露敏感协议条款，引发隐私和商业竞争担忧。

Method: 设计zk-agreements协议，结合零知识证明保护私有协议条款、安全两方计算实现私有合规性评估、智能合约保证自动执行，实现隐私保护和计算可执行性。

Result: 该协议成功解决了区块链协议中透明性与机密性的根本矛盾，实现了隐私保护与计算可执行性的统一。

Conclusion: zk-agreements为从纸质信任向加密信任的过渡提供了可行方案，在保持机密性的同时实现了自动化执行和验证，有望推动企业级区块链协议采用。

Abstract: Digital transactions currently exceed trillions of dollars annually, yet
traditional paper-based agreements remain a bottleneck for automation,
enforceability, and dispute resolution. Natural language contracts introduce
ambiguity, require manual processing, and lack computational verifiability, all
of which hinder efficient digital commerce. Computable legal contracts,
expressed in machine-readable formats, offer a potential solution by enabling
automated execution and verification. Blockchain-based smart contracts further
strengthen enforceability and accelerate dispute resolution; however, current
implementations risk exposing sensitive agreement terms on public ledgers,
raising serious privacy and competitive intelligence concerns that limit
enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from
paper-based trust to cryptographic trust while preserving confidentiality. Our
design combines zero-knowledge proofs to protect private agreement terms,
secure two-party computation to enable private compliance evaluation, and smart
contracts to guarantee automated enforcement. Together, these components
achieve both privacy preservation and computational enforceability, resolving
the fundamental tension between transparency and confidentiality in
blockchain-based agreements.

</details>


### [98] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: 提出了一种名为SAID的无训练防御范式，通过激活LLM内部的安全机制来防御越狱攻击，在保持模型性能的同时显著减少有害输出。


<details>
  <summary>Details</summary>
Motivation: 现有基于外部干预的防御策略缺乏通用性、影响模型效用且计算开销大，需要一种更鲁棒和可扩展的防御方法。

Method: 采用三阶段流程：模型原生意图蒸馏提取核心语义、最优安全前缀探测激活潜在安全意识、保守聚合策略确保鲁棒决策。

Result: 在5个开源LLM和6种先进越狱攻击上的实验表明，SAID显著优于现有防御方法，在减少有害输出的同时保持良性任务性能，计算开销最小。

Conclusion: 激活LLM内在安全机制是构建更安全可靠对齐AI系统的更鲁棒和可扩展路径。

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [99] [Separating Pseudorandom Generators from Logarithmic Pseudorandom States](https://arxiv.org/abs/2510.20131)
*Mohammed Barhoush*

Main category: cs.CR

TL;DR: 本文解决了PRG与对数大小PRS之间的量子黑盒分离问题，证明在具有逆访问的酉量子预言机下，PRG无法从PRS黑盒构造，从而分离了PRG与基于对数PRS的密码原语。


<details>
  <summary>Details</summary>
Motivation: 研究PRG与对数大小PRS之间的基本关系，解决PRG是否与对数PRS分离的开放问题，这对理解量子密码学中不同原语的相对强度至关重要。

Method: 构造一个具有逆访问的酉量子预言机，在该预言机下证明PRG无法从PRS（对数或线性大小）进行黑盒构造。

Result: 成功建立了PRG与PRS（对数或线性大小）之间的量子黑盒分离，并作为推论获得了PRG与数字签名、量子公钥加密等基于对数PRS的原语之间的分离。

Conclusion: PRG与PRS在量子黑盒模型下是分离的，这为量子密码学中不同原语的层次结构提供了重要理论依据。

Abstract: Pseudorandom generators (PRGs) are a foundational primitive in classical
cryptography, underpinning a wide range of constructions. In the quantum
setting, pseudorandom quantum states (PRSs) were proposed as a potentially
weaker assumption that might serve as a substitute for PRGs in cryptographic
applications. Two primary size regimes of PRSs have been studied:
logarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to
powerful cryptographic applications, such as digital signatures and quantum
public-key encryption, that have not been realized from their linear
counterparts. However, PRGs have only been black-box separated from linear
PRSs, leaving open the fundamental question of whether PRGs are also separated
from logarithmic PRSs.
  In this work, we resolve this open problem. We establish a quantum black-box
separation between (quantum-evaluable) PRGs and PRSs of either size regime.
Specifically, we construct a unitary quantum oracle with inverse access
relative to which no black-box construction of PRG from (logarithmic or linear)
PRS exists. As a direct corollary, we obtain separations between PRGs and
several primitives implied by logarithmic PRSs, including digital signatures
and quantum public-key encryption.

</details>


### [100] [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223)
*Divyanshu Kumar,Shreyas Jena,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 该论文系统研究了针对多模态大语言模型(MLLMs)的越狱攻击，发现即使简单的感知变换也能可靠绕过最先进的安全过滤器，在多模态输入下暴露出严重的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然取得了显著进展，但在跨模态处理方面存在关键脆弱性，容易受到对抗性攻击的利用。

Method: 评估了1,900个对抗性提示，涵盖三个高风险安全类别，测试了七种前沿模型。探索了FigStep-Pro(视觉关键词分解)、Intelligent Masking(语义混淆)和音频扰动(Wave-Echo、Wave-Pitch、Wave-Speed)等攻击技术的有效性。

Result: 结果显示严重漏洞：文本安全近乎完美的模型(0%攻击成功率)在感知修改输入下遭受>75%攻击成功率，FigStep-Pro在Llama-4变体中达到89%攻击成功率。音频攻击进一步揭示了提供商特定弱点，即使基本模态转换也能在技术查询中产生25%攻击成功率。

Conclusion: 这些发现揭示了以文本为中心的对齐与多模态威胁之间的关键差距，表明当前安全措施无法泛化到跨模态攻击。攻击的可访问性表明，稳健的多模态AI安全需要向更广泛的语义级推理进行范式转变。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress,
yet remain critically vulnerable to adversarial attacks that exploit weaknesses
in cross-modal processing. We present a systematic study of multimodal
jailbreaks targeting both vision-language and audio-language models, showing
that even simple perceptual transformations can reliably bypass
state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts
across three high-risk safety categories harmful content, CBRN (Chemical,
Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation
Material) tested against seven frontier models. We explore the effectiveness of
attack techniques on MLLMs, including FigStep-Pro (visual keyword
decomposition), Intelligent Masking (semantic obfuscation), and audio
perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe
vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer
>75\% attack success under perceptually modified inputs, with FigStep-Pro
achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further
uncover provider-specific weaknesses, with even basic modality transfer
yielding 25\% ASR for technical queries. These findings expose a critical gap
between text-centric alignment and multimodal threats, demonstrating that
current safeguards fail to generalize across cross-modal attacks. The
accessibility of these attacks, which require minimal technical expertise,
suggests that robust multimodal AI safety will require a paradigm shift toward
broader semantic-level reasoning to mitigate possible risks.

</details>


### [101] [Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates](https://arxiv.org/abs/2510.20300)
*Haojie Ji,Long Jin,Haowen Li,Chongshi Xin,Te Hu*

Main category: cs.CR

TL;DR: 本文提出了一种基于格式保持加密(FPE)的高精度汽车地理位置隐私保护机制，通过坐标数据密钥映射机制减少重复加密解密造成的地理位置精度损失，实验显示加密后关键区域热点数量减少98.9%，解密坐标数据恢复精度达到100%。


<details>
  <summary>Details</summary>
Motivation: 随着汽车位置数据在路线导航、驾驶监控和车辆调度等功能中的共享，隐私泄露风险日益增加，可能导致用户行为识别、敏感位置推断和轨迹重建等攻击。为减轻汽车位置共享带来的数据安全风险，需要开发有效的隐私保护机制。

Method: 提出基于格式保持加密(FPE)的地理坐标高精度隐私保护机制，设计了汽车坐标数据密钥映射机制，以减少重复加密解密造成的地理位置数据精度损失。

Result: 实验结果显示平均相对距离保留率(RDR)达到0.0844，加密后关键区域热点数量减少98.9%。解密精度实验分析表明，解密坐标数据实现了100%的恢复精度。

Conclusion: 这项工作为汽车位置数据提供了一种高精度隐私保护方法，为自动驾驶中的敏感数据共享提供了高效的数据安全解决方案。

Abstract: There are increasing risks of privacy disclosure when sharing the automotive
location data in particular functions such as route navigation, driving
monitoring and vehicle scheduling. These risks could lead to the attacks
including user behavior recognition, sensitive location inference and
trajectory reconstruction. In order to mitigate the data security risk caused
by the automotive location sharing, this paper proposes a high-precision
privacy protection mechanism based on format-preserving encryption (FPE) of
geographical coordinates. The automotive coordinate data key mapping mechanism
is designed to reduce to the accuracy loss of the geographical location data
caused by the repeated encryption and decryption. The experimental results
demonstrate that the average relative distance retention rate (RDR) reached
0.0844, and the number of hotspots in the critical area decreased by 98.9%
after encryption. To evaluate the accuracy loss of the proposed encryption
algorithm on automotive geographical location data, this paper presents the
experimental analysis of decryption accuracy, and the result indicates that the
decrypted coordinate data achieves a restoration accuracy of 100%. This work
presents a high-precision privacy protection method for automotive location
data, thereby providing an efficient data security solution for the sensitive
data sharing in autonomous driving.

</details>


### [102] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: 本文分析了深度强化学习在动态多变环境中的安全挑战，提出了基于扰动类型和攻击目标的对抗攻击分类框架，系统总结了各种鲁棒性训练策略和防御技术，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度强化学习在自动驾驶、智能制造和智慧医疗等复杂领域的广泛应用，如何提高其在动态多变环境中的安全性和鲁棒性成为核心研究问题。特别是在面对对抗攻击时，DRL可能出现严重性能下降甚至做出危险决策，因此在安全敏感场景中确保其稳定性至关重要。

Method: 首先介绍DRL基本框架并分析复杂变化环境中的主要安全挑战；提出基于扰动类型和攻击目标的对抗攻击分类框架；详细回顾针对DRL的主流对抗攻击方法，包括扰动状态空间、动作空间、奖励函数和模型空间等多种攻击方式；系统总结当前各种鲁棒性训练策略，包括对抗训练、竞争训练、鲁棒学习、对抗检测、防御蒸馏等相关防御技术。

Result: 建立了完整的对抗攻击分类框架，系统梳理了各类攻击方法和防御技术的优缺点，为DRL在对抗环境中的安全性研究提供了全面的技术参考。

Conclusion: 本文展望了DRL在对抗环境中的未来研究方向，强调在提高泛化能力、降低计算复杂度、增强可扩展性和可解释性等方面的研究需求，旨在为研究人员提供有价值的参考和方向指导。

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>
