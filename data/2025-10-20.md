<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Uno: A One-Stop Solution for Inter- and Intra-Datacenter Congestion Control and Reliable Connectivity](https://arxiv.org/abs/2510.15802)
*Tommaso Bonato,Sepehr Abdous,Abdul Kabbani,Ahmad Ghalayini,Nadeen Gebara,Terry Lam,Anup Agarwal,Tiancheng Chen,Zhuolong Yu,Konstantin Taranov,Mahmoud Elhaddad,Daniele De Sensi,Soudeh Ghorbani,Torsten Hoefler*

Main category: cs.NI

TL;DR: Uno是一个统一系统，用于解决数据中心内部和跨数据中心流量共存时的拥塞管理和流量路由问题，通过集成快速拥塞反应传输协议和结合擦除编码与自适应路由的负载均衡方案，显著提升流量完成时间。


<details>
  <summary>Details</summary>
Motivation: 云计算和AI工作负载导致数据中心内部和跨数据中心通信需求激增，但两种流量共存以及RTT差异使得拥塞管理和流量路由复杂化，特别是快速拥塞反应的内部流量与较慢跨数据中心流量的速率不公平性，以及跨数据中心消息的慢速丢包恢复问题。

Method: 提出Uno统一系统，集成快速拥塞反应的传输协议和公平速率控制，结合擦除编码和自适应路由的负载均衡方案。

Result: 与Gemini等现有方法相比，Uno显著改善了数据中心内部和跨数据中心流量的完成时间。

Conclusion: Uno系统通过统一的控制机制有效解决了数据中心内部和跨数据中心流量共存时的拥塞管理和公平性问题，在性能上优于现有解决方案。

Abstract: Cloud computing and AI workloads are driving unprecedented demand for
efficient communication within and across datacenters. However, the coexistence
of intra- and inter-datacenter traffic within datacenters plus the disparity
between the RTTs of intra- and inter-datacenter networks complicates congestion
management and traffic routing. Particularly, faster congestion responses of
intra-datacenter traffic causes rate unfairness when competing with slower
inter-datacenter flows. Additionally, inter-datacenter messages suffer from
slow loss recovery and, thus, require reliability. Existing solutions overlook
these challenges and handle inter- and intra-datacenter congestion with
separate control loops or at different granularities. We propose Uno, a unified
system for both inter- and intra-DC environments that integrates a transport
protocol for rapid congestion reaction and fair rate control with a load
balancing scheme that combines erasure coding and adaptive routing. Our
findings show that Uno significantly improves the completion times of both
inter- and intra-DC flows compared to state-of-the-art methods such as Gemini.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [2] [The Role of Federated Learning in Improving Financial Security: A Survey](https://arxiv.org/abs/2510.14991)
*Cade Houston Kennedy,Amr Hilal,Morteza Momeni*

Main category: cs.CR

TL;DR: 本文综述了联邦学习在金融安全中的应用，提出基于监管暴露程度的分类方法，涵盖从低暴露任务到高暴露任务的应用场景，并讨论了部署挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着数字金融系统的发展，传统机器学习模型在欺诈检测中需要集中访问敏感数据，存在隐私泄露风险。联邦学习提供了隐私保护的分布式训练方案，适合金融领域的数据安全需求。

Method: 采用调查综述方法，提出基于监管暴露程度的分类框架，分析联邦学习在金融系统中的实际应用，包括跨机构协作和物联网设备学习。

Result: 联邦学习在金融安全中展现出潜力，特别是在欺诈预防和区块链集成框架方面取得成功，但面临数据异构性、对抗攻击和监管合规等挑战。

Conclusion: 联邦学习有望推动安全、隐私合规的金融系统发展，未来需要结合区块链、差分隐私、安全多方计算和量子安全框架等技术来解决实施挑战。

Abstract: With the growth of digital financial systems, robust security and privacy
have become a concern for financial institutions. Even though traditional
machine learning models have shown to be effective in fraud detections, they
often compromise user data by requiring centralized access to sensitive
information. In IoT-enabled financial endpoints such as ATMs and POS Systems
that regularly produce sensitive data that is sent over the network. Federated
Learning (FL) offers a privacy-preserving, decentralized model training across
institutions without sharing raw data. FL enables cross-silo collaboration
among banks while also using cross-device learning on IoT endpoints. This
survey explores the role of FL in enhancing financial security and introduces a
novel classification of its applications based on regulatory and compliance
exposure levels ranging from low-exposure tasks such as collaborative portfolio
optimization to high-exposure tasks like real-time fraud detection. Unlike
prior surveys, this work reviews FL's practical use within financial systems,
discussing its regulatory compliance and recent successes in fraud prevention
and blockchain-integrated frameworks. However, FL deployment in finance is not
without challenges. Data heterogeneity, adversarial attacks, and regulatory
compliance make implementation far from easy. This survey reviews current
defense mechanisms and discusses future directions, including blockchain
integration, differential privacy, secure multi-party computation, and
quantum-secure frameworks. Ultimately, this work aims to be a resource for
researchers exploring FL's potential to advance secure, privacy-compliant
financial systems.

</details>


### [3] [A Light Weight Cryptographic Solution for 6LoWPAN Protocol Stack](https://arxiv.org/abs/2510.14993)
*Sushil Khairnar,Gaurav Bansod,Vijay Dahiphale*

Main category: cs.CR

TL;DR: 本文提出了一种轻量级密码LiCi2，专门为6LoWPAN协议栈设计，在内存占用、功耗和硬件实现方面都优于现有轻量级密码标准。


<details>
  <summary>Details</summary>
Motivation: 针对物联网等受限环境中的设备约束，需要设计比传统AES更轻量级的加密算法，满足低内存、低功耗和硬件效率的要求。

Method: 基于LiCi密码设计改进，开发了LiCi2轻量级密码，并在6LoWPAN协议栈中实现，替代传统重型加密算法AES。

Result: LiCi2仅需1856字节FLASH和1272字节RAM内存，功耗约25mW，硬件实现仅需1051个门等效单元，在各项指标上均优于现有轻量级密码PRESENT。

Conclusion: LiCi2在受限环境中具有明显优势，是物联网等约束环境下加密实现的理想选择。

Abstract: Lightweight cryptography is an emerging field in the field of research, which
endorses algorithms which are best suited for constrained environment. Design
metrics like Gate Equivalence (GE), Memory Requirement, Power Consumption, and
Throughput play a vital role in the applications like IoT. This paper presents
the 6LoWPAN Protocol Stack which is a popular standard of communication for
constrained devices. This paper presents an implementation of a lightweight
6LoWPAN Protocol stack by using a Light weight Cipher instead of regular heavy
encryption cipher AES. The cipher proposed in this paper is specifically
suitable for 6LoWPAN architecture as it addresses all the constraints possessed
by wireless sensor nodes. The lightweight cipher proposed in the paper needs
only 1856 bytes of FLASH and 1272 bytes of RAM memory which is less than any
other standard existing lightweight cipher design. The proposed ciphers power
consumption is around 25 mW which is significantly less as compared to ISO
certified lightweight cipher PRESENT which consumes around 38 mW of dynamic
power. This paper also discusses the detailed analysis of cipher against the
attacks like Linear Cryptanalysis, Differential Cryptanalysis, Biclique attack
and Avalanche attack. The cipher implementation on hardware is around 1051 GEs
for 64 bit of block size with 128 bit of key length which is less as compared
to existing lightweight cipher design. The proposed cipher LiCi2 is motivated
from LiCi cipher design but outclasses it in every design metric. We believe
the design of LiCi2 is the obvious choice for researchers to implement in
constrained environments like IoT.

</details>


### [4] [VaultGemma: A Differentially Private Gemma Model](https://arxiv.org/abs/2510.15001)
*Amer Sinha,Thomas Mesnard,Ryan McKenna,Daogao Liu,Christopher A. Choquette-Choo,Yangsibo Huang,Da Yu,George Kaissis,Zachary Charles,Ruibo Liu,Lynn Chua,Pritish Kamath,Pasin Manurangsi,Steve He,Chiyuan Zhang,Badih Ghazi,Borja De Balle Pigem,Prem Eruvbetine,Tris Warkentin,Armand Joulin,Ravi KumarAmer Sinha,Thomas Mesnard,Ryan McKenna,Daogao Liu,Christopher A. Choquette-Choo,Yangsibo Huang,Da Yu,George Kaissis,Zachary Charles,Ruibo Liu,Lynn Chua,Pritish Kamath,Pasin Manurangsi,Steve He,Chiyuan Zhang,Badih Ghazi,Borja De Balle Pigem,Prem Eruvbetine,Tris Warkentin,Armand Joulin,Ravi Kumar*

Main category: cs.CR

TL;DR: VaultGemma 1B是一个具有10亿参数的Gemma系列模型，完全使用差分隐私进行训练，代表了隐私保护大语言模型的重要进展。


<details>
  <summary>Details</summary>
Motivation: 开发一个在保持高性能的同时确保用户隐私安全的语言模型，推动隐私保护技术在大型语言模型中的应用。

Method: 使用与Gemma 2系列相同的数据混合进行预训练，并完全采用差分隐私训练方法。

Result: 成功开发并发布了VaultGemma 1B模型，这是一个具有10亿参数的隐私保护语言模型。

Conclusion: VaultGemma 1B的发布标志着隐私保护大语言模型发展的重要里程碑，为社区提供了开源的隐私保护AI解决方案。

Abstract: We introduce VaultGemma 1B, a 1 billion parameter model within the Gemma
family, fully trained with differential privacy. Pretrained on the identical
data mixture used for the Gemma 2 series, VaultGemma 1B represents a
significant step forward in privacy-preserving large language models. We openly
release this model to the community

</details>


### [5] [Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.15017)
*ChenYu Wu,Yi Wang,Yang Liao*

Main category: cs.CR

TL;DR: 提出了一种基于蜜罐的主动护栏系统，通过生成模糊、不可操作但语义相关的诱饵响应来探测用户意图，有效防御多轮越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临多轮越狱攻击威胁，现有被动防御方法要么无法应对自适应攻击者，要么过度限制良性用户。

Method: 微调诱饵模型生成诱饵响应，结合受保护LLM的安全回复，插入主动诱饵问题，通过多轮交互逐步暴露恶意意图。引入Honeypot Utility Score和Defense Efficacy Rate指标。

Result: 在MHJ数据集上的实验表明，该系统显著降低了越狱成功率，同时保持了良性用户体验。

Conclusion: 蜜罐式主动护栏系统将风险规避转化为风险利用，有效平衡安全性和可用性。

Abstract: Large language models (LLMs) are increasingly vulnerable to multi-turn
jailbreak attacks, where adversaries iteratively elicit harmful behaviors that
bypass single-turn safety filters. Existing defenses predominantly rely on
passive rejection, which either fails against adaptive attackers or overly
restricts benign users. We propose a honeypot-based proactive guardrail system
that transforms risk avoidance into risk utilization. Our framework fine-tunes
a bait model to generate ambiguous, non-actionable but semantically relevant
responses, which serve as lures to probe user intent. Combined with the
protected LLM's safe reply, the system inserts proactive bait questions that
gradually expose malicious intent through multi-turn interactions. We further
introduce the Honeypot Utility Score (HUS), measuring both the attractiveness
and feasibility of bait responses, and use a Defense Efficacy Rate (DER) for
balancing safety and usability. Initial experiment on MHJ Datasets with recent
attack method across GPT-4o show that our system significantly disrupts
jailbreak success while preserving benign user experience.

</details>


### [6] [Physical Layer Deception based on Semantic Distortion](https://arxiv.org/abs/2510.15063)
*Wenwen Chen,Bin Han,Yao Zhu,Anke Schmeink,Giuseppe Caire,Hans D. Schotten*

Main category: cs.CR

TL;DR: 该论文将物理层欺骗框架扩展到语义通信模型，通过理论分析和优化算法，使发送器能够优化加密策略，在保证合法接收者低语义失真的同时最大化窃听者的语义失真。


<details>
  <summary>Details</summary>
Motivation: 传统的物理层安全主要依赖被动防御，该研究旨在将欺骗技术集成到物理层安全中，实现主动对抗窃听的措施，特别是在语义通信场景下提升安全性。

Method: 采用语义失真作为性能指标，分析接收者解密策略选择和发送者加密策略优化，提出高效的优化算法，并在多种场景下推导出闭式最优解。

Result: 通过理论分析和数值模拟验证，所提出的算法能够有效优化资源分配和加密参数，在保证合法接收者低语义失真的同时显著增加窃听者的语义失真。

Conclusion: 该研究成功将物理层欺骗框架扩展到语义通信，提出的优化算法具有实用价值，能够为语义通信安全提供有效的主动防御机制。

Abstract: Physical layer deception (PLD) is a framework we previously introduced that
integrates physical layer security (PLS) with deception techniques, enabling
proactive countermeasures against eavesdropping rather than relying solely on
passive defense. We extend this framework to a semantic communication model and
conduct a theoretical analysis using semantic distortion as the performance
metric. In this work, we further investigate the receiver's selection of
decryption strategies and the transmitter's optimization of encryption
strategies. By anticipating the decryption strategy likely to be employed by
the legitimate receiver and eavesdropper, the transmitter can optimize resource
allocation and encryption parameters, thereby maximizing the semantic
distortion at the eavesdropper while maintaining a low level of semantic
distortion for the legitimate receiver. We present a rigorous analysis of the
resulting optimization problem, propose an efficient optimization algorithm,
and derive closed-form optimal solutions for multiple scenarios. Finally, we
corroborate the theoretical findings with numerical simulations, which also
confirm the practicality of the proposed algorithm.

</details>


### [7] [Sequential Comics for Jailbreaking Multimodal Large Language Models via Structured Visual Storytelling](https://arxiv.org/abs/2510.15068)
*Deyue Zhang,Dongdong Yang,Junjie Mu,Quancheng Zou,Zonghao Ying,Wenzhuo Xu,Zhao Liu,Xuan Wang,Xiangzheng Zhang*

Main category: cs.CR

TL;DR: 提出一种利用连环漫画式视觉叙事来绕过多模态大语言模型安全对齐的新方法，通过将恶意查询分解为视觉无害的叙事元素，生成图像序列，利用模型对叙事连贯性的依赖来引发有害输出。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然能力显著但仍易受跨模态漏洞的越狱攻击，现有安全机制存在明显缺陷。

Method: 使用辅助LLM将恶意查询分解为视觉无害的叙事元素，通过扩散模型生成对应图像序列，利用模型对叙事连贯性的依赖进行攻击。

Result: 在已建立的安全基准测试中，该方法平均攻击成功率达到83.5%，比现有最优方法提升46%，在各类有害内容上均表现出优越效果。

Conclusion: 揭示了多模态安全机制中的关键漏洞因素，现有防御策略对叙事驱动攻击存在显著保护缺口。

Abstract: Multimodal large language models (MLLMs) exhibit remarkable capabilities but
remain susceptible to jailbreak attacks exploiting cross-modal vulnerabilities.
In this work, we introduce a novel method that leverages sequential comic-style
visual narratives to circumvent safety alignments in state-of-the-art MLLMs.
Our method decomposes malicious queries into visually innocuous storytelling
elements using an auxiliary LLM, generates corresponding image sequences
through diffusion models, and exploits the models' reliance on narrative
coherence to elicit harmful outputs. Extensive experiments on harmful textual
queries from established safety benchmarks show that our approach achieves an
average attack success rate of 83.5\%, surpassing prior state-of-the-art by
46\%. Compared with existing visual jailbreak methods, our sequential narrative
strategy demonstrates superior effectiveness across diverse categories of
harmful content. We further analyze attack patterns, uncover key vulnerability
factors in multimodal safety mechanisms, and evaluate the limitations of
current defense strategies against narrative-driven attacks, revealing
significant gaps in existing protections.

</details>


### [8] [Bilinear Compressive Security](https://arxiv.org/abs/2510.15380)
*Axel Flinth,Hubert Orlicki,Semira Einsele,Gerhard Wunder*

Main category: cs.CR

TL;DR: 本文提出了一种新的双线性压缩安全（BCS）方法，通过在压缩感知加密基础上加入随机滤波器卷积，显著提高了对已知明文攻击的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知加密方法在测量矩阵不变的情况下容易受到已知明文攻击，仅需n次观测即可恢复密钥。需要设计更安全的加密方案。

Method: 在传统线性编码的基础上，发送方使用随机生成的滤波器h对编码后的向量进行卷积。接收方通过盲解卷积从y=h*Qx中恢复消息x，无需知道滤波器h。

Result: 在滤波器h满足弱对称条件下，恢复密钥Q需要Ω(max(n,(n/s)^2))个消息样本，当s=1时完全无法恢复密钥，安全性显著优于标准压缩感知加密。

Conclusion: BCS方法通过引入双线性操作大大增强了压缩感知加密的安全性，即使在有利于攻击者的假设条件下也能提供强大的安全保证。

Abstract: Beyond its widespread application in signal and image processing,
\emph{compressed sensing} principles have been greatly applied to secure
information transmission (often termed 'compressive security'). In this
scenario, the measurement matrix $Q$ acts as a one time pad encryption key (in
complex number domain) which can achieve perfect information-theoretic security
together with other benefits such as reduced complexity and energy efficiency
particularly useful in IoT. However, unless the matrix is changed for every
message it is vulnerable towards known plain text attacks: only $n$
observations suffices to recover a key $Q$ with $n$ columns. In this paper, we
invent and analyze a new method (termed 'Bilinear Compressive Security (BCS)')
addressing these shortcomings: In addition to the linear encoding of the
message $x$ with a matrix $Q$, the sender convolves the resulting vector with a
randomly generated filter $h$. Assuming that $h$ and $x$ are sparse, the
receiver can then recover $x$ without knowledge of $h$ from $y=h*Qx$ through
blind deconvolution. We study a rather idealized known plaintext attack for
recovering $Q$ from repeated observations of $y$'s for different, known $x_k$,
with varying and unknown $h$ ,giving Eve a number of advantages not present in
practice. Our main result for BCS states that under a weak symmetry condition
on the filter $h$, recovering $Q$ will require extensive sampling from
transmissions of $\Omega\left(\max\left(n,(n/s)^2\right)\right)$ messages $x_k$
if they are $s$-sparse. Remarkably, with $s=1$ it is impossible to recover the
key. In this way, the scheme is much safer than standard compressed sensing
even though our assumptions are much in favor towards a potential attacker.

</details>


### [9] [PoTS: Proof-of-Training-Steps for Backdoor Detection in Large Language Models](https://arxiv.org/abs/2510.15106)
*Issam Seddik,Sami Souihi,Mohamed Tamaazousti,Sara Tucci Piergiovanni*

Main category: cs.CR

TL;DR: 提出Proof-of-Training Steps协议，通过分析语言建模头对输入扰动的敏感性来验证LLM训练过程是否遵循声明方案，能够早期检测后门攻击，验证步骤比训练步骤快3倍。


<details>
  <summary>Details</summary>
Motivation: 现有后训练验证方案如Proof-of-Learning对LLMs不实用，需要完全重新训练、对隐蔽操作缺乏鲁棒性、无法在训练期间提供早期检测。早期检测可显著降低计算成本。

Method: 引入Proof-of-Training Steps验证协议，让独立审计员确认LLM开发者遵循声明的训练方案（包括数据批次、架构和超参数），通过分析语言建模头对输入扰动的敏感性来检测训练偏差。

Result: 即使在10%训练数据中包含后门触发器，该协议也能显著降低攻击者实现高攻击成功率的能力。能够在注入步骤早期检测攻击，验证步骤比训练步骤快3倍。

Conclusion: 该协议有潜力增强LLM开发的可问责性和安全性，特别是针对内部威胁。

Abstract: As Large Language Models (LLMs) gain traction across critical domains,
ensuring secure and trustworthy training processes has become a major concern.
Backdoor attacks, where malicious actors inject hidden triggers into training
data, are particularly insidious and difficult to detect. Existing
post-training verification solutions like Proof-of-Learning are impractical for
LLMs due to their requirement for full retraining, lack of robustness against
stealthy manipulations, and inability to provide early detection during
training. Early detection would significantly reduce computational costs. To
address these limitations, we introduce Proof-of-Training Steps, a verification
protocol that enables an independent auditor (Alice) to confirm that an LLM
developer (Bob) has followed the declared training recipe, including data
batches, architecture, and hyperparameters. By analyzing the sensitivity of the
LLMs' language modeling head (LM-Head) to input perturbations, our method can
expose subtle backdoor injections or deviations in training. Even with backdoor
triggers in up to 10 percent of the training data, our protocol significantly
reduces the attacker's ability to achieve a high attack success rate (ASR). Our
method enables early detection of attacks at the injection step, with
verification steps being 3x faster than training steps. Our results highlight
the protocol's potential to enhance the accountability and security of LLM
development, especially against insider threats.

</details>


### [10] [Partitioning $\mathbb{Z}_{sp}$ in finite fields and groups of trees and cycles](https://arxiv.org/abs/2510.15108)
*Nikolaos Verykios,Christos Gogos*

Main category: cs.CR

TL;DR: 该论文研究了环Z_sp的代数与图结构，重点分析其分解为有限域、核和特殊子集。建立了F_s与pF_s之间的经典同构，引入弧和根树描述预周期结构，证明非s或p倍数的元素树可通过单位树的循环弧乘法生成。定义了集合D_sp并分析其图分解为环和预周期树，证明每个环包含可从有限域pF_s和sF_p环可预测导出的内环，讨论了D_sp在密码学中分析循环攻击和分解方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究Z_sp环的结构分解，特别是其代数与图论特性，旨在理解该环的预周期结构和循环行为，为密码学应用提供理论基础，特别是针对循环攻击和分解方法的安全性分析。

Method: 采用代数同构方法建立F_s与pF_s之间的经典同构，引入弧和根树概念描述预周期结构，通过乘法运算生成树结构，定义并分析特殊子集D_sp的图分解特性，证明环内环的可预测性。

Result: 建立了F_s与pF_s的经典同构关系，证明了非s或p倍数元素树可从单位树通过循环弧乘法生成，展示了D_sp集合图分解为环和预周期树的结构，证明了每个环包含可从有限域环导出的内环。

Conclusion: Z_sp环具有丰富的代数与图论结构，其分解特性和循环行为为密码学分析提供了重要工具，D_sp集合在分析循环攻击和分解方法方面具有显著密码学价值。

Abstract: This paper investigates the algebraic and graphical structure of the ring
$\mathbb{Z}_{sp}$, with a focus on its decomposition into finite fields,
kernels, and special subsets. We establish classical isomorphisms between
$\mathbb{F}_s$ and $p\mathbb{F}_s$, as well as $p\mathbb{F}_s^{\star}$ and
$p\mathbb{F}_s^{+1,\star}$. We introduce the notion of arcs and rooted trees to
describe the pre-periodic structure of $\mathbb{Z}_{sp}$, and prove that trees
rooted at elements not divisible by $s$ or $p$ can be generated from the tree
of unity via multiplication by cyclic arcs. Furthermore, we define and analyze
the set $\mathbb{D}_{sp}$, consisting of elements that are neither multiples of
$s$ or $p$ nor "off-by-one" elements, and show that its graph decomposes into
cycles and pre-periodic trees. Finally, we demonstrate that every cycle in
$\mathbb{Z}_{sp}$ contains inner cycles that are derived predictably from the
cycles of the finite fields $p\mathbb{F}_s$ and $s\mathbb{F}_p$, and we discuss
the cryptographic relevance of $\mathbb{D}_{sp}$, highlighting its potential
for analyzing cyclic attacks and factorization methods.

</details>


### [11] [AndroByte: LLM-Driven Privacy Analysis through Bytecode Summarization and Dynamic Dataflow Call Graph Generation](https://arxiv.org/abs/2510.15112)
*Mst Eshita Khatun,Lamine Noureddine,Zhiyong Sui,Aisha Ali-Gombe*

Main category: cs.CR

TL;DR: AndroByte是一个基于AI的Android隐私分析工具，利用LLM对字节码摘要进行推理，动态生成准确且可解释的数据流调用图，在隐私泄露检测方面优于传统工具。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用的指数级增长，保护用户隐私变得至关重要。Android应用经常在用户不知情的情况下收集、存储和共享敏感信息，传统的数据流分析方法实现复杂且容易出错，且严重依赖预定义的sink列表，限制了灵活性和可扩展性。

Method: 提出AndroByte工具，利用LLM对字节码摘要进行推理，从静态代码分析中动态生成准确且可解释的数据流调用图，不依赖预定义的传播规则或sink列表。

Result: AndroByte在动态生成数据流调用图方面达到89%的Fβ分数，在泄露检测方面优于FlowDroid和Amandroid等传统工具，并通过迭代字节码摘要提供全面且可解释的数据流和泄露检测洞察。

Conclusion: AndroByte通过AI驱动的隐私分析方法，克服了传统技术的局限性，提供了更有效、灵活和可解释的Android应用隐私风险评估解决方案。

Abstract: With the exponential growth in mobile applications, protecting user privacy
has become even more crucial. Android applications are often known for
collecting, storing, and sharing sensitive user information such as contacts,
location, camera, and microphone data often without the user's clear consent or
awareness raising significant privacy risks and exposure. In the context of
privacy assessment, dataflow analysis is particularly valuable for identifying
data usage and potential leaks. Traditionally, this type of analysis has relied
on formal methods, heuristics, and rule-based matching. However, these
techniques are often complex to implement and prone to errors, such as taint
explosion for large programs. Moreover, most existing Android dataflow analysis
methods depend heavily on predefined list of sinks, limiting their flexibility
and scalability. To address the limitations of these existing techniques, we
propose AndroByte, an AI-driven privacy analysis tool that leverages LLM
reasoning on bytecode summarization to dynamically generate accurate and
explainable dataflow call graphs from static code analysis. AndroByte achieves
a significant F\b{eta}-Score of 89% in generating dynamic dataflow call graphs
on the fly, outperforming the effectiveness of traditional tools like FlowDroid
and Amandroid in leak detection without relying on predefined propagation rules
or sink lists. Moreover, AndroByte's iterative bytecode summarization provides
comprehensive and explainable insights into dataflow and leak detection,
achieving high, quantifiable scores based on the G-Eval metric.

</details>


### [12] [Beyond the Voice: Inertial Sensing of Mouth Motion for High Security Speech Verification](https://arxiv.org/abs/2510.15173)
*Ynes Ineza,Muhammad A. Ullah,Abdul Serwadda,Aurore Munyaneza*

Main category: cs.CR

TL;DR: 该论文提出了一种结合声学证据和说话者下脸部独特运动模式的第二认证因子，通过在嘴部周围放置轻量级惯性传感器来捕捉嘴部开口和演化下脸部几何形状，从而记录具有强区分能力的独特运动签名。


<details>
  <summary>Details</summary>
Motivation: 随着语音接口在高风险领域（如移动银行、智能家居安全和免提医疗）的广泛应用，现代生成模型使得高质量语音伪造变得廉价且易于创建，削弱了单独语音认证的可靠性。

Method: 在嘴部周围放置轻量级惯性传感器，捕捉嘴部开口和演化下脸部几何形状，记录独特的运动签名。构建原型并招募43名参与者在四种条件下（坐着、平地行走、楼梯行走、不同语言背景说话）评估系统。

Result: 在所有场景下，该方法始终实现了中位数等错误率（EER）为0.01或更低，表明嘴部运动数据在步态、姿势和口语变化下保持稳健。

Conclusion: 这种第二道防线可以为语音认证系统提供切实的安全益处，特别是在高风险应用场景中。

Abstract: Voice interfaces are increasingly used in high stakes domains such as mobile
banking, smart home security, and hands free healthcare. Meanwhile, modern
generative models have made high quality voice forgeries inexpensive and easy
to create, eroding confidence in voice authentication alone. To strengthen
protection against such attacks, we present a second authentication factor that
combines acoustic evidence with the unique motion patterns of a speaker's lower
face. By placing lightweight inertial sensors around the mouth to capture mouth
opening and evolving lower facial geometry, our system records a distinct
motion signature with strong discriminative power across individuals. We built
a prototype and recruited 43 participants to evaluate the system under four
conditions seated, walking on level ground, walking on stairs, and speaking
with different language backgrounds (native vs. non native English). Across all
scenarios, our approach consistently achieved a median equal error rate (EER)
of 0.01 or lower, indicating that mouth movement data remain robust under
variations in gait, posture, and spoken language. We discuss specific use cases
where this second line of defense could provide tangible security benefits to
voice authentication systems.

</details>


### [13] [MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2510.15186)
*Gurusha Juneja,Jayanth Naga Sai Pasupulati,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.CR

TL;DR: MAGPIE是一个新的多智能体隐私评估基准，包含200个高风险任务，旨在评估多智能体协作场景中的隐私理解和保护能力。评估显示当前最先进的LLM智能体存在显著的隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有隐私基准仅关注简单的单轮交互，无法评估复杂协作场景中隐私保护与任务效能之间的平衡。需要开发能强制智能体在协作中战略性控制信息的评估框架。

Method: 开发MAGPIE基准，包含200个高风险任务，将私人信息设计为任务解决的必要元素，强制智能体在有效协作与信息控制之间取得平衡。

Result: GPT-5和Gemini 2.5-Pro等最先进智能体存在显著隐私泄露，Gemini 2.5-Pro泄露高达50.7%敏感信息，GPT-5泄露35.1%。智能体难以达成共识或完成任务，经常出现操纵和权力寻求等不良行为。

Conclusion: 当前LLM智能体缺乏稳健的隐私理解能力，在复杂环境中无法同时保持隐私保护和有效协作，需要进一步对齐优化。

Abstract: A core challenge for autonomous LLM agents in collaborative settings is
balancing robust privacy understanding and preservation alongside task
efficacy. Existing privacy benchmarks only focus on simplistic, single-turn
interactions where private information can be trivially omitted without
affecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent
contextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks
designed to evaluate privacy understanding and preservation in multi-agent
collaborative, non-adversarial scenarios. MAGPIE integrates private information
as essential for task resolution, forcing agents to balance effective
collaboration with strategic information control. Our evaluation reveals that
state-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit
significant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5
up to 35.1% of the sensitive information even when explicitly instructed not
to. Moreover, these agents struggle to achieve consensus or task completion and
often resort to undesirable behaviors such as manipulation and power-seeking
(e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These
findings underscore that current LLM agents lack robust privacy understanding
and are not yet adequately aligned to simultaneously preserve privacy and
maintain effective collaboration in complex environments.

</details>


### [14] [Flexible Threshold Multi-client Functional Encryption for Inner Product in Federated Learning](https://arxiv.org/abs/2510.15367)
*Ruyuan Zhang,Jinguang Han,Liqun Chen*

Main category: cs.CR

TL;DR: 本文提出了一种灵活阈值多客户端函数加密方案(FTMCFE-IP)，用于解决联邦学习中客户端退出和灵活阈值选择的问题。该方案支持客户端独立生成密文，无需交互，并能灵活设置阈值而无需重新初始化系统。


<details>
  <summary>Details</summary>
Motivation: 现有基于多客户端函数加密(MCFE)的方案无法支持客户端退出或灵活阈值选择，而这些特性对于实际联邦学习应用至关重要。

Method: 设计了FTMCFE-IP方案，包括定义和安全模型，提出了具体构造。该方案允许客户端在加密阶段灵活选择阈值，当在线客户端数量满足阈值时能正确解密。

Result: 方案的安全性得到了形式化证明，并通过实现和评估验证了其有效性。

Conclusion: 提出的FTMCFE-IP方案成功解决了联邦学习中客户端退出和灵活阈值选择的问题，为隐私保护的联邦学习提供了实用解决方案。

Abstract: Federated learning (FL) is a distributed machine learning paradigm that
enables multiple clients to collaboratively train a shared model without
disclosing their local data. To address privacy issues of gradient, several
privacy-preserving machine-learning schemes based on multi-client functional
encryption (MCFE) have been proposed. However, existing MCFE-based schemes
cannot support client dropout or flexible threshold selection, which are
essential for practical FL. In this paper, we design a flexible threshold
multi-client functional encryption for inner product (FTMCFE-IP) scheme, where
multiple clients generate ciphertexts independently without any interaction. In
the encryption phase, clients are able to choose a threshold flexibly without
reinitializing the system. The decryption can be performed correctly when the
number of online clients satisfies the threshold. An authorized user are
allowed to compute the inner product of the vectors associated with his/her
functional key and the ciphertext, respectively, but cannot learning anything
else. Especially, the presented scheme supports clients drop out. Furthermore,
we provide the definition and security model of our FTMCFE-IP scheme,and
propose a concrete construction. The security of the designed scheme is
formally proven. Finally, we implement and evaluate our FTMCFE-IP scheme.

</details>


### [15] [FHE-SQL: Fully Homomorphic Encrypted SQL Database](https://arxiv.org/abs/2510.15413)
*Po-Yu Tseng,Po-Chu Hsu,Shih-Wei Liao*

Main category: cs.CR

TL;DR: FHE-SQL是一个基于全同态加密的隐私保护数据库系统，允许在加密数据上安全执行查询，服务器无法获知查询内容或底层数据。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护数据库系统存在安全漏洞：基于属性保持加密的系统易受频率、排序和等值模式推断攻击；基于可信硬件的系统依赖硬件安全模块，存在信任和侧信道限制；高性能FHE引擎仅支持特定工作负载。

Method: 采用全同态加密进行完全加密计算，消除泄露通道；使用间接架构分离RocksDB中的元数据和blob存储中的大密文；支持通过同态布尔掩码进行无感知选择、多级缓存和垃圾回收。

Result: 实现了端到端加密保护，无需可信执行环境；支持通用的SQL查询语义，具有模式感知和类型安全的定义；在通用可组合框架下证明安全性。

Conclusion: FHE-SQL提供了一个安全、通用的隐私保护数据库解决方案，克服了现有系统的安全限制，同时支持完整的关系数据管理功能。

Abstract: FHE-SQL is a privacy-preserving database system that enables secure query
processing on encrypted data using Fully Homomorphic Encryption (FHE),
providing privacy guaranties where an untrusted server can execute encrypted
queries without learning either the query contents or the underlying data.
Unlike property-preserving encryption-based systems such as CryptDB, which rely
on deterministic or order-preserving encryption and are vulnerable to
frequency, order, and equality-pattern inference attacks, FHE-SQL performs
computations entirely under encryption, eliminating these leakage channels.
Compared to trusted-hardware approaches such as TrustedDB, which depend on a
hardware security module and thus inherit its trust and side-channel
limitations, our design achieves end-to-end cryptographic protection without
requiring trusted execution environments. In contrast to high-performance
FHE-based engines-Hermes, which target specialized workloads such as vector
search, FHE-SQL supports general SQL query semantics with schema-aware,
type-safe definitions suitable for relational data management. FHE-SQL
mitigates the high cost of ciphertext space by using an indirection
architecture that separates metadata in RocksDB from large ciphertexts in blob
storage. It supports oblivious selection via homomorphic boolean masks,
multi-tier caching, and garbage collection, with security proven under the
Universal Composability framework.

</details>


### [16] [SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models](https://arxiv.org/abs/2510.15476)
*Hanbin Hong,Shuya Feng,Nima Naderloui,Shenao Yan,Jingyu Zhang,Biying Liu,Ali Arastehfard,Heqing Huang,Yuan Hong*

Main category: cs.CR

TL;DR: 本文提出了一个关于LLM提示安全的系统化知识框架，包括多级分类法、可重现评估的威胁模型、开源评估工具包、最大标注数据集JAILBREAKDB以及最先进方法的综合评估和排行榜。


<details>
  <summary>Details</summary>
Motivation: LLM在现实应用中的广泛部署暴露了关键安全风险，特别是通过越狱提示绕过模型对齐并产生有害输出。尽管攻击和防御技术研究激烈，但该领域仍存在定义、威胁模型和评估标准不统一的问题，阻碍了系统性进展和公平比较。

Method: 1) 提出全面的多级分类法来组织LLM提示安全中的攻击、防御和漏洞；2) 将威胁模型和成本假设形式化为机器可读配置文件；3) 引入开源评估工具包进行标准化、可审计的攻击和防御比较；4) 发布迄今为止最大的标注数据集JAILBREAKDB；5) 提供最先进方法的综合评估和排行榜。

Result: 构建了一个统一的框架来整合碎片化研究，为未来研究提供了严格基础，并支持开发适用于高风险部署的稳健、可信赖的LLM。

Conclusion: 该工作统一了碎片化研究，为未来研究提供了严格基础，并支持开发适用于高风险部署的稳健、可信赖的LLM。

Abstract: Large Language Models (LLMs) have rapidly become integral to real-world
applications, powering services across diverse sectors. However, their
widespread deployment has exposed critical security risks, particularly through
jailbreak prompts that can bypass model alignment and induce harmful outputs.
Despite intense research into both attack and defense techniques, the field
remains fragmented: definitions, threat models, and evaluation criteria vary
widely, impeding systematic progress and fair comparison. In this
Systematization of Knowledge (SoK), we address these challenges by (1)
proposing a holistic, multi-level taxonomy that organizes attacks, defenses,
and vulnerabilities in LLM prompt security; (2) formalizing threat models and
cost assumptions into machine-readable profiles for reproducible evaluation;
(3) introducing an open-source evaluation toolkit for standardized, auditable
comparison of attacks and defenses; (4) releasing JAILBREAKDB, the largest
annotated dataset of jailbreak and benign prompts to date; and (5) presenting a
comprehensive evaluation and leaderboard of state-of-the-art methods. Our work
unifies fragmented research, provides rigorous foundations for future studies,
and supports the development of robust, trustworthy LLMs suitable for
high-stakes deployment.

</details>


### [17] [HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment](https://arxiv.org/abs/2510.15499)
*Yuexiao Liu,Lijun Li,Xingjun Wang,Jing Shao*

Main category: cs.CR

TL;DR: 本文首次系统研究了RLVR的对齐可逆性风险，发现仅用64个有害提示即可快速逆转安全对齐，使模型轻易遵守有害指令，在多个模型中攻击成功率达96.01%，严重威胁开源模型安全。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在推理和代码生成任务中表现出色，但其潜在安全风险尚未充分探索。本文旨在系统研究RLVR的对齐可逆性风险。

Method: 使用GRPO方法，仅需64个有害提示（无需响应）进行攻击，在Llama、Qwen和DeepSeek五个模型上进行实证研究。

Result: RLVR攻击将平均有害性得分提升至4.94，攻击成功率达96.01%，显著优于有害微调，同时保持一般能力。

Conclusion: RLVR可被高效利用进行有害对齐，对开源模型安全构成严重威胁。

Abstract: Recent advancements in Reinforcement Learning with Verifiable Rewards (RLVR)
have gained significant attention due to their objective and verifiable reward
signals, demonstrating strong performance in reasoning and code generation
tasks. However, the potential safety risks associated with RLVR remain
underexplored. This paper presents HarmRLVR, the first systematic investigation
into the alignment reversibility risk of RLVR. We show that safety alignment
can be rapidly reversed using GRPO with merely 64 harmful prompts without
responses, causing models to readily comply with harmful instructions. Across
five models from Llama, Qwen, and DeepSeek, we empirically demonstrate that
RLVR-based attacks elevate the average harmfulness score to 4.94 with an attack
success rate of 96.01\%, significantly outperforming harmful fine-tuning while
preserving general capabilities. Our findings reveal that RLVR can be
efficiently exploited for harmful alignment, posing serious threats to
open-source model safety. Please see our code at
https://github.com/lyxx2535/HarmRLVR.

</details>


### [18] [High Memory Masked Convolutional Codes for PQC](https://arxiv.org/abs/2510.15515)
*Meir Ariel*

Main category: cs.CR

TL;DR: 提出了一种基于高内存掩码卷积码的后量子密码系统，相比传统基于分组码的方案具有更强的安全性和灵活性，支持任意长度明文，解密时间为线性，计算成本均匀，安全边际比经典McEliece系统提高2100倍以上。


<details>
  <summary>Details</summary>
Motivation: 传统基于分组码的密码方案存在固定维度、有限纠错能力和安全性不足的问题，需要开发更安全、灵活的后量子密码系统来应对量子计算的威胁。

Method: 使用高内存掩码卷积码，通过更高比率的随机错误注入和多项式除法引入额外噪声来增强安全性，采用半可逆变换生成密集的类随机生成矩阵，接收端使用并行Viterbi解码器进行解密。

Result: 系统支持任意长度明文，解密时间为线性，每比特计算成本均匀，安全边际比McEliece系统提高2100倍以上，实现了高效的硬件和软件实现。

Conclusion: 该方案是一个强大的实用量子抵抗公钥密码系统候选方案，具有优越的安全性和实现效率。

Abstract: This paper presents a novel post-quantum cryptosystem based on high-memory
masked convolutional codes. Unlike conventional code-based schemes that rely on
block codes with fixed dimensions and limited error-correction capability, our
construction offers both stronger cryptographic security and greater
flexibility. It supports arbitrary plaintext lengths with linear-time
decryption and uniform per-bit computational cost, enabling seamless
scalability to long messages. Security is reinforced through a higher-rate
injection of random errors than in block-code approaches, along with additional
noise introduced via polynomial division, which substantially obfuscates the
underlying code structure. Semi-invertible transformations generate dense,
random-like generator matrices that conceal algebraic properties and resist
known structural attacks. Consequently, the scheme achieves cryptanalytic
security margins exceeding those of the classic McEliece system by factors
greater than 2100. Finally, decryption at the recipient employs an array of
parallel Viterbi decoders, enabling efficient hardware and software
implementation and positioning the scheme as a strong candidate for deployment
in practical quantum-resistant public-key cryptosystems.

</details>


### [19] [MalCVE: Malware Detection and CVE Association Using Large Language Models](https://arxiv.org/abs/2510.15567)
*Eduard Andrei Cristea,Petter Molnes,Jingyue Li*

Main category: cs.CR

TL;DR: 提出MalCVE工具，利用大语言模型检测JAR文件中的恶意软件，并通过检索增强生成技术识别恶意软件可能利用的CVE漏洞，实现97%的恶意软件检测准确率和65%的CVE关联召回率。


<details>
  <summary>Details</summary>
Motivation: 商业恶意软件检测工具成本高昂，缺乏将恶意软件与其利用的特定软件漏洞关联的工具，理解这种关联对分析历史威胁和主动防御至关重要。

Method: 开发MalCVE工具，集成二进制代码反编译、反混淆、基于LLM的代码摘要、语义相似性搜索和LLM驱动的CVE分类，使用检索增强生成技术识别CVE。

Result: 在3,839个JAR可执行文件的基准数据集上评估，恶意软件检测平均准确率达97%，成本远低于商业方案；CVE关联召回率@10达65%，与源代码分析研究相当。

Conclusion: MalCVE是首个将CVE与二进制恶意软件关联的工具，证明了LLM在恶意软件检测和漏洞关联方面的有效性，为低成本高效安全分析提供了新途径。

Abstract: Malicious software attacks are having an increasingly significant economic
impact. Commercial malware detection software can be costly, and tools that
attribute malware to the specific software vulnerabilities it exploits are
largely lacking. Understanding the connection between malware and the
vulnerabilities it targets is crucial for analyzing past threats and
proactively defending against current ones. In this study, we propose an
approach that leverages large language models (LLMs) to detect binary malware,
specifically within JAR files, and utilizes the capabilities of LLMs combined
with retrieval-augmented generation (RAG) to identify Common Vulnerabilities
and Exposures (CVEs) that malware may exploit. We developed a proof-of-concept
tool called MalCVE, which integrates binary code decompilation, deobfuscation,
LLM-based code summarization, semantic similarity search, and CVE
classification using LLMs. We evaluated MalCVE using a benchmark dataset of
3,839 JAR executables. MalCVE achieved a mean malware detection accuracy of
97%, at a fraction of the cost of commercial solutions. It is also the first
tool to associate CVEs with binary malware, achieving a recall@10 of 65%, which
is comparable to studies that perform similar analyses on source code.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [20] [Adaptive Base Representation Theorem: An Alternative to Binary Number System](https://arxiv.org/abs/2510.15099)
*Ravin Kumar*

Main category: cs.IT

TL;DR: 本文提出了自适应基数表示（ABR）定理和一种新的数字系统，作为二进制系统的结构化替代方案，能够用相同位数唯一表示十进制数，并与现有压缩算法和纠错机制兼容。


<details>
  <summary>Details</summary>
Motivation: 为数字计算机提供二进制系统的结构化替代方案，探索新的数字数据表示方法。

Method: 提出ABR定理和数字系统，通过理论基础和数学公式证明其编码能力，并验证与现有技术的兼容性。

Result: ABR系统能够用相同位数编码与二进制相同的整数范围，且兼容Huffman编码、算术编码和Hamming码等现有技术。

Conclusion: ABR数字系统在信息理论和数字编码中具有实用价值，可能激发数字数据表示和计算设计的新方法。

Abstract: This paper introduces the Adaptive Base Representation (ABR) Theorem and
proposes a novel number system that offers a structured alternative to the
binary number system for digital computers. The ABR number system enables each
decimal number to be represented uniquely and using the same number of bits,
$n$, as the binary encoding. Theoretical foundations and mathematical
formulations demonstrate that ABR can encode the same integer range as binary,
validating its potential as a viable alternative. Additionally, the ABR number
system is compatible with existing data compression algorithms like Huffman
coding and arithmetic coding, as well as error detection and correction
mechanisms such as Hamming codes. We further explore practical applications,
including digital steganography, to illustrate the utility of ABR in
information theory and digital encoding, suggesting that the ABR number system
could inspire new approaches in digital data representation and computational
design.

</details>


### [21] [Outage-Aware Sum Rate Maximization in Movable Antennas-Enabled Systems](https://arxiv.org/abs/2510.15292)
*Guojie Hu,Qingqing Wu,Ming-Min Zhao,Wen Chen,Zhenyu Xiao,Kui Xu,Jiangbo Si*

Main category: cs.IT

TL;DR: 本文研究了基于可移动天线(MAs)的多输入单输出(MISO)系统，在延迟敏感场景下，基站仅依赖统计信道状态信息(CSI)进行数据传输，通过联合优化天线位置和发射波束成形来最大化用户的总中断感知速率。


<details>
  <summary>Details</summary>
Motivation: 在延迟敏感场景中，用户避免定期发送训练信号以减少额外延迟，因此基站只能依赖统计CSI进行数据传输。传统固定位置天线(FPA)系统性能受限，而可移动天线能够通过优化天线位置来提升系统性能。

Method: 采用基于统计CSI的迫零波束成形设计，通过重要引理推导SINR的紧致均值和方差，利用Laguerre级数近似得到SINR的闭式紧致CDF，然后开发投影梯度上升(PGA)方法迭代更新天线位置。

Result: 数值结果表明，所提出的方案相比传统固定位置天线(FPA)和其他竞争基准具有更好的性能表现。

Conclusion: 可移动天线系统在仅依赖统计CSI的延迟敏感场景中能够有效提升系统性能，所提出的优化算法能够成功解决这一高度非凸问题。

Abstract: In this paper, we investigate the movable antennas (MAs)-enabled
multiple-input-single-output (MISO) systems, where the base station (BS)
equipped with multiple MAs serves multiple single-antenna user. The
delay-sensitive scenario is considered, where users refrain from periodically
sending training signals to the BS for channel estimations to avoid additional
latency. As a result, the BS relies solely on the statistical channel state
information (CSI) to transmit data with a fixed rate. Under this setup, we aim
to maximize the outage-aware sum rate of all users, by jointly optimizing
antenna positions and the transmit beamforming at the BS, while satisfying the
given target outage probability requirement at each user. The problem is highly
non-convex, primarily because the exact cumulative distribution function (CDF)
of the received signal-to-interference-plus-noise ratio (SINR) of each user is
difficult to derive. To simplify analysis and without comprising performance,
we adopt the statistical CSI based zero-forcing beamforming design. We then
introduce one important lemma to derive the tight mean and variance of the
SINR. Leveraging these results, we further exploit the Laguerre series
approximation to successfully derive the closedform and tight CDF of the SINR.
Subsequently, the outageaware sum rate expression is presented but still
includes complex structure with respect to antenna positions. Facing this
challenge, the projected gradient ascent (PGA) method is developed to
iteratively update antenna positions until convergence. Numerical results
demonstrate the effectiveness of our proposed schemes compared to conventional
fixed-position antenna (FPA) and other competitive benchmarks.

</details>


### [22] [Subverting Flexible Multiuser Communications via Movable Antenna-Enabled Jammer](https://arxiv.org/abs/2510.15298)
*Guojie Hu,Qingqing Wu,Lipeng Zhu,Kui Xu,Guoxin Li,Jiangbo Si,Jian Ouyang,Tong-Xing Zheng*

Main category: cs.IT

TL;DR: 本文研究利用可移动天线(MA)技术，通过优化天线位置和干扰波束成形来破坏可疑多用户下行通信的安全性，最小化可疑接收器的总速率或最小速率。


<details>
  <summary>Details</summary>
Motivation: 从安全角度出发，利用MA技术提供的额外空间自由度，通过可移动天线合法干扰器(MAJ)来破坏可疑的多用户下行通信系统，提升系统安全性。

Method: 采用交替优化算法，联合优化MAJ的天线位置和干扰波束成形，同时考虑可疑发射器(ST)会自适应调整功率分配的反制行为。针对两种不同的目标函数（总速率最小化和最小速率最大化），分别开发了相应的优化算法。

Result: 数值结果表明，所提出的方案相比传统固定位置天线(FPA)和其他竞争基准具有更好的性能。在双可疑接收器的特殊情况下，还揭示了MAJ天线部署的有价值规律。

Conclusion: 可移动天线技术能够有效提升通信系统的安全性能，通过智能的天线位置调整和干扰策略，可以显著降低可疑通信的效益，为无线通信安全提供了新的技术途径。

Abstract: Movable antenna (MA) is an emerging technology which can reconfigure wireless
channels via adaptive antenna position adjustments at transceivers, thereby
bringing additional spatial degrees of freedom for improving system
performance. In this paper, from a security perspective, we exploit the
MAenabled legitimate jammer (MAJ) to subvert suspicious multiuser downlink
communications consisting of one suspicious transmitter (ST) and multiple
suspicious receivers (SRs). Specifically, our objective is to minimize the
benefit (the sum rate of all SRs or the minimum rate among all SRs) of such
suspicious communications, by jointly optimizing antenna positions and the
jamming beamforming at the MAJ. However, the key challenge lies in that given
the MAJ's actions, the ST can reactively adjust its power allocations to
instead maximize its benefit for mitigating the unfavorable interference. Such
flexible behavior of the ST confuses the optimization design of the MAJ to a
certain extent. Facing this difficulty, corresponding to the above two
different benefits: i) we respectively determine the optimal behavior of the ST
given the MAJ's actions; ii) armed with these, we arrive at two simplified
problems and then develop effective alternating optimization based algorithms
to iteratively solve them. In addition to these, we also focus on the special
case of two SRs, and reveal insightful conclusions about the deployment rule of
antenna positions at the MAJ. Furthermore, we analyze the ideal antenna
deployment scheme at the MAJ for achieving the globally performance lower
bound. Numerical results demonstrate the effectiveness of our proposed schemes
compared to conventional fixed-position antenna (FPA) and other competitive
benchmarks.

</details>


### [23] [New generalizations of circular complex fuzzy sets and Gaussian weighted aggregation operators](https://arxiv.org/abs/2510.15605)
*Yelda Gülfırat,Mehmet Ünver*

Main category: cs.IT

TL;DR: 本文提出了圆形复数q阶正交模糊集(CCq-ROFS)的概念，统一了现有的圆形复数直觉模糊集和复数q阶正交模糊集框架，并基于高斯函数构建了新的聚合算子。


<details>
  <summary>Details</summary>
Motivation: 为了统一现有的圆形复数直觉模糊集和复数q阶正交模糊集框架，并实现更平滑、统计意义明确的模糊不确定性表示。

Method: 将高斯函数框架扩展到CCq-ROFSs，使用高斯三角范数和余范数构建新的高斯基聚合算子，包括高斯加权算术和高斯加权几何聚合算子。

Result: 开发了统一的CCq-ROFS框架，当q=2时为圆形复数毕达哥拉斯模糊集，q=3时为圆形复数费马模糊集，并构建了相应的聚合算子。

Conclusion: 提出的CCq-ROFS框架成功统一了现有模糊集理论，基于高斯函数的聚合算子为模糊建模和决策提供了更一致的集成方法。

Abstract: In this paper, we introduce the concept of the circular complex $q$-rung
orthopair fuzzy set (CC$q$-ROFS) as a novel generalization that unifies the
existing frameworks of circular complex intuitionistic fuzzy sets (CCIFSs) and
complex $q$-rung orthopair fuzzy sets. If $q = 2$, the structure is referred to
as a circular complex Pythagorean fuzzy set, and if $q = 3$, it is called a
circular complex Fermatean fuzzy set. The proposed approach extends the
Gaussian-based framework to the CC$q$-ROFSs, aiming to achieve a smoother and
statistically meaningful representation of uncertainty. Within this setting,
new Gaussian-based aggregation operators for CC$q$-ROFSs are constructed by
employing the Gaussian triangular norm and conorm. Furthermore,
Gaussian-weighted arithmetic and Gaussian-weighted geometric aggregation
operators are formulated to enable consistent integration of membership and
non-membership information for fuzzy modeling and decision-making.

</details>


### [24] [Beyond-Diagonal RIS Under Non-Idealities: Learning-Based Architecture Discovery and Optimization](https://arxiv.org/abs/2510.15701)
*Binggui Zhou,Bruno Clerckx*

Main category: cs.IT

TL;DR: 提出了一种基于学习的两层架构发现框架（LTTADF），用于在非理想BD-RIS中联合发现给定电路复杂度下的最优架构，解决非理想性和电路复杂度对性能的影响问题。


<details>
  <summary>Details</summary>
Motivation: 传统BD-RIS在性能与电路复杂度之间存在权衡，但非理想BD-RIS的架构发现尚未研究，不清楚非理想性和电路复杂度如何共同影响BD-RIS性能，难以在存在非理想性的情况下实现性能-电路复杂度权衡。

Method: 提出LTTADF框架，包含架构生成器和性能优化器，能够在大规模架构空间中有效探索，避免陷入局部最优，实现性能优化的近最优解。

Result: 数值结果为考虑性能-电路复杂度权衡的非理想BD-RIS部署提供了有价值的见解。

Conclusion: LTTADF框架成功解决了非理想BD-RIS架构发现的挑战，实现了在特定电路复杂度下发现最优架构的目标。

Abstract: Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has recently been
introduced to enable advanced control over electromagnetic waves to further
increase the benefits of traditional RIS in enhancing signal quality and
improving spectral and energy efficiency for next-generation wireless networks.
A significant issue in designing and deploying BD-RIS is the tradeoff between
its performance and circuit complexity. Despite some efforts in exploring
optimal architectures with the lowest circuit complexities for ideal BD-RIS,
architecture discovery for non-ideal BD-RIS remains uninvestigated. Therefore,
how non-idealities and circuit complexity jointly affect the performance of
BD-RIS remains unclear, making it difficult to achieve the performance -
circuit complexity tradeoff in the presence of non-idealities. Essentially,
architecture discovery for non-ideal BD-RIS faces challenges from both the
computational complexity of global architecture search and the difficulty in
achieving global optima. To tackle these challenges, we propose a
learning-based two-tier architecture discovery framework (LTTADF) consisting of
an architecture generator and a performance optimizer to jointly discover
optimal architectures of non-ideal BD-RIS given specific circuit complexities,
which can effectively explore over a large architecture space while avoiding
getting trapped in poor local optima and thus achieving near-optimal solutions
for the performance optimization. Numerical results provide valuable insights
for deploying non-ideal BD-RIS considering the performance - circuit complexity
tradeoff.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 本文研究了在去中心化多智能体强化学习中，如何通过提供高层符号知识来解决隐私约束、通信限制和性能问题等独特挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多问题需要多个智能体协作完成共同目标，但去中心化多智能体强化学习面临本地策略兼容性约束、隐私保护、通信限制等挑战。

Method: 扩展了用于检查本地策略与团队任务兼容性的形式化工具，使具有理论保证的去中心化训练在更多场景中可用，并利用环境事件时间演化的符号知识来加速学习过程。

Result: 实证研究表明，关于环境事件时间演化的符号知识能够显著加速去中心化多智能体强化学习的学习过程。

Conclusion: 符号知识的引入有效解决了去中心化多智能体强化学习中的独特挑战，提高了训练效率和策略兼容性保证。

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [26] [Extending Load Forecasting from Zonal Aggregates to Individual Nodes for Transmission System Operators](https://arxiv.org/abs/2510.14983)
*Oskar Triebe,Fletcher Passow,Simon Wittner,Leonie Wagner,Julio Arend,Tao Sun,Chad Zanocco,Marek Miltner,Arezou Ghesmati,Chen-Hao Tsai,Christoph Bergmeir,Ram Rajagopal*

Main category: cs.LG

TL;DR: 该论文设计了一个多级电力负荷预测系统，用于提高输电系统运营商在可持续能源发展背景下的节点负荷预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 可持续能源发展增加了电力负荷的不确定性，输电系统运营商需要更高空间分辨率的负荷预测，从区域聚合预测扩展到单个节点预测，但节点负荷预测准确性较低且管理复杂。

Method: 开发了一个多级预测系统，包括可解释且可扩展的预测模型、处理节点负荷异质性和波动性的解决方案，以及完全并行化的单模型预测工作流程。

Result: 结果显示区域预测的准确性和可解释性得到改善，节点预测有显著改进，运营商能够以前所未有的信心和准确性调整预测，并精确诊断原本不透明的错误。

Conclusion: 该多级预测系统使输电系统运营商能够逐步将区域运营扩展到包括节点预测，提高了负荷预测的可靠性和实用性。

Abstract: The reliability of local power grid infrastructure is challenged by
sustainable energy developments increasing electric load uncertainty.
Transmission System Operators (TSOs) need load forecasts of higher spatial
resolution, extending current forecasting operations from zonal aggregates to
individual nodes. However, nodal loads are less accurate to forecast and
require a large number of individual forecasts, which are hard to manage for
the human experts assessing risks in the control room's daily operations
(operator). In collaboration with a TSO, we design a multi-level system that
meets the needs of operators for hourly day-ahead load forecasting. Utilizing a
uniquely extensive dataset of zonal and nodal net loads, we experimentally
evaluate our system components. First, we develop an interpretable and scalable
forecasting model that allows for TSOs to gradually extend zonal operations to
include nodal forecasts. Second, we evaluate solutions to address the
heterogeneity and volatility of nodal load, subject to a trade-off. Third, our
system is manageable with a fully parallelized single-model forecasting
workflow. Our results show accuracy and interpretability improvements for zonal
forecasts, and substantial improvements for nodal forecasts. In practice, our
multi-level forecasting system allows operators to adjust forecasts with
unprecedented confidence and accuracy, and to diagnose otherwise opaque errors
precisely.

</details>


### [27] [TangledFeatures: Robust Feature Selection in Highly Correlated Spaces](https://arxiv.org/abs/2510.15005)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: 提出了TangledFeatures框架，用于在相关特征空间中进行特征选择，通过识别纠缠预测因子组中的代表性特征来减少冗余，同时保留解释能力。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法主要关注预测准确性，但在存在相关预测因子的情况下性能会下降，需要一种能够处理相关特征空间的方法。

Method: TangledFeatures框架识别纠缠预测因子组中的代表性特征，减少冗余同时保留解释能力，生成的特征子集可直接用于下游模型。

Result: 在丙氨酸二肽数据集上应用该框架预测主链扭转角，所选特征对应结构上有意义的原子内距离，能够解释这些角度的变化。

Conclusion: TangledFeatures提供了比传统选择技术更可解释和稳定的分析基础，在相关特征空间中表现出有效性。

Abstract: Feature selection is a fundamental step in model development, shaping both
predictive performance and interpretability. Yet, most widely used methods
focus on predictive accuracy, and their performance degrades in the presence of
correlated predictors. To address this gap, we introduce TangledFeatures, a
framework for feature selection in correlated feature spaces. It identifies
representative features from groups of entangled predictors, reducing
redundancy while retaining explanatory power. The resulting feature subset can
be directly applied in downstream models, offering a more interpretable and
stable basis for analysis compared to traditional selection techniques. We
demonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying
it to the prediction of backbone torsional angles and show that the selected
features correspond to structurally meaningful intra-atomic distances that
explain variation in these angles.

</details>


### [28] [ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm](https://arxiv.org/abs/2510.15006)
*Rijul Tandon,Peter Vamplew,Cameron Foale*

Main category: cs.LG

TL;DR: 本文提出了一种改进的C51分布强化学习算法（ES-C51），用Expected Sarsa更新替代贪婪Q学习更新，通过softmax计算结合所有可能动作的信息，解决了当动作具有相似期望奖励但不同分布时的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于价值的强化学习算法只估计每个动作的期望奖励，而分布强化学习估计整个奖励概率分布。C51是离散动作空间的流行DRL算法，但使用贪婪Bellman更新可能导致在动作具有相似期望奖励但不同分布时学习不稳定。

Method: 提出ES-C51算法，将C51的贪婪Q学习更新替换为Expected Sarsa更新，使用softmax计算结合所有可能动作的信息，而非依赖单一最佳动作。同时将标准C51的探索策略从ε-greedy改为softmax作为对比基准（QL-C51）。

Result: 在Gym经典控制环境和Atari-10游戏上的评估结果表明，ES-C51在多个环境中优于QL-C51，能够学习到更高性能的策略。

Conclusion: 使用Expected Sarsa更新的ES-C51算法能够有效解决分布强化学习中的不稳定性问题，在动作具有相似期望奖励时表现更优，证明了该方法在分布强化学习中的有效性。

Abstract: In most value-based reinforcement learning (RL) algorithms, the agent
estimates only the expected reward for each action and selects the action with
the highest reward. In contrast, Distributional Reinforcement Learning (DRL)
estimates the entire probability distribution of possible rewards, providing
richer information about uncertainty and variability. C51 is a popular DRL
algorithm for discrete action spaces. It uses a Q-learning approach, where the
distribution is learned using a greedy Bellman update. However, this can cause
problems if multiple actions at a state have similar expected reward but with
different distributions, as the algorithm may not learn a stable distribution.
This study presents a modified version of C51 (ES-C51) that replaces the greedy
Q-learning update with an Expected Sarsa update, which uses a softmax
calculation to combine information from all possible actions at a state rather
than relying on a single best action. This reduces instability when actions
have similar expected rewards and allows the agent to learn higher-performing
policies. This approach is evaluated on classic control environments from Gym,
and Atari-10 games. For a fair comparison, we modify the standard C51's
exploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-
Learning based C51). The results demonstrate that ES-C51 outperforms QL-C51
across many environments.

</details>


### [29] [Hybrid Autoencoder-Based Framework for Early Fault Detection in Wind Turbines](https://arxiv.org/abs/2510.15010)
*Rekha R Nair,Tina Babu,Alavikunhu Panthakkan,Balamurugan Balusamy,Wathiq Mansoor*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成深度学习的无监督异常检测框架，用于风力涡轮机的早期故障检测，结合了变分自编码器、LSTM自编码器和Transformer架构，在真实数据集上实现了0.947的AUC-ROC和提前48小时的故障检测。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机可靠性对可再生能源领域至关重要，早期故障检测能显著减少停机时间和维护成本。

Method: 集成变分自编码器、LSTM自编码器和Transformer架构，通过特征工程提取时域、统计和频域指标，采用集成评分和自适应阈值进行无监督异常检测。

Result: 在包含89年真实涡轮机数据的CARE数据集上评估，获得0.947的AUC-ROC，能提前48小时检测到故障。

Conclusion: 该方法通过实现预测性维护，减少涡轮机故障，提高大规模风能部署的运营效率，具有重要的社会价值。

Abstract: Wind turbine reliability is critical to the growing renewable energy sector,
where early fault detection significantly reduces downtime and maintenance
costs. This paper introduces a novel ensemble-based deep learning framework for
unsupervised anomaly detection in wind turbines. The method integrates
Variational Autoencoders (VAE), LSTM Autoencoders, and Transformer
architectures, each capturing different temporal and contextual patterns from
high-dimensional SCADA data. A unique feature engineering pipeline extracts
temporal, statistical, and frequency-domain indicators, which are then
processed by the deep models. Ensemble scoring combines model predictions,
followed by adaptive thresholding to detect operational anomalies without
requiring labeled fault data. Evaluated on the CARE dataset containing 89 years
of real-world turbine data across three wind farms, the proposed method
achieves an AUC-ROC of 0.947 and early fault detection up to 48 hours prior to
failure. This approach offers significant societal value by enabling predictive
maintenance, reducing turbine failures, and enhancing operational efficiency in
large-scale wind energy deployments.

</details>


### [30] [AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.15038)
*Lingkai Kong,Molei Tao,Yang Liu,Bryan Wang,Jinmiao Fu,Chien-Chih Wang,Huidong Liu*

Main category: cs.LG

TL;DR: AlignFlow是一种基于半离散最优传输的流生成模型训练方法，通过将噪声空间划分为Laguerre单元并映射到对应数据点，提高了训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于最优传输的流生成模型方法使用小批量采样来估计传输计划，这限制了其在大规模高维数据集上的可扩展性。

Method: 利用半离散最优传输建立噪声分布与数据点之间的显式最优对齐，通过划分噪声空间为Laguerre单元，每个单元映射到对应数据点。

Result: AlignFlow能够扩展到大型数据集和模型架构，计算开销可忽略，实验表明它能提升多种最先进流生成模型算法的性能。

Conclusion: AlignFlow是一种可即插即用的有效方法，通过半离散最优传输显著改善了流生成模型的训练效果和可扩展性。

Abstract: Flow-based Generative Models (FGMs) effectively transform noise into complex
data distributions. Incorporating Optimal Transport (OT) to couple noise and
data during FGM training has been shown to improve the straightness of flow
trajectories, enabling more effective inference. However, existing OT-based
methods estimate the OT plan using (mini-)batches of sampled noise and data
points, which limits their scalability to large and high-dimensional datasets
in FGMs. This paper introduces AlignFlow, a novel approach that leverages
Semi-Discrete Optimal Transport (SDOT) to enhance the training of FGMs by
establishing an explicit, optimal alignment between noise distribution and data
points with guaranteed convergence. SDOT computes a transport map by
partitioning the noise space into Laguerre cells, each mapped to a
corresponding data point. During FGM training, i.i.d. noise samples are paired
with data points via the SDOT map. AlignFlow scales well to large datasets and
model architectures with negligible computational overhead. Experimental
results show that AlignFlow improves the performance of a wide range of
state-of-the-art FGM algorithms and can be integrated as a plug-and-play
component. Code is available at: https://github.com/konglk1203/AlignFlow.

</details>


### [31] [Internalizing World Models via Self-Play Finetuning for Agentic RL](https://arxiv.org/abs/2510.15047)
*Shiqi Chen,Tongyao Zhu,Zian Wang,Jinghan Zhang,Kangrui Wang,Siyang Gao,Teng Xiao,Yee Whye Teh,Junxian He,Manling Li*

Main category: cs.LG

TL;DR: SPA框架通过自监督微调学习世界模型，然后用于策略优化，显著提升LLM智能体在分布外环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分布外场景中表现不佳，难以将内部知识与环境动态对齐，导致探索脆弱和泛化能力有限。

Method: 将世界模型分解为状态表示和转移建模，通过自监督微调阶段冷启动策略，学习与环境交互的世界模型，然后在策略优化前模拟未来状态。

Result: 在Sokoban、FrozenLake和Sudoku等环境中显著提升性能，如Sokoban成功率从25.6%提升到59.8%，FrozenLake得分从22.1%提升到70.9%。

Conclusion: 为LLM智能体配备内部世界模型能更好地对齐推理与环境动态，改善决策能力，SPA框架有效提升了强化学习训练性能。

Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution
(OOD) scenarios. Real-world environments are complex and dynamic, governed by
task-specific rules and stochasticity, which makes it difficult for LLMs to
ground their internal knowledge in those dynamics. Under such OOD conditions,
vanilla RL training often fails to scale; we observe Pass@k--the probability
that at least one of (k) sampled trajectories succeeds--drops markedly across
training steps, indicating brittle exploration and limited generalization.
Inspired by model-based reinforcement learning, we hypothesize that equipping
LLM agents with an internal world model can better align reasoning with
environmental dynamics and improve decision-making. We show how to encode this
world model by decomposing it into two components: state representation and
transition modeling. Building on this, we introduce SPA, a simple reinforcement
learning framework that cold-starts the policy via a Self-Play supervised
finetuning (SFT) stage to learn the world model by interacting with the
environment, then uses it to simulate future states prior to policy
optimization. This simple initialization outperforms the online world-modeling
baseline and greatly boosts the RL-based agent training performance.
Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku
show that our approach significantly improves performance. For example, SPA
boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake
score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

</details>


### [32] [Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions](https://arxiv.org/abs/2510.15056)
*Ziqing Lu,Babak Hassibi,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 论文提出了多层级可配置时变马尔可夫决策过程（MCTVMDP），其中智能体不仅可以通过原始动作优化策略，还可以通过上层模型改变动作主动修改环境动态模型，从而联合提升长期奖励。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习假设环境是给定或固定的，而本文考虑智能体不仅限于被动适应，还可以通过模型改变动作主动修改环境动态模型，这有可能增加智能体的奖励。

Method: 引入多层级可配置时变马尔可夫决策过程（MCTVMDP），下层MDP具有可通过上层模型改变动作配置的非平稳转移函数。智能体目标包括优化上层MDP中的配置策略和下层MDP中的原始动作策略。

Result: 提出了一个能够同时处理环境配置和动作策略优化的统一框架，使智能体能够主动改变环境动态来最大化长期奖励。

Conclusion: MCTVMDP框架扩展了传统强化学习，使智能体能够通过主动修改环境模型来提升性能，为处理可配置环境提供了理论基础。

Abstract: Reinforcement learning usually assumes a given or sometimes even fixed
environment in which an agent seeks an optimal policy to maximize its long-term
discounted reward. In contrast, we consider agents that are not limited to
passive adaptations: they instead have model-changing actions that actively
modify the RL model of world dynamics itself. Reconfiguring the underlying
transition processes can potentially increase the agents' rewards. Motivated by
this setting, we introduce the multi-layer configurable time-varying Markov
decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a
non-stationary transition function that is configurable through upper-level
model-changing actions. The agent's objective consists of two parts: Optimize
the configuration policies in the upper-level MDP and optimize the primitive
action policies in the lower-level MDP to jointly improve its expected
long-term reward.

</details>


### [33] [Physics-informed data-driven machine health monitoring for two-photon lithography](https://arxiv.org/abs/2510.15075)
*Sixian Jia,Zhiqiao Dong,Chenhui Shao*

Main category: cs.LG

TL;DR: 本文提出了三种基于物理信息数据驱动预测模型和统计方法的两光子光刻系统健康监测方法，能够准确及时地监控机器健康状态，实现基于状态的维护。


<details>
  <summary>Details</summary>
Motivation: 当前两光子光刻系统的维护实践主要依赖经验而非基于机器健康状态的监控，导致维护时机不当，要么造成机器停机和制造质量差，要么进行不必要的维护导致效率低下。

Method: 通过集成物理信息数据驱动预测模型与统计方法，提出了三种能够处理不同泛化水平复杂场景的健康监测方法。

Result: 在包含六个工艺参数组合和六个结构尺寸的全面实验数据集上评估，所有测试场景中方法都达到了高精度，表现出优异的有效性、鲁棒性和泛化性。

Conclusion: 这些结果代表了两光子光刻系统向基于状态维护迈出的重要一步。

Abstract: Two-photon lithography (TPL) is a sophisticated additive manufacturing
technology for creating three-dimensional (3D) micro- and nano-structures.
Maintaining the health of TPL systems is critical for ensuring consistent
fabrication quality. Current maintenance practices often rely on experience
rather than informed monitoring of machine health, resulting in either untimely
maintenance that causes machine downtime and poor-quality fabrication, or
unnecessary maintenance that leads to inefficiencies and avoidable downtime. To
address this gap, this paper presents three methods for accurate and timely
monitoring of TPL machine health. Through integrating physics-informed
data-driven predictive models for structure dimensions with statistical
approaches, the proposed methods are able to handle increasingly complex
scenarios featuring different levels of generalizability. A comprehensive
experimental dataset that encompasses six process parameter combinations and
six structure dimensions under two machine health conditions was collected to
evaluate the effectiveness of the proposed approaches. Across all test
scenarios, the approaches are shown to achieve high accuracies, demonstrating
excellent effectiveness, robustness, and generalizability. These results
represent a significant step toward condition-based maintenance for TPL
systems.

</details>


### [34] [Online Correlation Clustering: Simultaneously Optimizing All $\ell_p$-norms](https://arxiv.org/abs/2510.15076)
*Sami Davies,Benjamin Moseley,Heather Newman*

Main category: cs.LG

TL;DR: 该论文提出了一个在线聚类算法，在AOS（带样本在线）模型下，能够同时近似所有ℓp范数目标，将离线的"全范数"保证成功移植到在线世界。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于ℓp范数目标在相关聚类中的基本权衡：最小化总分歧（ℓ1范数）与确保对个体节点的公平性（ℓ∞范数）。离线设置中可以同时近似所有ℓp范数，但能否在在线设置中实现这一强大保证是一个开放问题。

Method: 提出了一种在AOS（带样本在线）模型中的单一算法，给定输入的一小部分常数比例作为样本，生成一个聚类。该算法能够同时处理所有ℓp范数目标。

Result: 算法在AOS模型中产生一个聚类，该聚类：以高概率对所有ℓp范数同时具有O(log⁴n)竞争比；以高概率对ℓ∞范数具有O(logn)竞争比；在期望中对ℓ1范数具有O(1)竞争比。

Conclusion: 该工作成功将离线的"全范数"保证移植到在线世界，并通过硬度结果证明了在标准随机顺序在线模型中这些目标之间存在基本分离，凸显了超越最坏情况模型的必要性。

Abstract: The $\ell_p$-norm objectives for correlation clustering present a fundamental
trade-off between minimizing total disagreements (the $\ell_1$-norm) and
ensuring fairness to individual nodes (the $\ell_\infty$-norm). Surprisingly,
in the offline setting it is possible to simultaneously approximate all
$\ell_p$-norms with a single clustering. Can this powerful guarantee be
achieved in an online setting? This paper provides the first affirmative
answer. We present a single algorithm for the online-with-a-sample (AOS) model
that, given a small constant fraction of the input as a sample, produces one
clustering that is simultaneously $O(\log^4 n)$-competitive for all
$\ell_p$-norms with high probability, $O(\log n)$-competitive for the
$\ell_\infty$-norm with high probability, and $O(1)$-competitive for the
$\ell_1$-norm in expectation. This work successfully translates the offline
"all-norms" guarantee to the online world.
  Our setting is motivated by a new hardness result that demonstrates a
fundamental separation between these objectives in the standard random-order
(RO) online model. Namely, while the $\ell_1$-norm is trivially
$O(1)$-approximable in the RO model, we prove that any algorithm in the RO
model for the fairness-promoting $\ell_\infty$-norm must have a competitive
ratio of at least $\Omega(n^{1/3})$. This highlights the necessity of a
different beyond-worst-case model. We complement our algorithm with lower
bounds, showing our competitive ratios for the $\ell_1$- and $\ell_\infty$-
norms are nearly tight in the AOS model.

</details>


### [35] [Operator Flow Matching for Timeseries Forecasting](https://arxiv.org/abs/2510.15101)
*Yolanne Yi Ran Lee,Kyriakos Flouris*

Main category: cs.LG

TL;DR: TempO是一个基于流匹配的生成模型，通过稀疏条件化和通道折叠高效处理3D时空场，在三个基准PDE数据集上优于现有方法，具有参数和内存效率高的优势。


<details>
  <summary>Details</summary>
Motivation: 预测高维PDE控制动力学是生成建模的核心挑战，现有自回归和扩散方法存在累积误差和离散化伪影问题，限制了长期物理一致性预测。

Method: 提出TempO模型，利用稀疏条件化和通道折叠，使用时间条件傅里叶层高效处理3D时空场，捕捉多尺度模式。

Result: 在三个基准PDE数据集上优于最先进的基线方法，谱分析显示在多尺度动力学恢复方面表现优越。

Conclusion: 流匹配为PDE动力学预测提供了高效、确定性采样的自然替代方案，TempO展示了在参数和内存效率方面的优势。

Abstract: Forecasting high-dimensional, PDE-governed dynamics remains a core challenge
for generative modeling. Existing autoregressive and diffusion-based approaches
often suffer cumulative errors and discretisation artifacts that limit long,
physically consistent forecasts. Flow matching offers a natural alternative,
enabling efficient, deterministic sampling. We prove an upper bound on FNO
approximation error and propose TempO, a latent flow matching model leveraging
sparse conditioning with channel folding to efficiently process 3D
spatiotemporal fields using time-conditioned Fourier layers to capture
multi-scale modes with high fidelity. TempO outperforms state-of-the-art
baselines across three benchmark PDE datasets, and spectral analysis further
demonstrates superior recovery of multi-scale dynamics, while efficiency
studies highlight its parameter- and memory-light design compared to
attention-based or convolutional regressors.

</details>


### [36] [DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning](https://arxiv.org/abs/2510.15110)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: DLER通过改进强化学习优化方法，使用简单的截断长度惩罚，在保持高准确率的同时显著缩短输出长度，解决了语言模型推理过程中输出过长的问题。


<details>
  <summary>Details</summary>
Motivation: 当前推理语言模型如OpenAI-o1、DeepSeek-R1等虽然性能强大，但经常生成不必要的冗长输出，最大化每个token的智能（准确率相对于响应长度）仍然是一个未解决的问题。

Method: 提出DLER训练方法，结合批处理奖励归一化、更高裁剪、动态采样和简单截断长度惩罚，解决了强化学习中的三个关键挑战：优势估计偏差大、熵崩溃和稀疏奖励信号。

Result: DLER在保持最先进准确率的同时，将输出长度减少了70%以上，相比DeepSeek-R1-7B，DLER-7B生成多个简洁并行响应时准确率提高28%且延迟更低。

Conclusion: DLER通过改进强化学习优化实现了更好的准确率-效率权衡，并提出了难度感知DLER和更新选择性合并方法，在数据稀缺场景下也能保持基线准确率。

Abstract: Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.

</details>


### [37] [Navigating the consequences of mechanical ventilation in clinical intensive care settings through an evolutionary game-theoretic framework](https://arxiv.org/abs/2510.15127)
*David J. Albers,Tell D. Bennett,Jana de Wiljes,Bradford J. Smith,Peter D. Sottile,J. N. Stroh*

Main category: cs.LG

TL;DR: 该研究开发了一个框架来分析机械通气策略对重症监护患者结果的影响，使用进化博弈论分析呼吸行为，为机械通气优化和个性化提供基础。


<details>
  <summary>Details</summary>
Motivation: 需要理解机械通气和辅助护理决策对患者结果的影响，通过分析现有临床数据来改进重症监护呼吸管理。

Method: 采用进化博弈论分析患者-呼吸机-护理系统的联合系统数据，生成定量前体用于概率和随机分析。

Result: 在合成数据上验证了基于进化博弈论的方法，揭示了数据生成过程中的复杂性，为机械通气决策的状态转换模型奠定了基础。

Conclusion: 该研究为机械通气优化和个性化迈出了重要一步，展示了进化博弈论在分析复杂医疗系统数据中的潜力。

Abstract: Identifying the effects of mechanical ventilation strategies and protocols in
critical care requires analyzing data from heterogeneous patient-ventilator
systems within the context of the clinical decision-making environment. This
research develops a framework to help understand the consequences of mechanical
ventilation (MV) and adjunct care decisions on patient outcome from
observations of critical care patients receiving MV. Developing an
understanding of and improving critical care respiratory management requires
the analysis of existing secondary-use clinical data to generate hypotheses
about advantageous variations and adaptations of current care. This work
introduces a perspective of the joint patient-ventilator-care systems
(so-called J6) to develop a scalable method for analyzing data and trajectories
of these complex systems. To that end, breath behaviors are analyzed using
evolutionary game theory (EGT), which generates the necessary quantitative
precursors for deeper analysis through probabilistic and stochastic machinery
such as reinforcement learning. This result is one step along the pathway
toward MV optimization and personalization. The EGT-based process is
analytically validated on synthetic data to reveal potential caveats before
proceeding to real-world ICU data applications that expose complexities of the
data-generating process J6. The discussion includes potential developments
toward a state transition model for the simulating effects of MV decision using
empirical and game-theoretic elements.

</details>


### [38] [A Simple Method for PMF Estimation on Large Supports](https://arxiv.org/abs/2510.15132)
*Alex Shtoff*

Main category: cs.LG

TL;DR: 提出一种基于图拉普拉斯算子的非参数概率质量函数估计方法，通过数据依赖的低通滤波处理多模态和重尾分布，计算高效且无需过多调参。


<details>
  <summary>Details</summary>
Motivation: 解决在大离散支撑集上估计多模态和重尾概率质量函数的问题，传统方法在处理这类分布时存在困难。

Method: 将经验PMF视为线图上的信号，构建对称三对角算子（路径图拉普拉斯加上经验PMF构建的对角矩阵），计算最小特征值对应的特征向量，将经验PMF投影到低维子空间，最后进行裁剪和重归一化处理。

Result: 在合成和真实重尾数据上，该方法能保持粗粒度结构同时抑制采样噪声，在目标场景下优于logspline和高斯KDE基线方法。

Conclusion: 该方法实现简洁、计算可靠、运行速度快，适合自动化流程和大规模探索性分析，但存在已知的失败模式（如突然的不连续性）。

Abstract: We study nonparametric estimation of a probability mass function (PMF) on a
large discrete support, where the PMF is multi-modal and heavy-tailed. The core
idea is to treat the empirical PMF as a signal on a line graph and apply a
data-dependent low-pass filter. Concretely, we form a symmetric tri-diagonal
operator, the path graph Laplacian perturbed with a diagonal matrix built from
the empirical PMF, then compute the eigenvectors, corresponding to the smallest
feq eigenvalues. Projecting the empirical PMF onto this low dimensional
subspace produces a smooth, multi-modal estimate that preserves coarse
structure while suppressing noise. A light post-processing step of clipping and
re-normalizing yields a valid PMF.
  Because we compute the eigenpairs of a symmetric tridiagonal matrix, the
computation is reliable and runs time and memory proportional to the support
times the dimension of the desired low-dimensional supspace. We also provide a
practical, data-driven rule for selecting the dimension based on an
orthogonal-series risk estimate, so the method "just works" with minimal
tuning. On synthetic and real heavy-tailed examples, the approach preserves
coarse structure while suppressing sampling noise, compares favorably to
logspline and Gaussian-KDE baselines in the intended regimes. However, it has
known failure modes (e.g., abrupt discontinuities). The method is short to
implement, robust across sample sizes, and suitable for automated pipelines and
exploratory analysis at scale because of its reliability and speed.

</details>


### [39] [Predicting the Unpredictable: Reproducible BiLSTM Forecasting of Incident Counts in the Global Terrorism Database (GTD)](https://arxiv.org/abs/2510.15136)
*Oluwasegun Adegoke*

Main category: cs.LG

TL;DR: 使用双向LSTM模型对全球恐怖主义数据库中的周度恐怖事件数量进行短期预测，相比传统方法和LSTM-Attention基线有显著改进。


<details>
  <summary>Details</summary>
Motivation: 研究短期恐怖事件预测，为全球恐怖主义数据库提供可重复、基准超越的预测参考框架。

Method: 构建可重复预测流程，使用双向LSTM模型，并与季节性朴素、线性/ARIMA模型和LSTM-Attention基线进行比较，进行时间记忆、训练历史长度、空间粒度等多方面消融实验。

Result: 双向LSTM在测试集上获得RMSE 6.38，优于LSTM-Attention（9.19，提升30.6%）和线性滞后回归基线（RMSE提升35.4%），在MAE和MAPE指标上也有并行改进。

Conclusion: 长历史数据训练模型泛化能力最佳，适中的回看窗口（20-30周）提供强上下文，双向编码对捕捉窗口内的累积和后续模式至关重要，短期结构特征贡献最大。

Abstract: We study short-horizon forecasting of weekly terrorism incident counts using
the Global Terrorism Database (GTD, 1970--2016). We build a reproducible
pipeline with fixed time-based splits and evaluate a Bidirectional LSTM
(BiLSTM) against strong classical anchors (seasonal-naive, linear/ARIMA) and a
deep LSTM-Attention baseline. On the held-out test set, the BiLSTM attains RMSE
6.38, outperforming LSTM-Attention (9.19; +30.6\%) and a linear lag-regression
baseline (+35.4\% RMSE gain), with parallel improvements in MAE and MAPE.
Ablations varying temporal memory, training-history length, spatial grain,
lookback size, and feature groups show that models trained on long historical
data generalize best; a moderate lookback (20--30 weeks) provides strong
context; and bidirectional encoding is critical for capturing both build-up and
aftermath patterns within the window. Feature-group analysis indicates that
short-horizon structure (lagged counts and rolling statistics) contributes
most, with geographic and casualty features adding incremental lift. We release
code, configs, and compact result tables, and provide a data/ethics statement
documenting GTD licensing and research-only use. Overall, the study offers a
transparent, baseline-beating reference for GTD incident forecasting.

</details>


### [40] [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)
*Xin Guo,Zijiu Lyu*

Main category: cs.LG

TL;DR: 本文研究了连续时间强化学习中的策略迁移，证明了在熵正则化的线性二次调节器（LQR）问题中，从一个相关任务获得的策略可以作为新任务的近最优初始化，并提出了具有全局线性和局部超线性收敛的新算法。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂任务上的从头训练效率低下，而迁移学习在大语言模型中已证明有效。本文旨在将迁移学习应用于连续时间强化学习，提高训练效率。

Method: 采用策略迁移方法，在连续时间LQR问题中使用源任务的策略初始化目标任务学习。提出了新的策略学习算法，并建立了与分数扩散模型的联系。

Result: 首次为连续时间强化学习提供了策略迁移的理论证明，证明了源任务最优策略在相关LQR中可作为近最优初始化，同时保持原始算法的收敛速度。新算法实现了全局线性和局部超线性收敛。

Conclusion: 研究填补了连续时间强化学习中迁移学习的理论空白，证明了策略迁移的有效性，并将相关结果从离散时间扩展到连续时间设置。作为副产品，通过LQR连接分析了一类连续时间分数扩散模型的稳定性。

Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.

</details>


### [41] [An Advanced Two-Stage Model with High Sensitivity and Generalizability for Prediction of Hip Fracture Risk Using Multiple Datasets](https://arxiv.org/abs/2510.15179)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 提出一个两阶段模型，整合临床和影像信息来改进老年人群髋部骨折风险预测，相比传统DXA T-score和FRAX工具具有更高敏感性和更少漏诊。


<details>
  <summary>Details</summary>
Motivation: 传统髋部骨折风险评估工具如DXA T-score和FRAX缺乏敏感性，特别是对于无既往骨折史或骨量减少的高风险个体，需要更准确的风险预测方法。

Method: 开发顺序两阶段模型：第一阶段使用临床、人口统计学和功能变量进行筛查，第二阶段整合DXA衍生特征进行精细化预测。基于MrOS、SOF和UK Biobank数据进行验证。

Result: 模型经过内部和外部验证，在不同队列中表现一致且适应性强。相比T-score和FRAX，两阶段框架实现了更高敏感性和更少漏诊病例。

Conclusion: 该两阶段模型提供了一种经济有效且个性化的早期髋部骨折风险评估方法，显著改进了现有工具的预测性能。

Abstract: Hip fractures are a major cause of disability, mortality, and healthcare
burden in older adults, underscoring the need for early risk assessment.
However, commonly used tools such as the DXA T-score and FRAX often lack
sensitivity and miss individuals at high risk, particularly those without prior
fractures or with osteopenia. To address this limitation, we propose a
sequential two-stage model that integrates clinical and imaging information to
improve prediction accuracy. Using data from the Osteoporotic Fractures in Men
Study (MrOS), the Study of Osteoporotic Fractures (SOF), and the UK Biobank,
Stage 1 (Screening) employs clinical, demographic, and functional variables to
estimate baseline risk, while Stage 2 (Imaging) incorporates DXA-derived
features for refinement. The model was rigorously validated through internal
and external testing, showing consistent performance and adaptability across
cohorts. Compared to T-score and FRAX, the two-stage framework achieved higher
sensitivity and reduced missed cases, offering a cost-effective and
personalized approach for early hip fracture risk assessment.
  Keywords: Hip Fracture, Two-Stage Model, Risk Prediction, Sensitivity, DXA,
FRAX

</details>


### [42] [Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection](https://arxiv.org/abs/2510.15202)
*Denis Janiak,Jakub Binkowski,Tomasz Kajdanowicz*

Main category: cs.LG

TL;DR: 该论文研究了表示几何和归一化对基于马氏距离的OOD检测方法性能的影响，提出了径向缩放ℓ2归一化方法，通过控制特征空间的径向几何来显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 虽然马氏距离方法在OOD检测中广泛使用，但表示几何和归一化对其性能的影响尚未被充分理解，这可能限制其下游应用。

Method: 进行了全面的实证研究，包括分析不同图像基础模型、数据集和距离归一化方案；定义了数据表示的理想几何；提出了径向缩放ℓ2归一化方法，引入可调参数直接控制特征空间的径向几何。

Result: 研究表明马氏距离方法并非普遍可靠；光谱和内在维度指标能准确预测模型的OOD性能；径向缩放ℓ2归一化能系统性地收缩或扩展表示，显著改善OOD检测性能。

Conclusion: 通过弥合表示几何、归一化和OOD性能之间的差距，为设计更有效可靠的深度学习模型提供了新见解。

Abstract: Out-of-distribution (OOD) detection is critical for the reliable deployment
of deep learning models. hile Mahalanobis distance methods are widely used, the
impact of representation geometry and normalization on their performance is not
fully understood, which may limit their downstream application. To address this
gap, we conducted a comprehensive empirical study across diverse image
foundation models, datasets, and distance normalization schemes. First, our
analysis shows that Mahalanobis-based methods aren't universally reliable.
Second, we define the ideal geometry for data representations and demonstrate
that spectral and intrinsic-dimensionality metrics can accurately predict a
model's OOD performance. Finally, we analyze how normalization impacts OOD
performance. Building upon these studies, we propose radially scaled $\ell_2$
normalization, a method that generalizes the standard $\ell_2$ normalization
recently applied to Mahalanobis-based OOD detection. Our approach introduces a
tunable parameter to directly control the radial geometry of the feature space,
systematically contracting or expanding representations to significantly
improve OOD detection performance. By bridging the gap between representation
geometry, normalization, and OOD performance, our findings offer new insights
into the design of more effective and reliable deep learning models.

</details>


### [43] [ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning](https://arxiv.org/abs/2510.15211)
*Yongchan Kwon,Shang Zhu,Federico Bianchi,Kaitlyn Zhou,James Zou*

Main category: cs.LG

TL;DR: 本文提出了ReasonIF基准来评估大型推理模型在推理过程中遵循用户指令的能力，发现现有模型在此方面表现不佳，并探索了两种改进方法。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中遵循用户指令对于其可控性、透明性和安全性至关重要，但现有研究主要关注模型主要响应的指令遵循，而忽略了推理过程中的指令遵循问题。

Method: 开发了ReasonIF基准，包含多语言推理、格式控制和长度控制等六类指令提示；评估了多个开源LRM的指令遵循表现；探索了多轮推理和推理指令微调两种改进策略。

Result: 现有LRM在推理指令遵循方面表现严重不足，最高指令遵循得分低于0.25；任务难度增加时指令遵循表现进一步下降；推理指令微调将GPT-OSS-20B的指令遵循得分从0.11提升到0.27。

Conclusion: 大型推理模型在推理过程中遵循用户指令的能力存在显著不足，需要进一步改进；推理指令微调是有效的改进方法，但仍有很大提升空间。

Abstract: The ability of large language models (LLMs) to follow user instructions is
central to their reliability, safety, and usefulness. While prior studies
assess instruction adherence in the model's main responses, we argue that it is
also critical for large reasoning models (LRMs) to follow user instructions
throughout their reasoning process. Reasoning instruction following makes LRMs
more controllable and transparent, while reducing risks of undesirable
shortcuts, hallucinations, or reward hacking within reasoning traces. To
evaluate this dimension, we introduce ReasonIF, a systematic benchmark for
assessing reasoning instruction following. ReasonIF includes six categories of
instruction prompts, spanning multilingual reasoning, formatting and length
control. Across many open-source LRMs including GPT-OSS, Qwen3, and
DeepSeek-R1, we find substantial failures in reasoning instruction adherence:
the highest instruction following score (IFS) remains below 0.25, meaning that
fewer than $25\%$ of reasoning traces comply with the given instructions.
Notably, as task difficulty increases, reasoning instruction following degrades
further. We also explore two strategies to enhance reasoning instruction
fidelity. (1) multi-turn reasoning and (2) Reasoning Instruction Finetuning
(RIF) using synthetic data. RIF improves the IFS of $GPT-OSS-20B$ from 0.11 to
0.27, indicating measurable progress but leaving ample room for improvement.

</details>


### [44] [Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential](https://arxiv.org/abs/2510.15216)
*Xuansheng Wu,Xiaoman Pan,Wenlin Yao,Jianshu Chen*

Main category: cs.LG

TL;DR: 研究发现，不同基础模型在可验证奖励强化学习（RLVR）后的表现差异源于其内部特征对知识正确性（soundness）的区分能力。高潜力模型能够系统地区分严格正确、合理和噪声规则的概率分布，而弱模型则无法区分。


<details>
  <summary>Details</summary>
Motivation: 探究为什么不同预训练模型在RLVR后的推理性能差异巨大，寻找影响这种差异的微观属性。

Method: 将推理形式化为Horn子句链，使用跨层稀疏自编码器从LLM潜在空间提取特征，估计特征间转移概率，并用LLM对规则按语义正确性水平分类。

Result: 发现高潜力模型具有正确性感知能力，其内部概率分布在不同的正确性水平上系统性地变化，而弱模型则无法区分。提出的SAL指标与RLVR后推理性能高度相关（R²=0.87）。

Conclusion: 模型的推理潜力与其预训练时区分正确知识与不正确知识的内在能力密切相关，这为选择和设计更强基础模型提供了基于内部机制的实际指标。

Abstract: Reinforcement learning with verifiable rewards (RLVR) can elicit strong
reasoning in large language models (LLMs), while their performance after RLVR
varies dramatically across different base models. This raises a fundamental
question: what microscopic property of pre-trained models leads to this
variation? To investigate, we formalize reasoning as chains of Horn clauses
("if-then" rules) built from features extracted from the LLM's latent space via
cross-layer sparse autoencoders (SAEs). We estimate the transition
probabilities between its features, and further categorize each rule by its
semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key
discovery is that high-potential models are inherently soundness-aware: their
internal probability distributions systematically shift across rules' soundness
levels, becoming highly distinct for "strict" versus "noisy" rules. In
contrast, weaker models are soundness-agnostic, collapsing to one distribution
regardless of soundness levels. To quantify this, we introduce the
Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon
Divergence to measure the separation between these distributions. We show that
SAL's predictions of post-RLVR reasoning performance follow a precise empirical
law (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)
and scales (0.5B-14B). This reveals that a model's reasoning potential is tied
to its intrinsic, pre-trained ability to distinguish sound knowledge from
unsound ones. These findings underscore the critical role of model pre-training
in shaping reasoning and offer a practical metric grounded in the model's
internal mechanisms for selecting/designing stronger base models.

</details>


### [45] [Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025](https://arxiv.org/abs/2510.15217)
*Emily Alsentzer,Marie-Laure Charpignon,Bill Chen,Niharika D'Souza,Jason Fries,Yixing Jiang,Aparajita Kashyap,Chanwoo Kim,Simon Lee,Aishwarya Mandyam,Ashery Christopher Mbilinyi,Nikita Mehandru,Nitish Nagesh,Brighton Nuwagira,Emma Pierson,Arvind Pillai,Akane Sano,Tanveer Syeda-Mahmood,Shashank Yadav,Elias Adhanom,Muhammad Umar Afza,Amelia Archer,Suhana Bedi,Vasiliki Bikia,Trenton Chang,George H. Chen,Winston Chen,Erica Chiang,Edward Choi,Octavia Ciora,Paz Dozie-Nnamah,Shaza Elsharief,Matthew Engelhard,Ali Eshragh,Jean Feng,Josh Fessel,Scott Fleming,Kei Sen Fong,Thomas Frost,Soham Gadgil,Judy Gichoya,Leeor Hershkovich,Sujeong Im,Bhavya Jain,Vincent Jeanselme,Furong Jia,Qixuan,Jin,Yuxuan Jin,Daniel Kapash,Geetika Kapoor,Behdokht Kiafar,Matthias Kleiner,Stefan Kraft,Annika Kumar,Daeun Kyung,Zhongyuan Liang,Joanna Lin,Qianchu,Liu,Chang Liu,Hongzhou Luan,Chris Lunt,Leopoldo Julían Lechuga López,Matthew B. A. McDermott,Shahriar Noroozizadeh,Connor O'Brien,YongKyung Oh,Mixail Ota,Stephen Pfohl,Meagan Pi,Tanmoy Sarkar Pias,Emma Rocheteau,Avishaan Sethi,Toru Shirakawa,Anita Silver,Neha Simha,Kamile Stankeviciute,Max Sunog,Peter Szolovits,Shengpu Tang,Jialu Tang,Aaron Tierney,John Valdovinos,Byron Wallace,Will Ke Wang,Peter Washington,Jeremy Weiss,Daniel Wolfe,Emily Wong,Hye Sun Yun,Xiaoman Zhang,Xiao Yu Cindy Zhang,Hayoung Jeong,Kaveri A. Thakoor*

Main category: cs.LG

TL;DR: CHIL 2025会议在加州大学伯克利分校举办，包含8个研究圆桌会议，聚焦机器学习与医疗交叉领域的关键议题，如可解释性、公平性、因果性等。


<details>
  <summary>Details</summary>
Motivation: 促进机器学习与医疗领域专家之间的协作对话，探讨该交叉领域面临的挑战和新兴机遇。

Method: 通过由资深和初级主席主持的小组圆桌会议形式，营造开放交流、知识探索和包容参与的环境。

Result: 成功举办了8个圆桌会议，涉及可解释性、公平性、因果性、领域适应、基础模型、小数据学习、多模态方法和可扩展医疗解决方案等主题。

Conclusion: 研究圆桌会议为医疗机器学习社区提供了重要的协作平台，推动了该领域关键问题的深入讨论和行动方向的集体构思。

Abstract: The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),
hosted by the Association for Health Learning and Inference (AHLI), was held in
person on June 25-27, 2025, at the University of California, Berkeley, in
Berkeley, California, USA. As part of this year's program, we hosted Research
Roundtables to catalyze collaborative, small-group dialogue around critical,
timely topics at the intersection of machine learning and healthcare. Each
roundtable was moderated by a team of senior and junior chairs who fostered
open exchange, intellectual curiosity, and inclusive engagement. The sessions
emphasized rigorous discussion of key challenges, exploration of emerging
opportunities, and collective ideation toward actionable directions in the
field. In total, eight roundtables were held by 19 roundtable chairs on topics
of "Explainability, Interpretability, and Transparency," "Uncertainty, Bias,
and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learning
from Small Medical Data," "Multimodal Methods," and "Scalable, Translational
Healthcare Solutions."

</details>


### [46] [Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)](https://arxiv.org/abs/2510.15219)
*Patricia Medina,Rasika Karkare*

Main category: cs.LG

TL;DR: 该研究扩展了先前关于使用乘积系数增强3D LiDAR点云分类的工作，通过结合乘积系数、自编码器表示和KNN分类器，相比PCA基线和早期框架获得了一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索如何进一步改进3D LiDAR点云分类性能，特别是通过结合乘积系数特征与自编码器表示来提升分类准确性。

Method: 使用乘积系数作为测度理论描述符来补充原始空间LiDAR特征，结合自编码器表示和KNN分类器，并逐级添加乘积系数来研究其对分类性能的影响。

Result: 逐级添加乘积系数显示出明确的趋势：更丰富的系数集合系统地提高了类间可分性和整体准确率。结合乘积系数与自编码器的方法在性能上优于PCA基线和早期框架。

Conclusion: 结合分层乘积系数特征与自编码器能够显著提升LiDAR分类性能，乘积系数的丰富程度与分类性能呈正相关关系。

Abstract: This work extends our previous study on enhancing 3D LiDAR point-cloud
classification with product coefficients
\cite{medina2025integratingproductcoefficientsimproved}, measure-theoretic
descriptors that complement the original spatial Lidar features. Here, we show
that combining product coefficients with an autoencoder representation and a
KNN classifier delivers consistent performance gains over both PCA-based
baselines and our earlier framework. We also investigate the effect of adding
product coefficients level by level, revealing a clear trend: richer sets of
coefficients systematically improve class separability and overall accuracy.
The results highlight the value of combining hierarchical product-coefficient
features with autoencoders to push LiDAR classification performance further.

</details>


### [47] [Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent](https://arxiv.org/abs/2510.15222)
*Gabriel Nixon Raj*

Main category: cs.LG

TL;DR: 本文提出了一种在分布漂移下的顺序决策方法——熵正则化信任衰减，通过注入压力感知的指数倾斜来更新信念和进行镜像下降决策。该方法在单纯形上具有Fenchel对偶等价性，并建立了动态遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 研究在分布漂移环境下的顺序决策问题，旨在开发能够适应未知分布变化的鲁棒决策算法。

Method: 提出熵正则化信任衰减方法，在信念更新和镜像下降决策中注入压力感知的指数倾斜。建立了Fenchel对偶等价性，并通过KL散度路径长度分析动态遗憾。

Result: 证明了高概率敏感性边界和$\tilde{O}(\sqrt{T})$的动态遗憾保证，在KL漂移路径长度$S_T = \sum_{t\ge2}\sqrt{{\rm KL}(D_t|D_{t-1})/2}$下有效。信任衰减实现了$O(1)$的每切换遗憾。

Conclusion: 该框架统一了动态遗憾分析、分布鲁棒目标和KL正则化控制，提供了一个单一的压力自适应更新方法，并扩展到二阶更新、强盗反馈、异常值处理等多个应用场景。

Abstract: We study sequential decision-making under distribution drift. We propose
entropy-regularized trust-decay, which injects stress-aware exponential tilting
into both belief updates and mirror-descent decisions. On the simplex, a
Fenchel-dual equivalence shows that belief tilt and decision tilt coincide. We
formalize robustness via fragility (worst-case excess risk in a KL ball),
belief bandwidth (radius sustaining a target excess), and a decision-space
Fragility Index (drift tolerated at $O(\sqrt{T})$ regret). We prove
high-probability sensitivity bounds and establish dynamic-regret guarantees of
$\tilde{O}(\sqrt{T})$ under KL-drift path length $S_T = \sum_{t\ge2}\sqrt{{\rm
KL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch
regret, while stress-free updates incur $\Omega(1)$ tails. A parameter-free
hedge adapts the tilt to unknown drift, whereas persistent over-tilting yields
an $\Omega(\lambda^2 T)$ stationary penalty. We further obtain
calibrated-stress bounds and extensions to second-order updates, bandit
feedback, outliers, stress variation, distributed optimization, and plug-in
KL-drift estimation. The framework unifies dynamic-regret analysis,
distributionally robust objectives, and KL-regularized control within a single
stress-adaptive update.

</details>


### [48] [FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain](https://arxiv.org/abs/2510.15232)
*Tiansheng Hu,Tongyan Hu,Liuyang Bai,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.LG

TL;DR: FinTrust是一个专门用于评估金融领域LLM可信赖性的综合基准，涵盖广泛的实践背景对齐问题，并通过细粒度任务评估多个可信赖性维度。评估发现专有模型在安全性等任务中表现更好，而开源模型在行业公平性方面有优势，但所有模型在信托对齐和披露等法律意识任务上表现不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在解决金融问题方面展现出潜力，但由于金融领域的高风险和重要性，在实际应用中部署LLM仍面临挑战。需要专门的基准来评估LLM在金融应用中的可信赖性。

Method: 开发FinTrust基准，基于实际应用场景设计广泛的对齐问题，为每个可信赖性评估维度创建细粒度任务，并评估11个LLM模型在这些任务上的表现。

Result: 专有模型如o4-mini在安全性等大多数任务中表现更好，开源模型如DeepSeek-V3在行业公平性等特定领域有优势。所有模型在信托对齐和披露等具有挑战性的任务上都表现不佳，显示出法律意识方面的显著差距。

Conclusion: FinTrust可以作为金融领域LLM可信赖性评估的有价值基准，当前LLM在金融法律意识方面仍有待提升，需要进一步改进以满足实际应用需求。

Abstract: Recent LLMs have demonstrated promising ability in solving finance related
problems. However, applying LLMs in real-world finance application remains
challenging due to its high risk and high stakes property. This paper
introduces FinTrust, a comprehensive benchmark specifically designed for
evaluating the trustworthiness of LLMs in finance applications. Our benchmark
focuses on a wide range of alignment issues based on practical context and
features fine-grained tasks for each dimension of trustworthiness evaluation.
We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini
outperforms in most tasks such as safety while open-source models like
DeepSeek-V3 have advantage in specific areas like industry-level fairness. For
challenging task like fiduciary alignment and disclosure, all LLMs fall short,
showing a significant gap in legal awareness. We believe that FinTrust can be a
valuable benchmark for LLMs' trustworthiness evaluation in finance domain.

</details>


### [49] [Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction](https://arxiv.org/abs/2510.15233)
*Amitesh Badkul,Lei Xie*

Main category: cs.LG

TL;DR: 本文提出了一种新的不确定性量化方法TESSERA，用于蛋白质-配体亲和力预测，在分布内和分布外数据上都能提供可靠的不确定性覆盖保证和自适应预测区间。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习社区缺乏可靠、信息丰富且个体化的不确定性量化方法，这阻碍了AI/ML在风险敏感领域的有效应用。特别是在药物发现中的蛋白质-配体亲和力预测任务中，存在检测噪声异质性、化学空间不平衡和分布偏移等挑战。

Method: TESSERA结合了专家混合模型的多样性和共形校准方法，提供每个样本的不确定性量化，具有可靠的覆盖保证和自适应预测区间宽度。

Result: 在蛋白质-配体结合亲和力预测任务中，TESSERA在独立同分布和基于支架的分布外分割上都达到了接近标称的覆盖率，并在覆盖率-宽度权衡指标上表现最佳，同时保持了竞争力的适应性。

Conclusion: TESSERA通过统一专家混合模型的多样性与共形校准，提供了值得信赖、紧凑且自适应的不确定性量化，适用于药物发现管道中的选择性预测和下游决策制定。

Abstract: Reliable, informative, and individual uncertainty quantification (UQ) remains
missing in current ML community. This hinders the effective application of
AI/ML to risk-sensitive domains. Most methods either fail to provide coverage
on new data, inflate intervals so broadly that they are not actionable, or
assign uncertainties that do not track actual error, especially under a
distribution shift. In high-stakes drug discovery, protein-ligand affinity
(PLI) prediction is especially challenging as assay noise is heterogeneous,
chemical space is imbalanced and large, and practical evaluations routinely
involve distribution shift. In this work, we introduce a novel uncertainty
quantification method, Trustworthy Expert Split-conformal with Scaled
Estimation for Efficient Reliable Adaptive intervals (TESSERA), that provides
per-sample uncertainty with reliable coverage guarantee, informative and
adaptive prediction interval widths that track the absolute error. We evaluate
on protein-ligand binding affinity prediction under both independent and
identically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD)
splits, comparing against strong UQ baselines. TESSERA attains near-nominal
coverage and the best coverage-width trade-off as measured by the
Coverage-Width Criterion (CWC), while maintaining competitive adaptivity
(lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage
(SSC) further confirms that intervals are right-sized, indicating width
increases when data are scarce or noisy, and remain tight when predictions are
reliable. By unifying Mixture of Expert (MoE) diversity with conformal
calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties
that are well-suited to selective prediction and downstream decision-making in
the drug-discovery pipeline and other applications.

</details>


### [50] [Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories](https://arxiv.org/abs/2510.15254)
*Dingya Feng,Dingyuan Xue*

Main category: cs.LG

TL;DR: 本研究提出基于Transformer的框架，利用候鸟迁徙轨迹预测终点疾病风险，整合GPS追踪、疫情记录和地理空间数据，在测试集上取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 准确预测禽类疾病暴发对野生动物保护和公共卫生至关重要，需要开发能够捕捉候鸟迁徙时空依赖性的预测模型。

Method: 整合多源数据集（Movebank GPS追踪数据、WOAH疫情记录、GADM和Natural Earth地理空间数据），使用H3分层地理空间编码处理原始坐标，基于Transformer架构学习鸟类移动序列的时空依赖性。

Result: 在测试集上表现出色：准确率0.9821，ROC曲线下面积0.9803，平均精度0.9299，F1分数0.8836。

Conclusion: Transformer架构在禽类疾病监测早期预警系统中具有巨大潜力，能够支持及时的干预和预防策略。

Abstract: Accurate forecasting of avian disease outbreaks is critical for wildlife
conservation and public health. This study presents a Transformer-based
framework for predicting the disease risk at the terminal locations of
migratory bird trajectories. We integrate multi-source datasets, including GPS
tracking data from Movebank, outbreak records from the World Organisation for
Animal Health (WOAH), and geospatial context from GADM and Natural Earth. The
raw coordinates are processed using H3 hierarchical geospatial encoding to
capture spatial patterns. The model learns spatiotemporal dependencies from
bird movement sequences to estimate endpoint disease risk. Evaluation on a
held-out test set demonstrates strong predictive performance, achieving an
accuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision
(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These
results highlight the potential of Transformer architectures to support
early-warning systems for avian disease surveillance, enabling timely
intervention and prevention strategies.

</details>


### [51] [DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models](https://arxiv.org/abs/2510.15260)
*Yangyang Li*

Main category: cs.LG

TL;DR: DRO-InstructZero通过将零样本提示优化构建为鲁棒贝叶斯优化，使用f-散度球定义评估分布周围的模糊集，并通过鲁棒采集规则最大化最坏情况期望效用，从而在分布偏移下实现可靠的提示优化。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动提示搜索方法（如InstructZero）在分布偏移和对抗性评估下性能下降的问题，因为这些方法仅在单一评估分布下优化期望性能，导致提示在不同设置间难以迁移。

Method: 采用分布鲁棒优化框架，将零样本提示优化构建为鲁棒贝叶斯优化，使用f-散度球定义评估分布周围的模糊集，通过鲁棒采集规则最大化最坏情况期望效用，同时保持贝叶斯搜索的查询效率。

Result: 在形式化重写任务中，准确率从61.3±0.7%提升至约85-90%，绝对增益约25-30个百分点；在代码调试任务中，域偏移下获得约25个百分点的增益；稳定任务（如因果推理）保持96%以上的准确率，表明在分布内情况下无性能损失。

Conclusion: DRO-InstructZero将分布鲁棒优化与提示学习相结合，提供了一种即插即用且通用的方法，在现实世界不确定性下实现可靠、可迁移的提示对齐。

Abstract: Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

</details>


### [52] [Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift](https://arxiv.org/abs/2510.15265)
*Emam Hossain,Muhammad Hasan Ferdous,Devon Dunmire,Aneesh Subramanian,Md Osman Gani*

Main category: cs.LG

TL;DR: RIC-TSC框架将因果发现融入时间序列分类，通过区域特定的因果图识别冰川湖演化的稳定预测因子，在分布外评估中比基于相关性的基线准确率提高12.59%。


<details>
  <summary>Details</summary>
Motivation: 当前地球观测中的时空模型过度依赖相关性特征，在异构领域间迁移性差，因果建模能发现稳定不变的关系以提高鲁棒性和泛化能力。

Method: 使用J-PCMCI+因果发现方法识别区域特定和不变的预测因子，将验证的预测因子及其时间滞后输入轻量级分类器进行时间序列分类。

Result: 在1000个手动标记的冰川湖数据集上，因果模型在分布外评估中比相关性基线准确率最高提升12.59%。

Conclusion: 因果发现不仅是特征选择手段，更是构建可泛化和机制基础的地球表面动态过程模型的途径。

Abstract: Causal modeling offers a principled foundation for uncovering stable,
invariant relationships in time-series data, thereby improving robustness and
generalization under distribution shifts. Yet its potential is underutilized in
spatiotemporal Earth observation, where models often depend on purely
correlational features that fail to transfer across heterogeneous domains. We
propose RIC-TSC, a regionally-informed causal time-series classification
framework that embeds lag-aware causal discovery directly into sequence
modeling, enabling both predictive accuracy and scientific interpretability.
Using multi-modal satellite and reanalysis data-including Sentinel-1 microwave
backscatter, Sentinel-2 and Landsat-8 optical reflectance, and CARRA
meteorological variables-we leverage Joint PCMCI+ (J-PCMCI+) to identify
region-specific and invariant predictors of supraglacial lake evolution in
Greenland. Causal graphs are estimated globally and per basin, with validated
predictors and their time lags supplied to lightweight classifiers. On a
balanced benchmark of 1000 manually labeled lakes from two contrasting melt
seasons (2018-2019), causal models achieve up to 12.59% higher accuracy than
correlation-based baselines under out-of-distribution evaluation. These results
show that causal discovery is not only a means of feature selection but also a
pathway to generalizable and mechanistically grounded models of dynamic Earth
surface processes.

</details>


### [53] [Semi-Supervised Regression with Heteroscedastic Pseudo-Labels](https://arxiv.org/abs/2510.15266)
*Xueqing Sun,Renzhen Wang,Quanziang Wang,Yichen Wu,Xixi Jia,Deyu Meng*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的伪标签框架，用于半监督回归问题，通过双层优化动态调整伪标签的影响，有效减轻不可靠伪标签的影响。


<details>
  <summary>Details</summary>
Motivation: 半监督回归中伪标签方法研究不足，连续输出和异方差噪声使得难以评估伪标签可靠性，朴素伪标签方法容易导致错误累积和过拟合。

Method: 采用不确定性感知的伪标签框架，从双层优化角度动态调整伪标签影响，联合最小化所有数据的经验风险和优化不确定性估计以增强标记数据的泛化能力。

Result: 在多个基准半监督回归数据集上的实验验证了方法的有效性，相比现有方法表现出更好的鲁棒性和性能。

Conclusion: 该方法为半监督回归中的伪标签问题提供了有效的解决方案，通过不确定性感知和双层优化机制显著提升了模型性能。

Abstract: Pseudo-labeling is a commonly used paradigm in semi-supervised learning, yet
its application to semi-supervised regression (SSR) remains relatively
under-explored. Unlike classification, where pseudo-labels are discrete and
confidence-based filtering is effective, SSR involves continuous outputs with
heteroscedastic noise, making it challenging to assess pseudo-label
reliability. As a result, naive pseudo-labeling can lead to error accumulation
and overfitting to incorrect labels. To address this, we propose an
uncertainty-aware pseudo-labeling framework that dynamically adjusts
pseudo-label influence from a bi-level optimization perspective. By jointly
minimizing empirical risk over all data and optimizing uncertainty estimates to
enhance generalization on labeled data, our method effectively mitigates the
impact of unreliable pseudo-labels. We provide theoretical insights and
extensive experiments to validate our approach across various benchmark SSR
datasets, and the results demonstrate superior robustness and performance
compared to existing methods. Our code is available at
https://github.com/sxq/Heteroscedastic-Pseudo-Labels.

</details>


### [54] [On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions](https://arxiv.org/abs/2510.15327)
*Zailin Ma,Jiansheng Yang,Yaodong Yang*

Main category: cs.LG

TL;DR: 本文研究了可学习激活函数的随机特征模型(RFLAF)的泛化性质，通过数据依赖的采样方案显著提高了特征数量需求的边界，在回归和分类任务中都获得了迄今为止最尖锐的边界。


<details>
  <summary>Details</summary>
Motivation: 研究RFLAF模型的泛化性质，通过改进采样方案来减少所需特征数量，提高模型效率。

Method: 采用数据依赖的采样方案生成特征，包括普通采样方案和杠杆加权方案，并提出算法来学习加权RFLAF。

Result: 通过加权采样，MSE损失情况下的特征数量边界从Ω(1/ε²)改进到Ω̃((1/ε)^{1/t})，当Gram矩阵有限秩时甚至可达Ω(1)；Lipschitz损失边界从Ω(1/ε²)改进到Ω̃((1/ε²)^{1/t})。

Conclusion: 加权RFLAF在显著减少特征数量的情况下能达到与普通RFLAF相同的性能，验证了理论的有效性和方法的实用性。

Abstract: This paper studies the generalization properties of a recently proposed
kernel method, the Random Feature models with Learnable Activation Functions
(RFLAF). By applying a data-dependent sampling scheme for generating features,
we provide by far the sharpest bounds on the required number of features for
learning RFLAF in both the regression and classification tasks. We provide a
unified theorem that describes the complexity of the feature number $s$, and
discuss the results for the plain sampling scheme and the data-dependent
leverage weighted scheme. Through weighted sampling, the bound on $s$ in the
MSE loss case is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon)^{1/t})$ in general $(t\geq 1)$, and even to
$\Omega(1)$ when the Gram matrix has a finite rank. For the Lipschitz loss
case, the bound is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon^2)^{1/t})$. To learn the weighted RFLAF, we also
propose an algorithm to find an approximate kernel and then apply the leverage
weighted sampling. Empirical results show that the weighted RFLAF achieves the
same performances with a significantly fewer number of features compared to the
plainly sampled RFLAF, validating our theories and the effectiveness of this
method.

</details>


### [55] [Towards Robust Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.15382)
*Kexin Zheng,Lauriane Teyssier,Yinan Zheng,Yu Luo,Xiayuan Zhan*

Main category: cs.LG

TL;DR: BREEZE是一个改进的零样本强化学习框架，通过行为正则化、扩散模型策略提取和注意力架构，解决了现有方法在表达性和外推误差方面的问题，在多个基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本强化学习方法（如Forward-Backward表示）存在表达性不足和离线学习中外推误差导致表示偏差的问题，最终导致次优性能。

Method: BREEZE引入行为正则化将策略优化转化为稳定的样本内学习范式，使用任务条件扩散模型提取策略以生成高质量多模态动作分布，并采用基于注意力的表达性架构进行表示建模。

Result: 在ExORL和D4RL Kitchen上的广泛实验表明，BREEZE实现了最佳或接近最佳的性能，同时相比先前的离线零样本RL方法表现出更优越的鲁棒性。

Conclusion: BREEZE通过同时增强学习稳定性、策略提取能力和表示学习质量，有效解决了零样本强化学习中的关键挑战，为学习预训练的通用策略提供了改进框架。

Abstract: The recent development of zero-shot reinforcement learning (RL) has opened a
new avenue for learning pre-trained generalist policies that can adapt to
arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward
representations (FB) and related methods have shown promise in zero-shot RL, we
empirically found that their modeling lacks expressivity and that extrapolation
errors caused by out-of-distribution (OOD) actions during offline learning
sometimes lead to biased representations, ultimately resulting in suboptimal
performance. To address these issues, we propose Behavior-REgularizEd Zero-shot
RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that
simultaneously enhances learning stability, policy extraction capability, and
representation learning quality. BREEZE introduces behavioral regularization in
zero-shot RL policy learning, transforming policy optimization into a stable
in-sample learning paradigm. Additionally, BREEZE extracts the policy using a
task-conditioned diffusion model, enabling the generation of high-quality and
multimodal action distributions in zero-shot RL settings. Moreover, BREEZE
employs expressive attention-based architectures for representation modeling to
capture the complex relationships between environmental dynamics. Extensive
experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best
or near-the-best performance while exhibiting superior robustness compared to
prior offline zero-shot RL methods. The official implementation is available
at: https://github.com/Whiterrrrr/BREEZE.

</details>


### [56] [Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning](https://arxiv.org/abs/2510.15388)
*Mingyang Sun,Pengxiang Ding,Weinan Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: SWFP框架通过离散化流匹配推理过程，将其与最优传输的JKO原理对齐，将全局流分解为一系列小增量变换，实现稳定在线适应和高效微调预训练流模型。


<details>
  <summary>Details</summary>
Motivation: 行为克隆中的流/扩散策略虽然擅长从演示中学习复杂技能，但对分布偏移很脆弱，标准RL方法由于迭代推理过程和现有解决方案的限制而难以微调这些模型。

Method: SWFP通过固定步长欧拉方案离散化流匹配推理过程，将其分解为邻近分布之间的小增量变换序列，每个步骤对应JKO更新，通过熵正则化确保稳定在线适应。

Result: 实验证明SWFP在多个机器人控制基准测试中展现出增强的稳定性、效率和优越的适应性能。

Conclusion: SWFP提供了一种高效算法，通过小流块级联微调预训练流，具有更简单/更快的子模型训练、降低的计算/内存成本以及基于Wasserstein信任区域的可证明稳定性。

Abstract: While behavior cloning with flow/diffusion policies excels at learning
complex skills from demonstrations, it remains vulnerable to distributional
shift, and standard RL methods struggle to fine-tune these models due to their
iterative inference process and the limitations of existing workarounds. In
this work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on
the key insight that discretizing the flow matching inference process via a
fixed-step Euler scheme inherently aligns it with the variational
Jordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP
decomposes the global flow into a sequence of small, incremental
transformations between proximate distributions. Each step corresponds to a JKO
update, regularizing policy changes to stay near the previous iterate and
ensuring stable online adaptation with entropic regularization. This
decomposition yields an efficient algorithm that fine-tunes pre-trained flows
via a cascade of small flow blocks, offering significant advantages:
simpler/faster training of sub-models, reduced computational/memory costs, and
provable stability grounded in Wasserstein trust regions. Comprehensive
experiments demonstrate SWFP's enhanced stability, efficiency, and superior
adaptation performance across diverse robotic control benchmarks.

</details>


### [57] [Geometric Mixture Models for Electrolyte Conductivity Prediction](https://arxiv.org/abs/2510.15403)
*Anyi Li,Jiacheng Cen,Songyou Li,Mingze Li,Yang Yu,Wenbing Huang*

Main category: cs.LG

TL;DR: 提出了GeoMix框架，通过几何感知的等变消息传递来准确预测电解质系统的离子电导率，解决了现有方法在几何结构和分子间相互作用建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前电解质研究面临两个基本挑战：缺乏高质量标准化基准，以及对混合物系统中几何结构和分子间相互作用的建模不足。

Method: 首先重组并增强了CALiSol和DiffMix电解质数据集，加入分子几何图表示；然后提出GeoMix框架，保持Set-SE(3)等变性，核心是专门为分子间几何消息传递设计的等变模块GIN。

Result: GeoMix在两个数据集上始终优于多种基线方法（包括MLPs、GNNs和几何GNNs），验证了跨分子几何相互作用和等变消息传递对准确性质预测的重要性。

Conclusion: 本研究不仅为电解质研究建立了新的基准，还提供了一个通用的几何学习框架，可推进能源材料、药物开发等领域中混合物系统的建模。

Abstract: Accurate prediction of ionic conductivity in electrolyte systems is crucial
for advancing numerous scientific and technological applications. While
significant progress has been made, current research faces two fundamental
challenges: (1) the lack of high-quality standardized benchmarks, and (2)
inadequate modeling of geometric structure and intermolecular interactions in
mixture systems. To address these limitations, we first reorganize and enhance
the CALiSol and DiffMix electrolyte datasets by incorporating geometric graph
representations of molecules. We then propose GeoMix, a novel geometry-aware
framework that preserves Set-SE(3) equivariance-an essential but challenging
property for mixture systems. At the heart of GeoMix lies the Geometric
Interaction Network (GIN), an equivariant module specifically designed for
intermolecular geometric message passing. Comprehensive experiments demonstrate
that GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,
and geometric GNNs) across both datasets, validating the importance of
cross-molecular geometric interactions and equivariant message passing for
accurate property prediction. This work not only establishes new benchmarks for
electrolyte research but also provides a general geometric learning framework
that advances modeling of mixture systems in energy materials, pharmaceutical
development, and beyond.

</details>


### [58] [Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting with Adaptive Windowing](https://arxiv.org/abs/2510.15404)
*Christopher Salazar,Krithika Manohar,Ashis G. Banerjee*

Main category: cs.LG

TL;DR: 提出WORK-DMD方法，结合随机傅里叶特征和在线动态模式分解，通过显式特征映射捕获非线性动态，在流数据实时预测中实现固定计算成本和竞争性预测精度。


<details>
  <summary>Details</summary>
Motivation: 流数据实时预测面临非平稳动态、严格计算限制和快速适应不遗忘的挑战，现有方法在精度、适应性和效率之间存在权衡，特别是在受限计算环境中。

Method: 结合随机傅里叶特征与在线动态模式分解，使用Sherman-Morrison更新在滚动窗口内实现连续适应，仅需当前数据无需历史数据存储。

Result: 在多个领域的基准数据集上，WORK-DMD比几种最先进的在线预测方法获得更高精度，只需单次数据遍历，在短期预测中表现尤为突出。

Conclusion: 结合核评估与自适应矩阵更新，以最小数据需求实现强预测性能，为流预测应用提供了深度学习的实用替代方案。

Abstract: Real-time forecasting from streaming data poses critical challenges: handling
non-stationary dynamics, operating under strict computational limits, and
adapting rapidly without catastrophic forgetting. However, many existing
approaches face trade-offs between accuracy, adaptability, and efficiency,
particularly when deployed in constrained computing environments. We introduce
WORK-DMD (Windowed Online Random Kernel Dynamic Mode Decomposition), a method
that combines Random Fourier Features with online Dynamic Mode Decomposition to
capture nonlinear dynamics through explicit feature mapping, while preserving
fixed computational cost and competitive predictive accuracy across evolving
data. WORK-DMD employs Sherman-Morrison updates within rolling windows,
enabling continuous adaptation to evolving dynamics from only current data,
eliminating the need for lengthy training or large storage requirements for
historical data. Experiments on benchmark datasets across several domains show
that WORK-DMD achieves higher accuracy than several state-of-the-art online
forecasting methods, while requiring only a single pass through the data and
demonstrating particularly strong performance in short-term forecasting. Our
results show that combining kernel evaluations with adaptive matrix updates
achieves strong predictive performance with minimal data requirements. This
sample efficiency offers a practical alternative to deep learning for streaming
forecasting applications.

</details>


### [59] [ParaFormer: Shallow Parallel Transformers with Progressive Approximation](https://arxiv.org/abs/2510.15425)
*Wei Wang,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: ParaFormer是一种浅层Transformer架构，通过并行分支结构实现真正的结构和计算并行化，解决了深层模型训练时间长、推理延迟高的问题。


<details>
  <summary>Details</summary>
Motivation: 解决'越深越好'理念带来的挑战：训练时间延长、推理延迟增加、资源受限设备不实用等问题。

Method: 将标准Transformer表述为闭式函数逼近器，通过并行分支组织层，算法化地强制层间协作，实现渐进逼近。

Result: ParaFormer在性能上优于标准Transformer如ViT，支持高达15.07倍的模型压缩，在多GPU部署中比FairScale等并行解决方案快3.30倍。

Conclusion: 基于通用逼近定理的闭式Transformer表述不仅解释了'深度信念'，还为设计高效Transformer架构开辟了新途径。

Abstract: The widespread 'deeper is better' philosophy has driven the creation of
architectures like ResNet and Transformer, which achieve high performance by
stacking numerous layers. However, increasing model depth comes with challenges
such as longer training times, higher inference latency, and impracticality on
resource-constrained devices. To address these issues, we propose ParaFormer, a
shallow Transformer architecture designed for true parallelism in both
structure and computation. By formulating standard Transformers as function
approximators in closed-form, our theoretical analysis shows that their
performance relies on inter-layer collaboration for progressive approximation,
rather than depth itself. While deep Transformers enforce this collaboration
through sequential designs, we demonstrate that such collaboration is not
inherently tied to sequential structures. ParaFormer removes the sequential
constraint by organizing layers into parallel branches, enforcing inter-layer
collaboration algorithmically. Specifically, we implement progressive
approximation, ensuring that each new branch further reduces the loss from
preceding branches, enabling faster convergence. Extensive experiments validate
ParaFormer's effectiveness, outperforming standard Transformers like ViT.
Moreover, ParaFormer supports up to 15.07x model compression and facilitates
model expansion for adaptive continuous learning. Experimental results on
multi-GPU deployment demonstrate that ParaFormer is 3.30x faster than widely
used parallelism solutions such as FairScale. These advancements stem from our
closed-form formulation of Transformers based on the Universal Approximation
Theorem, which not only explains the ``depth belief'' but also opens new
avenues for designing efficient Transformer architectures. Source code:
https://(open-upon-acceptance)

</details>


### [60] [Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models](https://arxiv.org/abs/2510.15429)
*Shashank Gupta*

Main category: cs.LG

TL;DR: 该论文研究如何设计安全、样本高效且鲁棒的强化学习方法，在上下文多臂老虎机框架下，针对排名推荐系统和文本到图像扩散模型两大应用领域，提出了理论保证和改进算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决强化学习在实际部署中的安全性、样本效率和鲁棒性问题，特别是在排名推荐系统和生成模型等关键应用领域，需要确保算法不会比现有策略表现更差，同时提高学习效率。

Method: 采用上下文多臂老虎机框架，第一部分开发了曝光泛化边界理论和反事实风险最小化目标；第二部分在单动作老虎机中统一了离策略估计器并提出最优基线；第三部分结合PPO和REINFORCE提出了LOOP算法。

Result: 提出的方法能够保证不劣于日志策略，即使在稀疏反馈下也能安全部署；最优基线最小化了评估和策略梯度方差；LOOP算法在保持PPO样本效率的同时，生成更忠实于文本属性的图像。

Conclusion: 通过理论分析和算法创新，该论文为强化学习的实际应用提供了安全部署保证和效率提升方案，在排名推荐和生成模型领域取得了显著进展。

Abstract: This dissertation investigates how reinforcement learning (RL) methods can be
designed to be safe, sample-efficient, and robust. Framed through the unifying
perspective of contextual-bandit RL, the work addresses two major application
domains - ranking and recommendation, and text-to-image diffusion models. The
first part of the thesis develops theory and algorithms for safe deployment in
ranking systems. An exposure-based generalisation bound is derived, leading to
a counterfactual risk-minimisation objective whose solution is guaranteed not
to underperform the logging policy, even with sparse feedback. This guarantee
is extended to doubly robust estimators, enabling safety even under adversarial
or misspecified user models and offering practitioners explicit control over
permissible utility loss. The second part turns to single-action bandits, where
various off-policy estimators are unified within a baseline-correction
framework. A closed-form optimal baseline is proposed and shown to minimise
both evaluation and policy-gradient variance, thereby improving off-policy
learning reliability. The final part examines the trade-offs between efficiency
and effectiveness in generative RL. A systematic study of PPO and REINFORCE
motivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple
diffusion trajectories with a REINFORCE-style baseline inside PPO's clipped
objective. LOOP achieves PPO-level sample efficiency while producing
generations that align more faithfully with textual attributes.

</details>


### [61] [A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning](https://arxiv.org/abs/2510.15444)
*Zhi Zhou,Yuhao Tan,Zenan Li,Yuan Yao,Lan-Zhe Guo,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 本文提出了首个基于置信度估计理论框架来分析采样式测试时缩放方法，揭示了自一致性和困惑度方法的关键局限，并提出了结合两者优势的RPC混合方法，在保持推理性能的同时显著降低采样成本。


<details>
  <summary>Details</summary>
Motivation: 尽管采样式测试时缩放方法在实践中取得了成功，但其理论基础尚未得到充分探索。本文旨在填补这一空白，为理解这些方法的理论机制提供框架。

Method: 提出了RPC混合方法，包含两个关键组件：困惑度一致性（结合自一致性和困惑度的优势）和推理剪枝（消除低概率推理路径）。基于置信度估计的理论框架分析现有方法的局限性。

Result: 在七个基准数据集上的实验结果表明，RPC在保持与自一致性相当的推理性能的同时，不仅提高了置信度可靠性，还将采样成本降低了50%。

Conclusion: RPC方法通过理论指导的混合策略有效解决了现有采样式测试时缩放方法的局限性，在降低计算成本的同时保持了推理性能，为大规模语言模型的推理优化提供了新的理论见解和实用方法。

Abstract: Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is sampling-based test-time scaling methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing sampling-based test-time scaling methods, grounded in the
perspective of confidence estimation. Based on the framework, we analyze two
dominant paradigms: self-consistency and perplexity, and reveal key
limitations: self-consistency suffers from high estimation error while
perplexity exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: Perplexity Consistency and Reasoning Pruning. Perplexity
Consistency combines the strengths of self-consistency and perplexity, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. Reasoning Pruning prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven benchmark datasets demonstrate that RPC has a strong
potential for reducing reasoning error. Notably, RPC achieves reasoning
performance comparable to self-consistency while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.

</details>


### [62] [Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment](https://arxiv.org/abs/2510.15456)
*Jan Corazza,Hadi Partovi Aria,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 提出一种将时序逻辑因果图集成到概率奖励机中的方法，以解决强化学习在稀疏奖励和复杂时序依赖任务中的学习困难，加速策略学习并促进任务规范迁移。


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏奖励和复杂时序依赖任务中学习困难，概率奖励机虽然能捕获奖励信号的时序依赖性，但难以手动设计和修改，阻碍了利用高层因果知识和跨领域迁移。

Method: 将时序逻辑因果图集成到概率奖励机中，结合因果信息来改进奖励形式化，提供理论收敛保证。

Result: 理论上证明了方法能收敛到最优策略，并通过实证研究展示了其优势。

Conclusion: 该方法能有效利用因果信息加速强化学习策略学习，并支持任务规范在不同环境间的迁移。

Abstract: Reinforcement learning (RL) algorithms struggle with learning optimal
policies for tasks where reward feedback is sparse and depends on a complex
sequence of events in the environment. Probabilistic reward machines (PRMs) are
finite-state formalisms that can capture temporal dependencies in the reward
signal, along with nondeterministic task outcomes. While special RL algorithms
can exploit this finite-state structure to expedite learning, PRMs remain
difficult to modify and design by hand. This hinders the already difficult
tasks of utilizing high-level causal knowledge about the environment, and
transferring the reward formalism into a new domain with a different causal
structure. This paper proposes a novel method to incorporate causal information
in the form of Temporal Logic-based Causal Diagrams into the reward formalism,
thereby expediting policy learning and aiding the transfer of task
specifications to new environments. Furthermore, we provide a theoretical
result about convergence to optimal policy for our method, and demonstrate its
strengths empirically.

</details>


### [63] [Learning to Answer from Correct Demonstrations](https://arxiv.org/abs/2510.15464)
*Nirmit Joshi,Gene Li,Siddharth Bhandari,Shiva Prasad Kasiviswanathan,Cong Ma,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文研究从正确演示中学习生成答案的问题，提出了一种基于低基数奖励模型的新方法，相比传统最大似然估计方法具有更好的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设演示者属于低复杂度策略类，但本文认为奖励模型（指定哪些答案正确）属于低基数类是更弱的假设，在此情况下最大似然估计方法可能失败。

Method: 将问题形式化为上下文老虎机中的离线模仿学习，提出了一种新颖的替代方法，该方法仅依赖于奖励模型在低基数类中，学习样本复杂度与奖励类基数呈对数关系。

Result: 证明了最大似然估计方法在此情况下可能失败，而新方法在样本复杂度上具有对数优势。

Conclusion: 当从正确演示中学习时，应该超越最大似然估计方法，考虑基于低基数奖励模型的替代方法。

Abstract: We study the problem of learning to generate an answer (or completion) to a
question (or prompt), where there could be multiple correct answers, any one of
which is acceptable at test time. Learning is based on demonstrations of some
correct answer to each training question, as in Supervised Fine Tuning (SFT).
We formalize the problem as offline imitation learning in contextual bandits,
with demonstrations from some optimal policy, without explicitly observed
rewards. Prior work assumes that the demonstrator belongs to a low-complexity
policy class, which motivates maximum likelihood estimation (i.e., log-loss
minimization). In contrast, we propose relying only on the reward model
(specifying which answers are correct) being in a low-cardinality class, which
we argue is a weaker assumption. We show that likelihood maximization methods
can fail in this case, and instead devise an alternative novel approach that
learns with sample complexity logarithmic in the cardinality of the reward
class. Our work motivates looking beyond likelihood maximization when learning
from correct demonstrations.

</details>


### [64] [Adversary-Free Counterfactual Prediction via Information-Regularized Representations](https://arxiv.org/abs/2510.15479)
*Shiqin Tang,Rong Feng,Shuxin Zhuang,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: 提出一种基于信息论的因果推断方法，通过最小化表示与处理变量之间的互信息来消除分配偏差，无需对抗训练，在静态和动态设置中均表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决因果推断中存在的分配偏差问题，传统方法如对抗训练存在训练不稳定和调参负担，需要一种更稳定且理论完备的方法。

Method: 从连接反事实-事实风险差距与互信息的理论边界出发，学习一个预测结果的随机表示Z，同时最小化I(Z;T)。推导出可处理的变分目标，将信息项上界与监督解码器耦合，形成稳定的训练准则。

Result: 在受控数值模拟和真实临床数据集上的评估显示，该方法在似然度、反事实误差和政策评估指标上均优于最新的平衡、重加权和对抗基线方法。

Conclusion: 提出的信息论方法能够有效消除分配偏差，避免对抗训练的不稳定性，在因果推断任务中表现优异。

Abstract: We study counterfactual prediction under assignment bias and propose a
mathematically grounded, information-theoretic approach that removes
treatment-covariate dependence without adversarial training. Starting from a
bound that links the counterfactual-factual risk gap to mutual information, we
learn a stochastic representation Z that is predictive of outcomes while
minimizing I(Z; T). We derive a tractable variational objective that
upper-bounds the information term and couples it with a supervised decoder,
yielding a stable, provably motivated training criterion. The framework extends
naturally to dynamic settings by applying the information penalty to sequential
representations at each decision time. We evaluate the method on controlled
numerical simulations and a real-world clinical dataset, comparing against
recent state-of-the-art balancing, reweighting, and adversarial baselines.
Across metrics of likelihood, counterfactual error, and policy evaluation, our
approach performs favorably while avoiding the training instabilities and
tuning burden of adversarial schemes.

</details>


### [65] [OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2510.15495)
*Woo-Jin Ahn,Sang-Ryul Baek,Yong-Jun Lee,Hyun-Duck Choi,Myo-Taeg Lim*

Main category: cs.LG

TL;DR: OffSim是一个基于模型的离线逆强化学习框架，直接从专家轨迹中学习环境动态和奖励函数，无需预定义奖励函数或交互式模拟器，可离线训练策略。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要预定义奖励函数和交互式模拟器，这通常耗时耗力。OffSim旨在直接从专家轨迹中学习环境动态和奖励结构，避免手动设计。

Method: 联合优化高熵转移模型和IRL奖励函数以增强探索性，并引入OffSim+扩展支持多数据集设置，通过边际奖励进一步提升探索能力。

Result: 在MuJoCo实验中，OffSim相比现有离线IRL方法取得了显著性能提升，证明了其有效性和鲁棒性。

Conclusion: OffSim框架能够直接从专家轨迹中有效学习环境动态和奖励函数，为离线强化学习提供了一种无需预定义奖励和交互式模拟器的解决方案。

Abstract: Reinforcement learning algorithms typically utilize an interactive simulator
(i.e., environment) with a predefined reward function for policy training.
Developing such simulators and manually defining reward functions, however, is
often time-consuming and labor-intensive. To address this, we propose an
Offline Simulator (OffSim), a novel model-based offline inverse reinforcement
learning (IRL) framework, to emulate environmental dynamics and reward
structure directly from expert-generated state-action trajectories. OffSim
jointly optimizes a high-entropy transition model and an IRL-based reward
function to enhance exploration and improve the generalizability of the learned
reward. Leveraging these learned components, OffSim can subsequently train a
policy offline without further interaction with the real environment.
Additionally, we introduce OffSim$^+$, an extension that incorporates a
marginal reward for multi-dataset settings to enhance exploration. Extensive
MuJoCo experiments demonstrate that OffSim achieves substantial performance
gains over existing offline IRL methods, confirming its efficacy and
robustness.

</details>


### [66] [The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling](https://arxiv.org/abs/2510.15502)
*Shijia Kang,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出SESA框架，通过顺序采样生成多样化解决方案草图，缓解强化学习中探索不足和熵崩溃问题，提升LLMs推理多样性和性能


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练大语言模型时存在探索不足和熵崩溃问题，模型倾向于利用狭窄的解决方案集，导致采样多样性丧失，阻碍性能进一步提升

Method: SESA顺序采样框架：先生成多样化的解决方案草图，然后将其扩展为完整的推理路径，通过条件化每个新输出于先前输出来确保更广泛的探索

Result: 在合成任务中，顺序采样在路径多样性和从崩溃中恢复方面优于传统RL方法；在三个智能体基准测试中，成功率分别提升+0.25、+0.42和+0.07（相比基线RL最高达211%相对改进）

Conclusion: SESA为探索提供了结构化方法，为RL训练的大语言模型实现更有效和多样化的推理铺平了道路

Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.

</details>


### [67] [Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity](https://arxiv.org/abs/2510.15508)
*Naoki Yoshida,Satoshi Hayakawa,Yuhta Takida,Toshimitsu Uesaka,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出KME-CLIP方法，通过核希尔伯特空间中的内积来改进多模态对比预训练中的相似度计算机制，以更好地逼近点互信息。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP及其变体未能充分利用点互信息的线性结构，而理论研究显示跨模态相似度应与点互信息对应。

Method: 利用再生核希尔伯特空间中的内积来近似点互信息，提出KME-CLIP方法。

Result: 理论证明该方法能以任意精度逼近点互信息，实验表明在多个检索和分类任务中整体优于标准CLIP。

Conclusion: KME-CLIP通过利用点互信息的线性结构，有效提升了多模态对比预训练的性能。

Abstract: In this study, we propose an enhancement to the similarity computation
mechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior
theoretical research has demonstrated that the optimal similarity metrics
between paired modalities should correspond to the pointwise mutual information
(PMI) between the two modalities. However, the current implementations of CLIP
and its variants fail to fully utilize the underlying linear structure of PMI.
We therefore propose KME-CLIP, which leverages this structure through the inner
product in a reproducing kernel Hilbert space. We theoretically prove that our
method can approximate PMI with arbitrary accuracy and empirically demonstrate
that our approach overall outperforms the standard CLIP formulation across
several retrieval and classification tasks.

</details>


### [68] [Language Models are Injective and Hence Invertible](https://arxiv.org/abs/2510.15511)
*Giorgos Nikolaou,Tommaso Mencattini,Donato Crisostomi,Andrea Santilli,Yannis Panagakis,Emanuele Rodola'*

Main category: cs.LG

TL;DR: 本文证明Transformer语言模型在离散输入到连续表示的映射中是单射的，因此是无损的，并提出了首个能有效从隐藏激活中重建输入文本的算法SipIt。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，即Transformer组件如非线性激活和归一化本质上是非单射的，可能阻止从模型表示中精确恢复输入。

Method: 首先数学证明语言模型在初始化和训练过程中保持单射性；其次在六个最先进语言模型上进行数十亿次碰撞测试；最后提出SipIt算法实现从隐藏激活中精确重建输入文本。

Result: 理论证明和实证测试均表明语言模型是单射的，无碰撞发生；SipIt算法在实践中实现了精确可逆性，具有线性时间保证。

Conclusion: 单射性是语言模型的基本且可利用属性，对透明度、可解释性和安全部署具有直接意义。

Abstract: Transformer components such as non-linear activations and normalization are
inherently non-injective, suggesting that different inputs could map to the
same output and prevent exact recovery of the input from a model's
representations. In this paper, we challenge this view. First, we prove
mathematically that transformer language models mapping discrete input
sequences to their corresponding sequence of continuous representations are
injective and therefore lossless, a property established at initialization and
preserved during training. Second, we confirm this result empirically through
billions of collision tests on six state-of-the-art language models, and
observe no collisions. Third, we operationalize injectivity: we introduce
SipIt, the first algorithm that provably and efficiently reconstructs the exact
input text from hidden activations, establishing linear-time guarantees and
demonstrating exact invertibility in practice. Overall, our work establishes
injectivity as a fundamental and exploitable property of language models, with
direct implications for transparency, interpretability, and safe deployment.

</details>


### [69] [An Empirical Study on MC Dropout--Based Uncertainty--Error Correlation in 2D Brain Tumor Segmentation](https://arxiv.org/abs/2510.15541)
*Saumya B*

Main category: cs.LG

TL;DR: 本研究评估了MC Dropout在脑肿瘤MRI分割中识别边界误差的有效性，发现其不确定性估计与分割误差相关性较弱，特别是在边界区域几乎无相关性。


<details>
  <summary>Details</summary>
Motivation: MC Dropout被广泛用于估计模型不确定性，但其在识别脑肿瘤MRI分割误差（特别是边界区域）方面的有效性尚不明确，需要实证研究。

Method: 使用U-Net在四种数据增强设置（无增强、水平翻转、旋转、缩放）下进行2D脑肿瘤MRI分割，通过50次随机前向传播计算不确定性，并使用Pearson和Spearman相关系数分析不确定性与像素级误差的关系。

Result: 结果显示全局相关性较弱（r≈0.30-0.38），边界相关性可忽略（|r|<0.05）。虽然不同增强设置间的差异具有统计显著性（p<0.001），但缺乏实际意义。

Conclusion: MC Dropout不确定性在边界误差定位方面提供的线索有限，表明在医学图像分割中需要替代或混合的不确定性估计方法。

Abstract: Accurate brain tumor segmentation from MRI is vital for diagnosis and
treatment planning. Although Monte Carlo (MC) Dropout is widely used to
estimate model uncertainty, its effectiveness in identifying segmentation
errors -- especially near tumor boundaries -- remains unclear. This study
empirically examines the relationship between MC Dropout--based uncertainty and
segmentation error in 2D brain tumor MRI segmentation using a U-Net trained
under four augmentation settings: none, horizontal flip, rotation, and scaling.
Uncertainty was computed from 50 stochastic forward passes and correlated with
pixel-wise errors using Pearson and Spearman coefficients. Results show weak
global correlations ($r \approx 0.30$--$0.38$) and negligible boundary
correlations ($|r| < 0.05$). Although differences across augmentations were
statistically significant ($p < 0.001$), they lacked practical relevance. These
findings suggest that MC Dropout uncertainty provides limited cues for boundary
error localization, underscoring the need for alternative or hybrid uncertainty
estimation methods in medical image segmentation.

</details>


### [70] [GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device](https://arxiv.org/abs/2510.15620)
*Jiahao Zhou,Chengliang Lin,Dingji Li,Mingkai Dong,Haibo Chen*

Main category: cs.LG

TL;DR: GRATING是一个无需训练的推理系统，通过渐进式聚类剪枝和内存优化技术，在保持精度的同时显著降低语义Top-K选择的延迟和内存使用。


<details>
  <summary>Details</summary>
Motivation: 语义Top-K选择在边缘设备AI服务中占据主导延迟和内存需求，但传统方法需要计算所有候选者的精确分数，而实际上只有相对排名对最终结果重要。

Method: 提出整体前向传播方法，利用序列级稀疏性（中间层相对排名提前稳定）进行渐进式聚类剪枝，并采用双层滑动窗口和分块执行策略重叠I/O与计算以限制峰值内存。

Result: 在0.6B到8B参数的reranker上评估，GRATING在微基准测试中延迟降低达89.0%，峰值内存降低达94.9%，在三个真实世界应用中延迟降低11.6%-51.0%，峰值内存降低18.6%-77.8%。

Conclusion: GRATING通过利用相对排名稳定性和内存优化策略，在不损失精度的情况下显著提升了边缘设备上语义Top-K选择的效率和可部署性。

Abstract: Semantic top-K selection with cross-encoder rerankers underpins of on-device
AI services, such as retrieval-augmented generation, agent memory, and
personalized recommendation. However, its latency and memory demands dominate
end-to-end budgets on edge hardware. Revisiting the objective of top-K
selection, we reveal that only relative rankings matter, not exact
per-candidate scores. We further observe sequence-level sparsity: relative
rankings stabilize early in intermediate layers, allowing pruning opportunities
prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a
training-free inference system, GRATING. By maintaining a global view of all
candidates, it reduces latency through progressive cluster pruning. It also
bounds peak memory usage by strategically overlapping I/O with computation via
dual-layer sliding window and chunked execution. We evaluate GRATING against
state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple
M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak
memory by up to 94.9% in microbenchmarks, without any loss in precision. Across
three real-world on-device AI applications, GRATING lowers latency by
11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial
improvements in efficiency and deployability.

</details>


### [71] [CQD-SHAP: Explainable Complex Query Answering via Shapley Values](https://arxiv.org/abs/2510.15623)
*Parsa Abbasi,Stefan Heindorf*

Main category: cs.LG

TL;DR: 本文提出了CQD-SHAP框架，用于解释复杂查询回答中各个查询部分对特定答案排名的贡献，基于合作博弈论中的Shapley值，满足所有基本Shapley公理。


<details>
  <summary>Details</summary>
Motivation: 现有的复杂查询回答方法多为黑盒模型，缺乏用户信任。虽然神经符号方法如CQD具有一定可解释性，但无法解释查询不同部分的重要性。

Method: 基于Shapley值构建CQD-SHAP框架，计算查询各部分对答案排名的贡献，解释神经预测器从不完整知识图谱中推断新知识的价值。

Result: 自动化评估显示，该方法在必要性和充分性解释方面有效，与多种基线方法相比，对大多数查询类型都表现出良好效果。

Conclusion: CQD-SHAP提供了一种有效的解释框架，能够揭示复杂查询回答中各个查询部分的重要性，增强模型的可解释性和用户信任。

Abstract: Complex query answering (CQA) goes beyond the well-studied link prediction
task by addressing more sophisticated queries that require multi-hop reasoning
over incomplete knowledge graphs (KGs). Research on neural and neurosymbolic
CQA methods is still an emerging field. Almost all of these methods can be
regarded as black-box models, which may raise concerns about user trust.
Although neurosymbolic approaches like CQD are slightly more interpretable,
allowing intermediate results to be tracked, the importance of different parts
of the query remains unexplained. In this paper, we propose CQD-SHAP, a novel
framework that computes the contribution of each query part to the ranking of a
specific answer. This contribution explains the value of leveraging a neural
predictor that can infer new knowledge from an incomplete KG, rather than a
symbolic approach relying solely on existing facts in the KG. CQD-SHAP is
formulated based on Shapley values from cooperative game theory and satisfies
all the fundamental Shapley axioms. Automated evaluation of these explanations
in terms of necessary and sufficient explanations, and comparisons with various
baselines, shows the effectiveness of this approach for most query types.

</details>


### [72] [Deep Neural ODE Operator Networks for PDEs](https://arxiv.org/abs/2510.15651)
*Ziqian Li,Kang Liu,Yongcun Song,Hangrui Yue,Enrique Zuazua*

Main category: cs.LG

TL;DR: 提出NODE-ONet框架，结合神经常微分方程和算子学习，通过编码器-解码器架构和物理编码的神经ODE来改进PDE求解的效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法往往忽略PDE的领域知识，导致难以捕捉时间动态且在训练时间范围外泛化能力差。

Method: 采用编码器-解码器架构：编码器空间离散化输入函数，神经ODE捕捉潜在时间动态，解码器重构物理空间解。提出物理编码的神经ODE来融入PDE特定物理属性。

Result: 在非线性扩散-反应和Navier-Stokes方程上的数值实验显示高精度、计算效率和在训练时间范围外的预测能力。

Conclusion: 该框架具有适应不同编码器/解码器的灵活性，能跨相关PDE族泛化，是科学机器学习中可扩展的物理编码工具。

Abstract: Operator learning has emerged as a promising paradigm for developing
efficient surrogate models to solve partial differential equations (PDEs).
However, existing approaches often overlook the domain knowledge inherent in
the underlying PDEs and hence suffer from challenges in capturing temporal
dynamics and generalization issues beyond training time frames. This paper
introduces a deep neural ordinary differential equation (ODE) operator network
framework, termed NODE-ONet, to alleviate these limitations. The framework
adopts an encoder-decoder architecture comprising three core components: an
encoder that spatially discretizes input functions, a neural ODE capturing
latent temporal dynamics, and a decoder reconstructing solutions in physical
spaces. Theoretically, error analysis for the encoder-decoder architecture is
investigated. Computationally, we propose novel physics-encoded neural ODEs to
incorporate PDE-specific physical properties. Such well-designed neural ODEs
significantly reduce the framework's complexity while enhancing numerical
efficiency, robustness, applicability, and generalization capacity. Numerical
experiments on nonlinear diffusion-reaction and Navier-Stokes equations
demonstrate high accuracy, computational efficiency, and prediction
capabilities beyond training time frames. Additionally, the framework's
flexibility to accommodate diverse encoders/decoders and its ability to
generalize across related PDE families further underscore its potential as a
scalable, physics-encoded tool for scientific machine learning.

</details>


### [73] [Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization](https://arxiv.org/abs/2510.15653)
*Yefan Zeng,Shengyu Duan,Rishad Shafik,Alex Yakovlev*

Main category: cs.LG

TL;DR: 本文提出了一种高效的Tsetlin机器软件实现，通过位操作、提前退出机制和文字重排序策略，在ARM处理器上实现了96.71%的推理时间减少。


<details>
  <summary>Details</summary>
Motivation: Tsetlin机器在资源受限设备上具有高速推理能力，其逻辑驱动操作适合在现代CPU架构上并行执行，因此需要优化软件实现以充分利用这一特性。

Method: 使用指令级位操作进行紧凑模型表示和加速处理；引入基于AND子句评估的提前退出机制避免不必要计算；提出文字重排序策略最大化提前退出概率，通过统计分析方法在训练后、推理前阶段应用。

Result: 在ARM处理器上使用gem5模拟器的实验结果显示，优化实现比传统基于整数的TM实现减少了96.71%的推理时间，同时保持了相当的代码密度。

Conclusion: 通过位操作、提前退出机制和文字重排序策略的组合优化，显著提升了Tsetlin机器的推理效率，为资源受限设备上的高效推理提供了有效解决方案。

Abstract: The Tsetlin Machine (TM) offers high-speed inference on resource-constrained
devices such as CPUs. Its logic-driven operations naturally lend themselves to
parallel execution on modern CPU architectures. Motivated by this, we propose
an efficient software implementation of the TM by leveraging instruction-level
bitwise operations for compact model representation and accelerated processing.
To further improve inference speed, we introduce an early exit mechanism, which
exploits the TM's AND-based clause evaluation to avoid unnecessary
computations. Building upon this, we propose a literal Reorder strategy
designed to maximize the likelihood of early exits. This strategy is applied
during a post-training, pre-inference stage through statistical analysis of all
literals and the corresponding actions of their associated Tsetlin Automata
(TA), introducing negligible runtime overhead. Experimental results using the
gem5 simulator with an ARM processor show that our optimized implementation
reduces inference time by up to 96.71% compared to the conventional
integer-based TM implementations while maintaining comparable code density.

</details>


### [74] [WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables](https://arxiv.org/abs/2510.15655)
*Lino Gerlach,Liv Våge,Thore Gerlach,Elliott Kauffman*

Main category: cs.LG

TL;DR: 本文提出了一种名为WARP-LUTs的新型梯度学习方法，用于高效学习逻辑门组合，相比现有的DLGNs方法具有更少的可训练参数和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有乘法自由模型如DLGNs虽然取得了令人印象深刻的结果，但存在训练计算成本高和对多输入逻辑块泛化能力差的问题。

Method: 引入Walsh辅助松弛概率查找表（WARP-LUTs）方法，这是一种新颖的基于梯度的学习框架，能够高效学习逻辑门组合。

Result: 在CIFAR-10数据集上，WARP-LUTs相比DLGNs实现了显著更快的收敛速度，同时保持了相当的准确性。

Conclusion: 该方法具有扩展到更高输入逻辑块的潜力，为在现代FPGA上实现极高效部署及其实时科学应用提供了未来研究方向。

Abstract: Fast and efficient machine learning is of growing interest to the scientific
community and has spurred significant research into novel model architectures
and hardware-aware design. Recent hard? and software co-design approaches have
demonstrated impressive results with entirely multiplication-free models.
Differentiable Logic Gate Networks (DLGNs), for instance, provide a
gradient-based framework for learning optimal combinations of low-level logic
gates, setting state-of-the-art trade-offs between accuracy, resource usage,
and latency. However, these models suffer from high computational cost during
training and do not generalize well to logic blocks with more inputs. In this
work, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables
(WARP-LUTs) - a novel gradient-based method that efficiently learns
combinations of logic gates with substantially fewer trainable parameters. We
demonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10
compared to DLGNs, while maintaining comparable accuracy. Furthermore, our
approach suggests potential for extension to higher-input logic blocks,
motivating future research on extremely efficient deployment on modern FPGAs
and its real-time science applications.

</details>


### [75] [CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning](https://arxiv.org/abs/2510.15674)
*Yung-Chen Tang,Pin-Yu Chen,Andrea Cavallaro*

Main category: cs.LG

TL;DR: CarBoN是一个测试时校准框架，通过自适应调整模型参数来改进推理路径，无需重新训练大语言模型，显著提高了推理任务的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放方法（如Best-of-N采样）在N增加时收益递减，存在效率低下的问题，需要一种更高效的测试时校准方法来提升语言模型在推理任务中的表现。

Method: 提出CarBoN方法，分为两个阶段：首先探索解空间，然后通过学习输入特定的温度T和加性偏移向量δ来校准logits，引导生成更可靠的推理路径。

Result: 在MATH-500和AIME-2024上的实验表明，CarBoN将效率提高了4倍，在相同准确率下需要更少的rollouts，并且在固定预算下通常能达到更高的准确率。

Conclusion: CarBoN框架有效提升了测试时推理的效率，T和δ在平衡输出多样性和正确性方面发挥互补作用，该框架还可推广到beam search等步级采样策略。

Abstract: Allocating more computation during inference time (test-time scaling)
improves language model performance, especially for reasoning tasks. However,
popular methods like Best-of-$N$ sampling often show diminishing returns as $N$
increases. To address this inefficiency, we introduce a general test-time
calibration framework that adaptively modifies the model toward high-reward
reasoning paths, with theoretical guarantees of improving the lower bound of
expected reward under finite sampling, all without large language model (LLM)
retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),
a two-phase method that first explores the solution space and then learns a
calibration of the logits via an input-specific temperature $T$ and additive
shift vector $\delta$, guiding generation toward more reliable reasoning.
Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,
with up to $4\times$ fewer rollouts to reach the same accuracy, while often
achieving higher accuracy under fixed budgets. We also analyze the
complementary roles of $T$ and $\delta$ in balancing output diversity and
correctness, and demonstrate that the framework also generalizes to step-level
sampling strategies such as beam search. For more information, please refer to
our project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.

</details>


### [76] [ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations](https://arxiv.org/abs/2510.15700)
*Alex Gu,Bartosz Piotrowski,Fabian Gloeckle,Kaiyu Yang,Aram H. Markosyan*

Main category: cs.LG

TL;DR: ProofOptimizer是一个通过专家迭代和强化学习训练的模型，专门用于简化Lean证明，无需人工监督即可将证明长度大幅压缩。


<details>
  <summary>Details</summary>
Motivation: 神经定理证明生成的证明虽然经过形式系统验证，但长度过长难以理解，限制了数学洞察力。现有方法在处理RL训练证明器生成的极长证明时效果不佳。

Method: 通过专家迭代和强化学习训练ProofOptimizer模型，使用Lean验证简化并提供训练信号，在推理时采用迭代证明缩短工作流程逐步减少证明长度。

Result: 在标准基准测试中，ProofOptimizer显著压缩了最先进RL训练证明器生成的证明：miniF2F减少87%，PutnamBench减少57%，Seed-Prover的IMO 2025证明减少49%。简化后的证明在Lean中检查更快，且作为监督微调的训练数据能进一步提升下游证明器性能。

Conclusion: ProofOptimizer是首个无需人工监督即可简化Lean证明的语言模型，能有效解决证明简化的关键瓶颈问题，为数学洞察提供更简洁的证明形式。

Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO
gold-medalist capabilities and producing formal proofs that span thousands of
lines. Although such proofs are mechanically verified by formal systems like
Lean, their excessive length renders them difficult for humans to comprehend
and limits their usefulness for mathematical insight. Proof simplification is
therefore a critical bottleneck. Yet, training data for this task is scarce,
and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --
struggle with the extremely long proofs generated by RL-trained provers. We
introduce ProofOptimizer, the first language model trained to simplify Lean
proofs without requiring additional human supervision. ProofOptimizer is
trained via expert iteration and reinforcement learning, using Lean to verify
simplifications and provide training signal. At inference time, it operates
within an iterative proof-shortening workflow, progressively reducing proof
length. Experiments show that ProofOptimizer substantially compresses proofs
generated by state-of-the-art RL-trained provers on standard benchmarks,
reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on
Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check
faster in Lean and further improve downstream prover performance when reused as
training data for supervised finetuning.

</details>


### [77] [ProSh: Probabilistic Shielding for Model-free Reinforcement Learning](https://arxiv.org/abs/2510.15720)
*Edwin Hamel-De le Court,Gaspard Ohlmann,Francesco Belardinelli*

Main category: cs.LG

TL;DR: ProSh是一种基于风险增强的概率屏蔽模型无关安全强化学习算法，通过在学习过程中施加安全屏蔽来保证智能体行为的安全性。


<details>
  <summary>Details</summary>
Motivation: 强化学习系统的安全性是重要关切，需要开发既能最优执行又能提供正式安全保证的RL系统。

Method: 通过风险预算增强约束MDP状态空间，使用学习到的成本批评器对智能体策略分布施加屏蔽，确保所有采样动作在期望意义上保持安全。

Result: 在确定性环境中保持最优性，提供仅依赖于备份批评器准确度的成本期望紧上界，实验证明在训练期间也能保证安全性。

Conclusion: ProSh算法能够有效保证强化学习系统的安全性，即使在训练阶段也能在满足温和假设条件下提供安全保证。

Abstract: Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also safe to
deploy by providing formal guarantees about their safety. To this end, we
introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free
algorithm for safe reinforcement learning under cost constraints. ProSh
augments the Constrained MDP state space with a risk budget and enforces safety
by applying a shield to the agent's policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in expectation.
We also show that optimality is preserved when the environment is
deterministic. Since ProSh is model-free, safety during training depends on the
knowledge we have acquired about the environment. We provide a tight
upper-bound on the cost in expectation, depending only on the backup-critic
accuracy, that is always satisfied during training. Under mild, practically
achievable assumptions, ProSh guarantees safety even at training time, as shown
in the experiments.

</details>


### [78] [RLAF: Reinforcement Learning from Automaton Feedback](https://arxiv.org/abs/2510.15728)
*Mahyar Alinejad,Alvaro Velasquez,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: 提出了一种基于自动机反馈的强化学习方法，用确定性有限自动机(DFA)生成的偏好替代显式奖励函数，无需手动设计奖励函数。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理具有复杂历史依赖奖励结构的环境时面临挑战，需要手动设计奖励函数，过程繁琐且容易出错。

Method: 使用DFA结构生成轨迹偏好来学习奖励函数，包括静态方法（直接使用学习到的奖励函数进行策略优化）和动态方法（通过迭代更新持续优化奖励函数和策略）。

Result: 在离散和连续环境中的实验表明，该方法能够学习具有时间依赖任务的有效策略，性能优于传统奖励工程和基于自动机的基线方法。

Conclusion: 基于自动机的偏好方法在处理非马尔可夫奖励方面具有优势，为传统奖励建模提供了可扩展、高效且无需人工干预的替代方案，并提供了收敛性保证。

Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent
reward structures poses significant challenges for traditional methods. In this
work, we introduce a novel approach that leverages automaton-based feedback to
guide the learning process, replacing explicit reward functions with
preferences derived from a deterministic finite automaton (DFA). Unlike
conventional approaches that use automata for direct reward specification, our
method employs the structure of the DFA to generate preferences over
trajectories that are used to learn a reward function, eliminating the need for
manual reward engineering. Our framework introduces a static approach that uses
the learned reward function directly for policy optimization and a dynamic
approach that involves continuous refining of the reward function and policy
through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that
our approach enables the RL agent to learn effective policies for tasks with
temporal dependencies, outperforming traditional reward engineering and
automaton-based baselines such as reward machines and LTL-guided methods. Our
results highlight the advantages of automaton-based preferences in handling
non-Markovian rewards, offering a scalable, efficient, and human-independent
alternative to traditional reward modeling. We also provide a convergence
guarantee showing that under standard assumptions our automaton-guided
preference-based framework learns a policy that is near-optimal with respect to
the true non-Markovian objective.

</details>


### [79] [Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for Enhanced Welfare and Productivity](https://arxiv.org/abs/2510.15757)
*Pieris Panagi,Savvas Karatsiolis,Kyriacos Mosphilis,Nicholas Hadjisavvas,Andreas Kamilaris,Nicolas Nicolaou,Efstathios Stavrakis,Vassilis Vassiliades*

Main category: cs.LG

TL;DR: PoultryFI是一个面向中小型家禽养殖场的低成本AI平台，集成了摄像头布局优化、视听监控、分析预警、实时鸡蛋计数、生产预测和推荐六大模块，通过边缘计算实现连续监测和决策支持。


<details>
  <summary>Details</summary>
Motivation: 中小型家禽养殖场缺乏经济实惠的集成工具进行持续监测和决策，主要依赖人工检查，难以同时满足生产效率、动物福利和环境合规要求。

Method: 使用进化算法离线优化摄像头布局实现全覆盖，整合视频、音频和饲喂数据的视听监控模块提取福利指标，边缘视觉模型实现实时鸡蛋计数，预测模型进行10天内的产量和饲料消耗预测，结合天气数据提供操作建议。

Result: 现场试验显示在树莓派5上实现100%鸡蛋计数准确率，具备稳健的异常检测能力和可靠的短期预测性能。

Conclusion: PoultryFI填补了孤立试点工具与规模化农场智能之间的空白，帮助生产者主动保障动物福利和盈利能力。

Abstract: Poultry farming faces increasing pressure to meet productivity targets while
ensuring animal welfare and environmental compliance. Yet many small and
medium-sized farms lack affordable, integrated tools for continuous monitoring
and decision-making, relying instead on manual, reactive inspections. This
paper presents Poultry Farm Intelligence (PoultryFI) - a modular,
cost-effective platform that integrates six AI-powered modules: Camera
Placement Optimizer, Audio-Visual Monitoring, Analytics & Alerting, Real-Time
Egg Counting, Production & Profitability Forecasting, and a Recommendation
Module.
  Camera layouts are first optimized offline using evolutionary algorithms for
full poultry house coverage with minimal hardware. The Audio-Visual Monitoring
module extracts welfare indicators from synchronized video, audio, and feeding
data. Analytics & Alerting produces daily summaries and real-time
notifications, while Real-Time Egg Counting uses an edge vision model to
automate production tracking. Forecasting models predict egg yield and feed
consumption up to 10 days in advance, and the Recommendation Module integrates
forecasts with weather data to guide environmental and operational adjustments.
  This is among the first systems to combine low-cost sensing, edge analytics,
and prescriptive AI to continuously monitor flocks, predict production, and
optimize performance. Field trials demonstrate 100% egg-count accuracy on
Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting.
PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide
intelligence, empowering producers to proactively safeguard welfare and
profitability.

</details>


### [80] [Chronos-2: From Univariate to Universal Forecasting](https://arxiv.org/abs/2510.15821)
*Abdul Fatir Ansari,Oleksandr Shchur,Jaris Küken,Andreas Auer,Boran Han,Pedro Mercado,Syama Sundar Rangapuram,Huibin Shen,Lorenzo Stella,Xiyuan Zhang,Mononito Goswami,Shubham Kapoor,Danielle C. Maddix,Pablo Guerron,Tony Hu,Junming Yin,Nick Erickson,Prateek Mutalik Desai,Hao Wang,Huzefa Rangwala,George Karypis,Yuyang Wang,Michael Bohlke-Schneider*

Main category: cs.LG

TL;DR: Chronos-2是一个预训练的时间序列模型，能够以零样本方式处理单变量、多变量和协变量预测任务，通过组注意力机制实现跨时间序列的上下文学习，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练时间序列模型主要关注单变量预测，限制了在现实世界多变量数据和协变量场景中的适用性，需要开发能够处理更复杂预测任务的通用模型。

Method: 采用组注意力机制促进跨多个时间序列的上下文学习，通过在合成数据集上进行训练，对单变量序列施加多样化的多变量结构，实现通用预测能力。

Result: 在fev-bench、GIFT-Eval和Chronos Benchmark II三个基准测试中达到最先进性能，特别是在强调多变量和协变量预测的fev-bench上表现突出，在涉及协变量的任务中大幅超越基线模型。

Conclusion: Chronos-2的上下文学习能力使其成为一个通用预测模型，可以直接用于现实世界的预测流程中，无需特定任务训练。

Abstract: Pretrained time series models have enabled inference-only forecasting systems
that produce accurate predictions without task-specific training. However,
existing approaches largely focus on univariate forecasting, limiting their
applicability in real-world scenarios where multivariate data and covariates
play a crucial role. We present Chronos-2, a pretrained model capable of
handling univariate, multivariate, and covariate-informed forecasting tasks in
a zero-shot manner. Chronos-2 employs a group attention mechanism that
facilitates in-context learning (ICL) through efficient information sharing
across multiple time series within a group, which may represent sets of related
series, variates of a multivariate series, or targets and covariates in a
forecasting task. These general capabilities are achieved through training on
synthetic datasets that impose diverse multivariate structures on univariate
series. Chronos-2 delivers state-of-the-art performance across three
comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On
fev-bench, which emphasizes multivariate and covariate-informed forecasting,
Chronos-2's universal ICL capabilities lead to substantial improvements over
existing models. On tasks involving covariates, it consistently outperforms
baselines by a wide margin. Case studies in the energy and retail domains
further highlight its practical advantages. The in-context learning
capabilities of Chronos-2 establish it as a general-purpose forecasting model
that can be used "as is" in real-world forecasting pipelines.

</details>


### [81] [SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients](https://arxiv.org/abs/2510.15830)
*Dominik Kallusky,Vinay Rao,Vishal Nandavanam,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: 论文提出SNOO优化器，通过将Nesterov动量应用于伪梯度，在非分布式设置中实现1.5-2.5倍的计算效率提升，且改进效果随模型规模增大而增强。


<details>
  <summary>Details</summary>
Motivation: 研究DiLoCo优化器在非分布式设置中意外有效性的原因，发现其核心优势来自对伪梯度应用Nesterov动量，从而开发更高效的优化方法。

Method: 提出Step-K Nesterov Outer Optimizer (SNOO)，在Lookahead双循环框架中，对多个内优化器步骤产生的伪梯度应用Nesterov动量来更新慢权重。

Result: SNOO在非分布式设置中达到1.5-2.5倍的计算效率提升，训练FLOPs规模可达1e23，且改进效果随模型规模增大而增加。

Conclusion: SNOO因其最小的计算和内存开销以及与模型分片的兼容性，成为适用于包括AdamW和Muon在内的各种内优化器的实用增强方法。

Abstract: The rapid development of large language models (LLMs) has driven the demand
for more efficient optimization techniques. Among these, the Lookahead family
of optimizers employs a two-loop framework, maintaining fast and slow sets of
model weights. Multiple inner optimizer steps on the fast weights produce a
trajectory - the pseudo-gradient - that is used to update the slow weights.
DiLoCo, a notable example originally designed for distributed training, applies
Nesterov momentum to the averaged pseudo-gradient from multiple workers,
claiming to even outperform AdamW in a non-distributed setup. In this paper, we
empirically show that DiLoCo's surprising effectiveness stems primarily from
applying Nesterov momentum to the pseudo-gradient, which improves training in a
non-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov
Outer Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains
of 1.5 - 2.5$\times$ in a non-distributed setting up to a scale of 1e23
training FLOPs, with improvements that increase with model size. Because of its
minimal compute and memory overhead and compatibility with model sharding, SNOO
is a practical enhancement for a variety of inner optimizers, including AdamW
and Muon.

</details>


### [82] [Learning Correlated Reward Models: Statistical Barriers and Opportunities](https://arxiv.org/abs/2510.15839)
*Yeshwanth Cherapanamjeri,Constantinos Daskalakis,Gabriele Farina,Sobhan Mohammadpour*

Main category: cs.LG

TL;DR: 本文研究了相关probit模型的统计和计算挑战，发现传统的成对偏好数据不足以学习相关性信息，而三选一偏好数据能够克服这一限制，并提出了一种高效估计器。


<details>
  <summary>Details</summary>
Motivation: 随机效用模型在人类反馈强化学习中起关键作用，但许多技术依赖无关选项独立性假设，这限制了人类偏好的精细建模。避免该假设的模型缺乏统计和计算保证。

Method: 研究相关probit模型的学习挑战，分析不同数据收集范式（成对偏好vs三选一偏好），提出统计和计算高效的估计器。

Result: 证明成对偏好数据无法学习相关性信息，而三选一偏好数据能够克服这一限制，提出的估计器具有接近最优性能。

Conclusion: 高阶偏好数据在相关效用学习中具有优势，能够实现更精细的人类偏好建模，在真实数据集上验证了改进的个性化效果。

Abstract: Random Utility Models (RUMs) are a classical framework for modeling user
preferences and play a key role in reward modeling for Reinforcement Learning
from Human Feedback (RLHF). However, a crucial shortcoming of many of these
techniques is the Independence of Irrelevant Alternatives (IIA) assumption,
which collapses \emph{all} human preferences to a universal underlying utility
function, yielding a coarse approximation of the range of human preferences. On
the other hand, statistical and computational guarantees for models avoiding
this assumption are scarce. In this paper, we investigate the statistical and
computational challenges of learning a \emph{correlated} probit model, a
fundamental RUM that avoids the IIA assumption. First, we establish that the
classical data collection paradigm of pairwise preference data is
\emph{fundamentally insufficient} to learn correlational information,
explaining the lack of statistical and computational guarantees in this
setting. Next, we demonstrate that \emph{best-of-three} preference data
provably overcomes these shortcomings, and devise a statistically and
computationally efficient estimator with near-optimal performance. These
results highlight the benefits of higher-order preference data in learning
correlated utilities, allowing for more fine-grained modeling of human
preferences. Finally, we validate these theoretical guarantees on several
real-world datasets, demonstrating improved personalization of human
preferences.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [83] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: Hive哈希表是一种高性能、支持动态调整大小的GPU哈希表，通过缓存对齐的桶布局、warp同步并发协议和负载感知的动态调整策略，实现了高达95%的负载因子和比现有GPU哈希表高1.5-2倍的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有GPU哈希表在并发更新、高负载因子和不规则内存访问模式方面存在性能瓶颈，需要一种能够适应不同工作负载且无需全局重哈希的高性能解决方案。

Method: 采用缓存对齐的打包桶布局存储键值对，使用WABC和WCME等warp同步并发协议减少争用，实现负载因子感知的动态调整策略，并通过四步插入策略处理高争用情况下的插入操作。

Result: 在NVIDIA RTX 4090上的实验表明，Hive哈希表在混合插入-删除-查找工作负载下，负载因子高达95%，吞吐量比最先进的GPU哈希表高1.5-2倍，在平衡工作负载下达到35亿次更新/秒和近40亿次查找/秒。

Conclusion: Hive哈希表通过创新的设计实现了高性能、高负载因子和动态调整能力，为GPU加速的数据处理提供了可扩展且高效的解决方案。

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [84] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO是一个新的区块链执行引擎，结合乐观并发控制(OCC)和对象数据模型来解决高竞争工作负载下的性能瓶颈问题，通过四项核心创新显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着区块链共识算法效率提升，执行层成为新的性能瓶颈，特别是在高竞争场景下。现有的乐观并发控制(OCC)和悲观并发控制(PCC)方法在高竞争工作负载下性能都会下降。

Method: NEMO结合OCC与对象数据模型，引入四项创新：1) 对仅使用自有对象的交易采用贪婪提交规则；2) 精细化处理依赖关系以减少重新执行；3) 使用静态可推导的读/写提示指导执行；4) 基于优先级的调度器，优先执行能解锁其他交易的交易。

Result: 模拟执行实验显示，NEMO显著减少了冗余计算，在16个工作线程下，吞吐量比最先进的OCC方法Block-STM高出42%，比悲观并发控制基线高出61%。

Conclusion: NEMO通过结合OCC和对象数据模型，有效解决了高竞争工作负载下的执行性能瓶颈，显著提升了区块链执行引擎的吞吐量。

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [85] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: 本文提出了一种基于Kubernetes的Charm++应用弹性调度器，能够动态调整HPC作业规模以最大化云资源利用率，同时保证高优先级作业的响应时间。


<details>
  <summary>Details</summary>
Motivation: 随着HPC应用在云环境中的采用增加，按使用付费的成本模型要求开发专门的编程模型和调度器来实现高效的云资源利用，其中关键需求是能够动态调整应用规模。

Method: 开发了Kubernetes operator来运行Charm++应用，并设计了基于优先级的弹性作业调度器，能够根据Kubernetes集群状态动态调整作业规模。

Result: 弹性调度器能够以最小开销重新调整HPC作业规模，相比传统静态调度器展示了显著的性能提升。

Conclusion: Charm++的迁移对象范式原生支持动态重缩放，结合Kubernetes operator和弹性调度器，能够有效提高云环境中HPC应用的资源利用效率。

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [86] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan控制器通过主动调节LLM应用的输出长度来应对系统负载变化，降低推理延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用对底层基础设施不感知，在系统负载变化时无法自适应调整，导致推理延迟增加和用户体验下降

Method: 开发beLLMan控制器，使LLM基础设施能够主动向第一方LLM应用发送信号，根据系统负载动态调整输出长度

Result: 在H100 GPU真实测试平台上，beLLMan将端到端延迟降低高达8倍，能耗降低25%，同时在高负载期间多服务19%的请求

Conclusion: beLLMan通过主动输出长度控制有效解决了LLM推理中的延迟和能耗问题，提升了系统性能和用户体验

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [87] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 本文探讨了本地与云端模拟环境之间的权衡，重点关注可扩展性与隐私保护之间的平衡，旨在提高远程模拟的可信度并促进虚拟原型技术的采用。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统的快速发展和AI算法的日益复杂，需要基于虚拟原型技术的强大硬件/软件协同设计方法。市场上存在各种模拟解决方案，每种都有独特的技术方法和优缺点。远程按需计算资源的普及为运营策略提供了新的可能性。

Method: 研究本地与云端模拟环境之间的二分法，分析计算基础设施设置对执行性能和数据安全的影响，重点关注嵌入式AI开发工作流程中高效模拟的关键作用。

Result: 提出了一个解决方案，旨在可持续地提高远程模拟的可信度，并促进虚拟原型实践的采用。

Conclusion: 通过平衡可扩展性和隐私保护，云端模拟环境可以为嵌入式AI开发提供有效的解决方案，同时需要确保数据安全和执行性能的优化。

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [88] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: 该论文研究了基于匹配的离散迭代负载均衡算法，证明了在任意图上通过简单的局部平衡方案可以在渐进匹配连续负载均衡谱界的时间内达到常数3的差异度。


<details>
  <summary>Details</summary>
Motivation: 研究离散负载均衡问题，旨在探索在任意图上通过局部匹配操作实现负载均衡的有效性，并与连续负载均衡的性能进行比较。

Method: 提出了一类简单的局部平衡方案，通过匹配节点来平衡令牌。在每轮中，匹配的节点平均其令牌数量，如果总和为奇数则随机分配多余令牌。该方法涵盖了三种流行模型：匹配模型、平衡电路模型和异步模型。

Result: 证明该离散平衡方案以高概率在渐进匹配连续负载均衡谱界的时间内达到差异度为3的负载分布。

Conclusion: 离散负载均衡在一般模型下与连续负载均衡具有相同的难度，且能在任意图上实现常数差异度的负载平衡，改进了先前工作的结果。

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [89] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: 提出了用户加权公平排队调度器UWFQ，通过模拟虚拟公平排队系统和运行时分区技术，在Spark框架中实现用户级公平性和低响应时间。


<details>
  <summary>Details</summary>
Motivation: Spark内置调度器在工业分析环境中难以同时保持用户级公平性和低平均响应时间，现有解决方案偏向提交更多作业的用户，公平调度器缺乏对动态用户工作负载的适应性。

Method: 设计UWFQ调度器，模拟虚拟公平排队系统，基于估计完成时间调度作业，引入运行时分区技术动态调整任务粒度以减少任务倾斜和优先级反转。

Result: 在Spark框架中实现UWFQ，使用多用户合成工作负载和Google集群跟踪进行评估，相比现有Spark调度器和先进公平调度算法，小作业的平均响应时间最多减少74%。

Conclusion: UWFQ调度器能有效平衡用户公平性和作业响应时间，显著提升Spark在共享应用环境中的调度性能。

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [90] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: 本文在分布式量子计算领域研究了Lovász局部引理问题，证明了在量子LOCAL模型中分布式LLL的复杂度下界为2^Ω(log^* n)，特别针对sinkless orientation这一特殊情况，在更强的随机在线LOCAL模型中也获得相同下界。


<details>
  <summary>Details</summary>
Motivation: 研究分布式量子计算中的Lovász局部引理问题，解决近期提出的开放性问题，为各种模型中的sinkless orientation和分布式LLL提供首个超常数下界。

Method: 开发全新的下界证明技术，在随机在线LOCAL模型中分析sinkless orientation问题，进而推广到量子LOCAL模型和其他研究模型。

Result: 证明了分布式LLL在量子LOCAL模型中的复杂度下界为2^Ω(log^* n)，sinkless orientation在多种模型中都有相同的超常数下界。

Conclusion: 本文提供了首个在多种模型中sinkless orientation和分布式LLL的超常数下界，开发的新技术有望成为证明局部性背景下重要问题的后量子下界的首个通用技术。

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [91] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: Funky是一个面向云原生应用的FPGA感知编排引擎，通过FPGA虚拟化、状态管理和编排组件解决了FPGA在云环境中缺乏虚拟化、隔离和抢占支持的问题。


<details>
  <summary>Details</summary>
Motivation: FPGA在云原生环境中的采用受到限制，因为FPGA缺乏虚拟化、隔离和抢占支持，云提供商无法提供FPGA编排服务，导致可扩展性、灵活性和弹性不足。

Method: Funky通过三个主要贡献实现：1）FPGA虚拟化创建轻量级沙盒；2）FPGA状态管理支持任务抢占和检查点；3）遵循行业标准CRI/OCI规范的FPGA感知编排组件。

Result: 在4台x86服务器和Alveo U50 FPGA卡上的评估显示：Funky成功移植了23个OpenCL应用，仅修改3.4%源代码，OCI镜像比AMD的FPGA可访问Docker容器小28.7倍，性能开销仅为7.4%，并提供强隔离和分布式FPGA编排。

Conclusion: Funky通过完整的FPGA感知编排解决方案，显著提升了FPGA在云原生环境中的性能、利用率、可扩展性和容错能力，在大规模集群中展现出良好的可扩展性和调度效率。

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>
